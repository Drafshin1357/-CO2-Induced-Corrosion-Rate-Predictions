{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  Mount or Connect to Drive"
      ],
      "metadata": {
        "id": "8esu1SW7kQeE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GIxoDW_IFmf",
        "outputId": "1fa12b21-41f5-4554-8ef2-d0b5e4195cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Set Work Directory (Dataset Directory)"
      ],
      "metadata": {
        "id": "3F0sQ3cDkYyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJEWq1oPIauG",
        "outputId": "c68a2a1a-0b48-4630-ab7d-afea82b91f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/myDetection/ShapeRecogition/N5/OnePose_Plus_Plus/2024/CO2/NewApproach\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/gdrive/MyDrive/myDetection/ShapeRecogition/N5/OnePose_Plus_Plus/2024/CO2/NewApproach'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries"
      ],
      "metadata": {
        "id": "e_GizBQjkiTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTETYYW-eSUF"
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1-X_NAi2inBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684956ba-7909-4361-f630-129efbbb84d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "Collecting scikit-learn==1.5.2\n",
            "  Using cached scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "Using cached scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "Installing collected packages: scikit-learn\n",
            "Successfully installed scikit-learn-1.5.2\n",
            "Requirement already satisfied: matplotlib==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (1.4.8)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.1) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y scikit-learn\n",
        "!pip install scikit-learn==1.5.2\n",
        "!pip install matplotlib==3.8.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q7Bxs9ooHpF"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras\n",
        "\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install xlwt\n",
        "\n",
        "!pip install openpyxl\n",
        "!sudo apt install font-manager\n",
        "!rm ~/.cache/matplotlib -fr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAEb8__SpPBl"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary font (time New Roman)"
      ],
      "metadata": {
        "id": "phDF8eTfkm1K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qLzqLPNXyHQQ"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale=True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ljVKYFYpxXPw"
      },
      "outputs": [],
      "source": [
        "!sudo cp 'Times New Roman.ttf' '/usr/share/fonts/truetype/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caQt7yUUx8E4"
      },
      "outputs": [],
      "source": [
        "!sudo fc-cache -fv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mU7uI6OyQlJ",
        "outputId": "c2832d03-7565-482e-95d7-7cc64a1e0f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/share/fonts/truetype/Times New Roman.ttf: Times New Roman:style=Regular,Normal,obyčejné,Standard,Κανονικά,Normaali,Normál,Normale,Standaard,Normalny,Обычный,Normálne,Navadno,thường,Arrunta\n"
          ]
        }
      ],
      "source": [
        "!fc-list | grep \"Times New Roman\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "76SNoO3S1xMg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'serif'  # or 'sans-serif', 'monospace'\n",
        "# or\n",
        "plt.rcParams['font.serif'] = ['DejaVu Serif'] #specifies a specific serif font.\n",
        "plt.rcParams['font.family'] = 'Liberation Serif'  # Another alternative\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "fm.fontManager.addfont('Times New Roman.ttf')  # Update the path\n",
        "plt.rcParams['font.family'] = 'Times New Roman'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate the cleaned CSV file"
      ],
      "metadata": {
        "id": "OmiFvEdVldYz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv8JTqJpmeyw"
      },
      "outputs": [],
      "source": [
        "dataAll, n_exp = read_data('SequentialDataInhibitor_large', new=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries and Config"
      ],
      "metadata": {
        "id": "KzF68hOTlS3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "# from sklearn.svm import SVR\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Add , LayerNormalization\n",
        "from cuml.svm import SVR as cuSVR\n",
        "from cuml.ensemble import RandomForestRegressor as cuRF\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
        "matplotlib.rcParams['axes.linewidth'] = 1.5\n",
        "target = {'regression': 'corrosion_mm_yr'}"
      ],
      "metadata": {
        "id": "yRKKjZq3lXSy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables"
      ],
      "metadata": {
        "id": "UHy-jvwrl0x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param = dict(test_size=0.25, cv=5, scoring='mse', iterations=2,\n",
        "             summary= True, grid_search=True , compare=True, importance=True, parity=True,\n",
        "             production=True, sensitivity=True)\n",
        "\n",
        "input_dim = 1"
      ],
      "metadata": {
        "id": "C8tScYJdl4Mr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implemented Normal Functions"
      ],
      "metadata": {
        "id": "gw3OOeKml9nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stack_data(df, _set):\n",
        "    conc = df['concentration_ppm'].unique()\n",
        "    df2 = pd.DataFrame()\n",
        "    i = 0\n",
        "    for c in conc:\n",
        "        if c == conc[0]:\n",
        "            df2 = df.loc[df['concentration_ppm'] == c].reset_index(drop=True)\n",
        "            df2['time_hrs_original'] = df2['time_hrs']\n",
        "            _min, _max = df2['time_hrs'].min(), df2['time_hrs'].max()\n",
        "            df2['time_hrs'] = df2['time_hrs'] - _min\n",
        "            df2['pre_concentration_zero'] = 'Yes'\n",
        "            df2['pre_concentration_ppm'] = 0\n",
        "            if _set == 'training':\n",
        "                df2['initial_corrosion_mm_yr'] = df.loc[0, 'corrosion_mm_yr']\n",
        "        else:\n",
        "            df3 = df.loc[df['concentration_ppm'] == c].reset_index(drop=True)\n",
        "            df3['time_hrs_original'] = df3['time_hrs']\n",
        "            _min, _max = df3['time_hrs'].min(), df3['time_hrs'].max()\n",
        "            df3['time_hrs'] = df3['time_hrs'] - _min\n",
        "            if i == 1:\n",
        "                df3['pre_concentration_zero'] = 'Yes'\n",
        "            else:\n",
        "                df3['pre_concentration_zero'] = 'No'\n",
        "            df3['pre_concentration_ppm'] = conc[i - 1]\n",
        "            if _set == 'training':\n",
        "                df3['initial_corrosion_mm_yr'] = df.loc[0, 'corrosion_mm_yr']\n",
        "            df2 = pd.concat([df2, df3], ignore_index=True)\n",
        "        i += 1\n",
        "    return df2\n",
        "\n",
        "\n",
        "def read_exp(df, _set):\n",
        "    df.columns = df.columns.str.replace(', ', '_')\n",
        "    df.columns = df.columns.str.replace(' ', '_')\n",
        "    replicas = df['Description'].unique()\n",
        "    df2 = pd.DataFrame()\n",
        "    for replica in replicas:\n",
        "        if replica == replicas[0]:\n",
        "            df2 = df.loc[df['Description'] == replica].reset_index(drop=True)\n",
        "            df2 = stack_data(df2, _set)\n",
        "        else:\n",
        "            df3 = df.loc[df['Description'] == replica].reset_index(drop=True)\n",
        "            df3 = stack_data(df3, _set)\n",
        "            df2 = pd.concat([df2, df3], ignore_index=True)\n",
        "    return df2\n",
        "\n",
        "\n",
        "def clean_data(df):\n",
        "    df = df[df['corrosion_mm_yr'] >= 0.0]\n",
        "    aux, aux2 = np.log10(df['corrosion_mm_yr']), np.log10(df['initial_corrosion_mm_yr'])\n",
        "    df = df.drop(['corrosion_mm_yr', 'initial_corrosion_mm_yr'], axis=1)\n",
        "    df['corrosion_mm_yr'], df['initial_corrosion_mm_yr'] = aux, aux2\n",
        "    df = df.dropna(axis=0, how='any').reset_index(drop=True)\n",
        "    df['Lab'] = df['Lab'].str.rstrip()\n",
        "    df['Type_of_test'] = df['Type_of_test'].str.rstrip()\n",
        "    df = df.replace({'Type_of_test': {'Sequential Dose': 'sequential_dose',\n",
        "                                      'Single Dose YP': 'single_dose_YP',\n",
        "                                      'Single Dose NP': 'single_dose_NP'},\n",
        "                     'pH': {6: 'Controlled=6'}})\n",
        "    return df\n",
        "\n",
        "\n",
        "def read_data(file_name, new):\n",
        "    if new:\n",
        "        sheet_names = pd.ExcelFile('{}.xlsx'.format(file_name)).sheet_names\n",
        "        df = pd.DataFrame()\n",
        "        n = 0\n",
        "        for sheet_name in sheet_names:\n",
        "            if sheet_name == sheet_names[0]:\n",
        "                df = pd.read_excel('{}.xlsx'.format(file_name), sheet_name=sheet_name)\n",
        "                df = read_exp(df, 'training')\n",
        "                df['Experiment'] = n + 1\n",
        "            else:\n",
        "                df2 = pd.read_excel('{}.xlsx'.format(file_name), sheet_name=sheet_name)\n",
        "                df2 = read_exp(df2, 'training')\n",
        "                df2['Experiment'] = n + 1\n",
        "                df = pd.concat([df, df2], ignore_index=True)\n",
        "            n += 1\n",
        "            print(n)\n",
        "        df = clean_data(df)\n",
        "        excel_output(df, _root='', file_name='{}Cleaned'.format(file_name), csv=True)\n",
        "    else:\n",
        "        df = pd.read_csv('{}Cleaned.csv'.format(file_name))\n",
        "        df = df.drop(['Unnamed: 0'], axis=1)\n",
        "        n = len(df['Experiment'].unique())\n",
        "    return df, n\n",
        "\n",
        "\n",
        "def filter_lab(df, lab):\n",
        "    if lab != 'All':\n",
        "        df = df[df['Lab'] == lab].reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def update_data(df, lab):\n",
        "    df2 = filter_lab(df, lab)\n",
        "    return df2\n",
        "\n",
        "\n",
        "def columns_stats(df, _set, _root):\n",
        "    statistics = pd.DataFrame()\n",
        "    for column in df.columns:\n",
        "        if (column != 'time_hrs') and (column != 'time_hrs_original') and \\\n",
        "                (column != 'corrosion_mm_yr') and (column != 'initial_corrosion_mm_yr'):\n",
        "            if column == df.columns[0]:\n",
        "                statistics = pd.DataFrame(df[column].value_counts()).reset_index(drop=False)\n",
        "                statistics.rename(columns={'index': column, column: 'Num_samples'}, inplace=True)\n",
        "            else:\n",
        "                temp = pd.DataFrame(df[column].value_counts()).reset_index(drop=False)\n",
        "                temp.rename(columns={'index': column, column: 'Num_samples'}, inplace=True)\n",
        "                statistics = pd.concat([statistics, temp], axis=1)\n",
        "    excel_output(statistics, _root=_root, file_name='columnsStats_{}'.format(_set), csv=False)\n",
        "\n",
        "\n",
        "def experiments_stats(df, _set, _root):\n",
        "    statistics = pd.DataFrame(columns=['Experiment', 'num_replica', 'CI concentration (ppm, hrs)', 'Length_hrs',\n",
        "                                       'Pressure_bar_CO2', 'Temperature_C', 'CI', 'Shear_Pa',\n",
        "                                       'Brine_Ionic_Strength', 'pH', 'Brine_Type', 'Type_of_test', 'Lab'])\n",
        "    _experiments = df['Experiment'].unique()\n",
        "    for _exp in _experiments:\n",
        "        df2 = df.loc[df['Experiment'] == _exp].reset_index(drop=True)\n",
        "        df3 = df2.groupby('concentration_ppm')['time_hrs'].max()\n",
        "        n_replica = len(df2['Description'].unique())\n",
        "        conc = df2['concentration_ppm'].unique()\n",
        "        _conc_ppm = ''\n",
        "        for c in conc:\n",
        "            tt = df3.loc[c]\n",
        "            if c == conc[0]:\n",
        "                _conc_ppm = _conc_ppm + '({:.0f}, {:.0f})'.format(c, tt)\n",
        "            else:\n",
        "                _conc_ppm = _conc_ppm + ' - ({:.0f}, {:.0f})'.format(c, tt)\n",
        "        statistics = statistics._append({'Experiment': _exp,\n",
        "                                        'num_replica': n_replica,\n",
        "                                        'CI concentration (ppm, hrs)': _conc_ppm,\n",
        "                                        'Length_hrs': '~ {:.0f}'.format(df3.sum()),\n",
        "                                        'Pressure_bar_CO2': df2.loc[0, 'Pressure_bar_CO2'],\n",
        "                                        'Temperature_C': df2.loc[0, 'Temperature_C'],\n",
        "                                        'CI': df2.loc[0, 'CI'],\n",
        "                                        'Shear_Pa': df2.loc[0, 'Shear_Pa'],\n",
        "                                        'Brine_Ionic_Strength': df2.loc[0, 'Brine_Ionic_Strength'],\n",
        "                                        'pH': df2.loc[0, 'pH'],\n",
        "                                        'Brine_Type': df2.loc[0, 'Brine_Type'],\n",
        "                                        'Type_of_test': df2.loc[0, 'Type_of_test'],\n",
        "                                        'Lab': df2.loc[0, 'Lab']}, ignore_index=True)\n",
        "\n",
        "    excel_output(statistics, _root=_root, file_name='experimentsStats_{}'.format(_set), csv=False)\n",
        "\n",
        "\n",
        "def view_data_exp(df, y_axis_scale, _set, _root):\n",
        "    _root = '{}/{}{}'.format(_root, _set, y_axis_scale)\n",
        "    if not os.path.exists(_root):\n",
        "        os.makedirs(_root)\n",
        "    # ---------------------------------\n",
        "    _experiments = df['Experiment'].unique()\n",
        "    for _exp in _experiments:\n",
        "        df2 = df.loc[df['Experiment'] == _exp]\n",
        "        replicas = df2['Description'].unique()\n",
        "        fig, ax = plt.subplots(1, figsize=(9, 9))\n",
        "        _X_plot = pd.Series(dtype='float64')\n",
        "        n = 1\n",
        "        for rep in replicas:\n",
        "            df3 = df2.loc[df['Description'] == rep]\n",
        "            _X = df3['time_hrs_original']\n",
        "            _y = 10 ** (df3['corrosion_mm_yr'])\n",
        "            plt.scatter(_X, _y, label='Replica {}'.format(n))\n",
        "            if n == 1:\n",
        "                _X_plot = _X\n",
        "            n += 1\n",
        "        if y_axis_scale == 'Log':\n",
        "            plt.yscale('log')\n",
        "            ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "            if _exp == 14:\n",
        "                ax.set_ylim(0.001, 100)\n",
        "                ax.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
        "            else:\n",
        "                ax.set_ylim(0.01, 100)\n",
        "        # ---------------------------------\n",
        "        plt.text(0.02, 1.03, 'Experiment {}'.format(_exp),\n",
        "                 ha='left', va='center', transform=ax.transAxes, fontdict={'color': 'k', 'weight': 'bold', 'size': 21})\n",
        "        # ---------------------------------\n",
        "        plt.grid(linewidth=0.5)\n",
        "        x_axis_max = 10 * (1 + int(np.max(_X_plot) / 10))\n",
        "        if _exp == 6:\n",
        "            x_axis_max = 40\n",
        "        elif _exp == 11 or _exp == 13 or _exp == 17 or _exp == 18 or _exp == 19:\n",
        "            x_axis_max = 25\n",
        "        elif _exp == 14:\n",
        "            x_axis_max = 30\n",
        "        elif _exp == 16:\n",
        "            x_axis_max = 15\n",
        "        x_axis_index = np.linspace(0, x_axis_max, num=6)\n",
        "        ax.set_xticks(x_axis_index)\n",
        "        ax.set_xlim(0, x_axis_max)\n",
        "        ax.set_xticklabels(x_axis_index, fontsize=20)\n",
        "        ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        ax.set_xlabel('Time (hr)', fontsize=27)\n",
        "        plt.yticks(fontsize=20)\n",
        "        ax.set_ylabel('Corrosion Rate (mm/year)', fontsize=27)\n",
        "        n_col, leg_fontsize = 1, 20\n",
        "        if _exp == 10 or _exp == 14:\n",
        "            n_col, leg_fontsize = 2, 18\n",
        "        plt.legend(loc='upper right', fontsize=leg_fontsize, ncol=n_col, fancybox=True, shadow=True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('{}/exp{}.png'.format(_root, _exp))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def experiments_types(df, y_axis_scale, _experiments, _root):\n",
        "    _root = '{}/experimentsTypes{}'.format(_root, y_axis_scale)\n",
        "    if not os.path.exists(_root):\n",
        "        os.makedirs(_root)\n",
        "    # ---------------------------------\n",
        "    for _e in _experiments:\n",
        "        df2 = df.loc[df['Experiment'] == _e[0]]\n",
        "        df3 = df2.loc[df['Description'] == _e[1]]\n",
        "        fig, ax = plt.subplots(1, figsize=(9, 9))\n",
        "        _X = df3['time_hrs_original'].to_numpy()\n",
        "        _y = 10 ** (df3['corrosion_mm_yr'].to_numpy())\n",
        "        marker_size = [50 + i * 0 for i in _y]\n",
        "        plt.scatter(_X, _y, s=marker_size, c='black')\n",
        "        if y_axis_scale == 'Log':\n",
        "            plt.yscale('log')\n",
        "            ax.set_ylim(0.01, 100)\n",
        "            plt.yticks(fontsize=20)\n",
        "        else:\n",
        "            if _e[0] == 3:\n",
        "                y_axis_mas = 40\n",
        "            elif _e[0] == 20:\n",
        "                y_axis_mas = 10\n",
        "            else:\n",
        "                y_axis_mas = 6\n",
        "            y_axis_index = np.linspace(0, y_axis_mas, num=6)\n",
        "            ax.set_yticks(y_axis_index)\n",
        "            ax.set_ylim(0, y_axis_mas)\n",
        "            ax.set_yticklabels(y_axis_index, fontsize=20)\n",
        "            ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        # ---------------------------------\n",
        "        plt.text(0.02, 1.03, '{}'.format(_e[2]),\n",
        "                 ha='left', va='center', transform=ax.transAxes, fontdict={'color': 'k', 'weight': 'bold', 'size': 21})\n",
        "        # ---------------------------------\n",
        "        plt.grid(linewidth=0.5)\n",
        "        x_axis_index = np.linspace(0, 10 * (1 + int(np.max(_X) / 10)), num=6)\n",
        "        ax.set_xticks(x_axis_index)\n",
        "        ax.set_xlim(0, 10 * (1 + int(np.max(_X) / 10)))\n",
        "        ax.set_xticklabels(x_axis_index, fontsize=20)\n",
        "        ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        ax.set_xlabel('Time (hr)', fontsize=27)\n",
        "        ax.set_ylabel('Corrosion Rate (mm/year)', fontsize=27)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('{}/exp{}.png'.format(_root, _e[0]))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def summary_data(df):\n",
        "    _root = 'regression/dataSummary'\n",
        "    if not os.path.exists(_root):\n",
        "        os.makedirs(_root)\n",
        "    # ---------------------------------\n",
        "    columns_stats(df, 'allReplicas', _root)\n",
        "    experiments_stats(df, 'allReplicas', _root)\n",
        "    view_data_exp(df, 'Log', 'allReplicas', _root)\n",
        "    view_data_exp(df, 'Normal', 'allReplicas', _root)\n",
        "\n",
        "\n",
        "def excel_output(_object, _root, file_name, csv):\n",
        "    if csv:\n",
        "        if _root != '':\n",
        "            _object.to_csv('{}/{}.csv'.format(_root, file_name))\n",
        "        else:\n",
        "            _object.to_csv('{}.csv'.format(file_name))\n",
        "    else:\n",
        "        if _root != '':\n",
        "            _object.to_excel('{}/{}.xlsx'.format(_root, file_name))\n",
        "        else:\n",
        "            _object.to_excel('{}.xlsx'.format(file_name))"
      ],
      "metadata": {
        "id": "70SWSCEDl7BX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implemented Models and Dataset Functionss  "
      ],
      "metadata": {
        "id": "Myzj2yqOmZvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_features(df):\n",
        "    df = df[['concentration_ppm', 'pre_concentration_ppm', 'time_hrs','time_hrs_original',\n",
        "             'Pressure_bar_CO2', 'Temperature_C', 'CI', 'Shear_Pa', 'Brine_Ionic_Strength',\n",
        "             'pH', 'Brine_Type', 'Type_of_test', 'initial_corrosion_mm_yr', 'Description', 'Experiment',\n",
        "             'corrosion_mm_yr']]\n",
        "    return df\n",
        "\n",
        "\n",
        "def encode_data(df):\n",
        "    cat_index = ['CI', 'pH', 'Brine_Type', 'Type_of_test']\n",
        "    num_index = ['Pressure_bar_CO2', 'Temperature_C', 'Shear_Pa', 'Brine_Ionic_Strength']\n",
        "    ohe = ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "    sc = StandardScaler()\n",
        "    ct = make_column_transformer((ohe, cat_index), (sc, num_index), remainder='passthrough')\n",
        "    ct.fit_transform(df)\n",
        "    df2 = ct.transform(df)\n",
        "    # ---------------------------------\n",
        "    names = []\n",
        "    for cat in cat_index:\n",
        "        unique = df[cat].value_counts().sort_index()\n",
        "        for name in unique.index:\n",
        "            names.append('{}_{}'.format(cat, name))\n",
        "    for num in num_index:\n",
        "        names.append(num)\n",
        "    names.append('concentration_ppm')\n",
        "    names.append('pre_concentration_ppm')\n",
        "    names.append('time_hrs')\n",
        "    names.append('time_hrs_original')\n",
        "    names.append('initial_corrosion_mm_yr')\n",
        "    names.append('Description')\n",
        "    names.append('Experiment')\n",
        "    names.append('corrosion_mm_yr')\n",
        "    # ---------------------------------\n",
        "    df2 = pd.DataFrame(df2)\n",
        "    df2.columns = names\n",
        "    return df2\n",
        "\n",
        "\n",
        "def split_data_random(df):\n",
        "    df = df.copy(deep=True)\n",
        "    df = shuffle(df)\n",
        "    head = int((1 - param['test_size']) * len(df))\n",
        "    tail = len(df) - head\n",
        "    df_train = df.head(head).reset_index(drop=True)\n",
        "    df_test = df.tail(tail).reset_index(drop=True)\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def split_xy(df, _shuffle):\n",
        "    if _shuffle:\n",
        "        df = shuffle(df)\n",
        "    df = df.drop(['Description', 'Experiment'], axis=1)\n",
        "    _X = df.iloc[:, 0:-1].reset_index(drop=True)\n",
        "    _y = df.iloc[:, -1].to_numpy()\n",
        "    return _X, _y\n",
        "\n",
        "\n",
        "def grid_search(model):\n",
        "    models = []\n",
        "    hp1 = {'MLP': [(2,), (4,), (6,), (8,), (10,),\n",
        "                    (2, 2), (4, 4), (6, 6), (8, 8), (10, 10),\n",
        "                    (2, 2, 2), (4, 4, 4), (6, 6, 6), (8, 8, 8), (10, 10, 10),\n",
        "                    (2, 2, 2, 2), (4, 4, 4, 4), (6, 6, 6, 6), (8, 8, 8, 8), (10, 10, 10, 10),\n",
        "                    (2, 2, 2, 2, 2), (4, 4, 4, 4, 4), (6, 6, 6, 6, 6), (8, 8, 8, 8, 8), (10, 10, 10, 10, 10)],\n",
        "           'SVM': [ 0.001, 0.0001],\n",
        "           'RF': [ 10, 50,100, 200, 500],\n",
        "           'KNN':  [ 3, 4 ,5, 6, 7],\n",
        "           'CDNN': [(64, 32), (128, 64)]}\n",
        "    hp2 = {'MLP': ['constant'],\n",
        "           'SVM': [ 10, 100, 1000],\n",
        "           'RF': [ 0.9, 1.0],\n",
        "           'KNN':['distance','uniform'],\n",
        "           'CDNN': ['swish' , 'tanh' ,'relu' ] }\n",
        "    hp3 = {\n",
        "        'CDNN_dropout': [ 0.3],\n",
        "        'CDNN_residual': [True]\n",
        "    }\n",
        "    for n in hp1[model]:\n",
        "        for m in hp2[model]:\n",
        "            if model == 'MLP':\n",
        "                models.append(('MLP_{}_{}'.format(n, m), MLPRegressor(max_iter=10000, random_state=5,\n",
        "                                                                      hidden_layer_sizes=n, learning_rate=m)))\n",
        "            elif model == 'SVM':\n",
        "                models.append(('SVM_{}_{}'.format(n, m), cuSVR(gamma=n, C=m)))\n",
        "            elif model == 'RF':\n",
        "                models.append(('RF_{}_{}'.format(n, m), cuRF(random_state=5,n_estimators=n, max_features=m)))\n",
        "            elif model == 'KNN':\n",
        "                models.append(('KNN_{}_{}'.format(n, m), KNeighborsRegressor(n_neighbors=n, weights=m)))\n",
        "            elif model == 'CDNN':\n",
        "\n",
        "                for dropout in hp3.get('CDNN_dropout', [0]):\n",
        "                    models.append(('CDNN_{}_{}_{}'.format(n, m, dropout),\n",
        "                                    KerasRegressor(build_fn=create_complex_deep_learning_model,\n",
        "                                    input_dim=input_dim,\n",
        "                                    hidden_layers=n,\n",
        "                                    activation=m,\n",
        "                                    dropout_rate=0.2,\n",
        "                                    epochs=1000, batch_size=256, verbose=1)))\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def create_complex_deep_learning_model(input_dim, hidden_layers, activation='relu', dropout_rate=0.3):\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16')  # Enable mixed precision for better GPU performance\n",
        "\n",
        "    with tf.device('/GPU:0'):\n",
        "        inputs = Input(shape=(input_dim,))\n",
        "        x = Dense(hidden_layers[0], activation=activation, kernel_initializer='he_normal')(inputs)\n",
        "        x = LayerNormalization()(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "        # Add residual blocks\n",
        "        for layer_size in hidden_layers[1:]:\n",
        "            shortcut = x  # Store previous layer for residual connection\n",
        "            x = Dense(layer_size, activation=activation, kernel_initializer='he_normal')(x)\n",
        "            x = LayerNormalization()(x)\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "            # Ensure shortcut dimensions match\n",
        "            if shortcut.shape[-1] != x.shape[-1]:\n",
        "                shortcut = Dense(layer_size, kernel_initializer='he_normal')(shortcut)\n",
        "\n",
        "            x = Add()([x, shortcut])  # Add residual connection\n",
        "\n",
        "        # Output layer\n",
        "        outputs = Dense(1, activation='linear', dtype='float32')(x)  # Ensure float32 output\n",
        "\n",
        "        # Create the model\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        optimizer = Adam(learning_rate=1e-2, clipnorm=1.0)  # Clipping to prevent exploding gradients\n",
        "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def compare_models(df, models):\n",
        "    scoring, cv, iterations = 'neg_mean_squared_error', param['cv'], param['iterations']\n",
        "    if param['scoring'] == 'r2':\n",
        "        scoring = 'r2'\n",
        "    # ---------------------------------\n",
        "    results = pd.DataFrame()\n",
        "    for i in range(iterations):\n",
        "        _X_train, _y_train = split_xy(df, True)\n",
        "        _X_train = _X_train.astype(np.float32)\n",
        "        temp = []\n",
        "        for name, model in models:\n",
        "            print(name)\n",
        "            cv_results = cross_val_score(model, _X_train, _y_train, cv=cv, scoring=scoring)\n",
        "            cv_results = np.mean(cv_results)\n",
        "            temp.append(cv_results)\n",
        "        if i == 0:\n",
        "            results = pd.DataFrame(temp)\n",
        "        else:\n",
        "            results = pd.concat([results, pd.DataFrame(temp)], axis=1, ignore_index=True)\n",
        "    results['mean'] = results.mean(axis=1)\n",
        "    results['std'] = results.std(axis=1)\n",
        "    # ---------------------------------\n",
        "    _names, _models = [], []\n",
        "    for name, model in models:\n",
        "        _names.append(name)\n",
        "        _models.append(model)\n",
        "    results['name'] = pd.Series(_names)\n",
        "    results['model'] = pd.Series(_models)\n",
        "    # ---------------------------------\n",
        "    id_best = results['mean'].idxmax()\n",
        "    _best = results.loc[id_best, 'model']\n",
        "    return results, _best\n",
        "\n",
        "\n",
        "def prediction(df, estimator):\n",
        "    errors = pd.DataFrame()\n",
        "    for i in range(param['iterations']):\n",
        "        df_training, df_testing = split_data_random(df)\n",
        "        _X_train, _y_train = split_xy(df_training, True)\n",
        "        estimator.fit(_X_train, _y_train)\n",
        "        _X_test, _y_test = split_xy(df_testing, True)\n",
        "        _y_pred = estimator.predict(_X_test)\n",
        "        errors.loc[i, 'r2'] = r2_score(_y_test, _y_pred)\n",
        "        errors.loc[i, 'mse'] = mean_squared_error(_y_test, _y_pred)\n",
        "        errors.loc[i, 'mae'] = mean_absolute_error(_y_test, _y_pred)\n",
        "        errors.loc[i, 'rmse'] = np.sqrt(mean_squared_error(_y_test, _y_pred))\n",
        "    _scores = [('R2', np.mean(errors['r2']), np.std(errors['r2'])),\n",
        "               ('MSE', np.mean(errors['mse']), np.std(errors['mse'])),\n",
        "               ('MAE', np.mean(errors['mae']), np.std(errors['mae'])),\n",
        "               ('RMSE', np.mean(errors['rmse']), np.std(errors['rmse']))]\n",
        "    return _scores\n",
        "\n",
        "\n",
        "def split_data_exp(df, _seat_out):\n",
        "    df_train = df.copy(deep=True)\n",
        "    df_test = pd.DataFrame()\n",
        "    for _exp in _seat_out:\n",
        "        df_train = df_train.loc[df_train['Experiment'] != _exp]\n",
        "        df_test = pd.concat([df_test, df.loc[df['Experiment'] == _exp]], ignore_index=True)\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def production(df_x, df_xy):\n",
        "    df_x_prod = df_x.copy(deep=True)\n",
        "    df_xy_prod = df_xy.copy(deep=True)\n",
        "    replicas = [i for i in df_x_prod['initial_corrosion_mm_yr'].unique()]\n",
        "    df_prod_final = df_x_prod.loc[df_x_prod['initial_corrosion_mm_yr'] == replicas[0]]\n",
        "    for replica in replicas:\n",
        "        df_temp = df_x_prod.loc[df_x_prod['initial_corrosion_mm_yr'] == replica]\n",
        "        df_prod_final = df_temp if len(df_temp) < len(df_prod_final) else df_prod_final\n",
        "    _y_prod = []\n",
        "    for replica in replicas:\n",
        "        _y_temp = df_xy_prod.loc[df_xy_prod['initial_corrosion_mm_yr'] == replica].iloc[:, -1].to_numpy()\n",
        "        _y_temp = _y_temp[:len(df_prod_final)]\n",
        "        _y_prod = _y_temp.tolist() if replica == replicas[0] else [xx + yy for xx, yy in zip(_y_prod, _y_temp)]\n",
        "    _y_prod = [i / len(replicas) for i in _y_prod]\n",
        "    _y_prod = np.array(_y_prod)\n",
        "    return df_prod_final, _y_prod\n",
        "\n",
        "\n",
        "def sensitivity(df_original, df, _experiment):\n",
        "    df_time = df_original.copy(deep=True)\n",
        "    df_time = df_time.loc[df_time['Experiment'] == _experiment].reset_index(drop=True)\n",
        "    replicas_time = df_time['initial_corrosion_mm_yr'].unique()\n",
        "    df_time = df_time.loc[df_time['initial_corrosion_mm_yr'] == replicas_time[0]]\n",
        "    time_hrs_sens = df_time['time_hrs_original']\n",
        "    # ---------------------------------\n",
        "    replicas = df['initial_corrosion_mm_yr'].unique()\n",
        "    df = df.loc[df['initial_corrosion_mm_yr'] == replicas[0]]\n",
        "    return df, time_hrs_sens"
      ],
      "metadata": {
        "id": "coBO3whpmLaL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization Functions"
      ],
      "metadata": {
        "id": "jteuDJ8OnqGf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QtiWHRX0Ia_V"
      },
      "outputs": [],
      "source": [
        "def compare_models_box_plot(df):\n",
        "    _root = 'regression/gridSearchModels'\n",
        "    if not os.path.exists(_root):\n",
        "        os.makedirs(_root)\n",
        "\n",
        "    cv, iterations = param['cv'], param['iterations']\n",
        "\n",
        "    x_axis_labels = [name for name in df['name']]\n",
        "    df = df.drop(['name', 'mean', 'std', 'model'], axis=1)\n",
        "    df = df.transform(lambda x: -x)\n",
        "    _y_matrix = df.values.tolist()\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "\n",
        "    # Use seaborn for a more professional look\n",
        "    sns.boxplot(data=_y_matrix, palette=\"Blues\", width=0.6, flierprops=dict(marker='o', markersize=5))\n",
        "\n",
        "    # Add title and labels\n",
        "    plt.title('Model Comparison using {}-fold CV ({} Replications)'.format(cv, iterations), fontsize=24, pad=20)\n",
        "    plt.xticks(ticks=np.arange(len(x_axis_labels)), labels=x_axis_labels, fontsize=18, rotation=45)\n",
        "    plt.yticks(fontsize=18)\n",
        "    plt.ylabel('Mean Squared Error (MSE)', fontsize=20)\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add info box\n",
        "    info = '{}-fold cross validation analysis \\n{} replications per algorithm'.format(cv, iterations)\n",
        "    plt.text(0.03, 0.96, info,\n",
        "             ha='left', va='top', transform=ax.transAxes,\n",
        "             fontdict={'color': 'k', 'size': 16},\n",
        "             bbox={'boxstyle': 'round', 'fc': 'snow', 'ec': 'gray', 'pad': 0.5})\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('{}/comparison.png'.format(_root))\n",
        "    plt.close()\n",
        "\n",
        "def importance_plot(df, estimator, _x, _y):\n",
        "    _root = 'regression/bestModelPerformance'\n",
        "    if not os.path.exists(_root):\n",
        "        os.makedirs(_root)\n",
        "\n",
        "    names = df.columns\n",
        "\n",
        "    if hasattr(estimator, \"feature_importances_\"):\n",
        "        # For tree-based models\n",
        "        imp = estimator.feature_importances_\n",
        "        indices = np.argsort(imp)\n",
        "\n",
        "    else:\n",
        "        print(\"Feature importance not available for this model. Using Permutation Importance instead.\")\n",
        "\n",
        "        # Compute permutation importance\n",
        "        result = permutation_importance(estimator, _x, _y, scoring='r2', n_repeats=10, random_state=42)\n",
        "        imp = result.importances_mean  # Mean importance scores\n",
        "        indices = np.argsort(imp)\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "\n",
        "    # Use a color gradient\n",
        "    colors = plt.cm.Blues(np.linspace(0.3, 1, len(indices)))\n",
        "    plt.barh(range(len(indices)), imp[indices], color=colors, align='center')\n",
        "\n",
        "    # Add title and labels\n",
        "    plt.title('Feature Importance', fontsize=24, pad=20)\n",
        "    plt.yticks(range(len(indices)), [names[i] for i in indices], fontsize=16)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.xlabel('Relative Importance', fontsize=20)\n",
        "\n",
        "    # Add a horizontal line for visual separation\n",
        "    ax.axvline(x=0, color='gray', linestyle='--', linewidth=0.8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('{}/featuresImp.png'.format(_root))\n",
        "    plt.close()\n",
        "\n",
        "    excel_output(pd.DataFrame(imp), _root, file_name='rf_feature_imp', csv=False)\n",
        "\n",
        "    permute_imp_results = permutation_importance(estimator, _x, _y, scoring='neg_mean_squared_error')\n",
        "    permute_imp = permute_imp_results.importances_mean\n",
        "    excel_output(pd.DataFrame(permute_imp), _root, file_name='permutation_imp', csv=False)\n",
        "\n",
        "    return imp, permute_imp\n",
        "\n",
        "def parity_plot(_y_test, _y_pred, _scores):\n",
        "    _root = 'regression/bestModelPerformance'\n",
        "    if not os.path.exists(_root):\n",
        "        os.makedirs(_root)\n",
        "\n",
        "    info = '{} = {:.3f} +/- {:.3f}\\n{} = {:.3f} +/- {:.3f}\\n{} = {:.3f} +/- {:.3f}\\n{} = {:.3f} +/- {:.3f}'. \\\n",
        "        format(_scores[0][0], _scores[0][1], _scores[0][2],\n",
        "               _scores[1][0], _scores[1][1], _scores[1][2],\n",
        "               _scores[2][0], _scores[2][1], _scores[2][2],\n",
        "               _scores[3][0], _scores[3][1], _scores[3][2])\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(9, 9))\n",
        "\n",
        "   # Convert to pandas.Series if they are numpy.ndarray\n",
        "    if isinstance(_y_test, np.ndarray):\n",
        "        _y_test = pd.Series(_y_test)\n",
        "    if isinstance(_y_pred, np.ndarray):\n",
        "        _y_pred = pd.Series(_y_pred)\n",
        "\n",
        "    # Convert to numeric\n",
        "    _y_test = pd.to_numeric(_y_test, errors='coerce')\n",
        "    _y_pred = pd.to_numeric(_y_pred, errors='coerce')\n",
        "\n",
        "    # Drop rows with NaN values\n",
        "    _y_test = _y_test.dropna()\n",
        "    _y_pred = _y_pred.loc[_y_test.index]  # Align _y_pred with _y_test\n",
        "\n",
        "\n",
        "    print(\"_y_test======>\",_y_test)\n",
        "    print(\"_y_test======>\",_y_pred)\n",
        "    _y_test = 10 ** _y_test\n",
        "    _y_pred = 10 ** _y_pred\n",
        "\n",
        "    # Use a more professional color scheme\n",
        "    plt.scatter(_y_pred, _y_test, c='blue', alpha=0.6, label='Testing set')\n",
        "    a, b = min(_y_test.min(), _y_pred.min()), max(_y_test.max(), _y_pred.max())\n",
        "    plt.plot([a, b], [a, b], '-', c='red', linewidth=2.0, label='y = x')\n",
        "\n",
        "    # Add title and labels\n",
        "    plt.title('Parity Plot', fontsize=24, pad=20)\n",
        "    plt.xlabel('Corrosion rate (mm/year) - Predicted', fontsize=20)\n",
        "    plt.ylabel('Corrosion rate (mm/year) - True', fontsize=20)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "\n",
        "    # Add info box\n",
        "    plt.text(0.03, 0.96, info,\n",
        "             ha='left', va='top', transform=ax.transAxes,\n",
        "             fontdict={'color': 'k', 'size': 16},\n",
        "             bbox={'boxstyle': 'round', 'fc': 'snow', 'ec': 'gray', 'pad': 0.5})\n",
        "\n",
        "    plt.legend(loc='upper right', fontsize=16, fancybox=True, shadow=True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('{}/parityPlot.png'.format(_root))\n",
        "    plt.close()\n",
        "\n",
        "    df = pd.DataFrame(columns=['True_value', 'Predicted_value'])\n",
        "    df['True_value'] = _y_test\n",
        "    df['Predicted_value'] = _y_pred\n",
        "    excel_output(df, _root, file_name='parityPlotData', csv=False)\n",
        "\n",
        "def production_plot(df_all, df_selected, _y_pred, _mse_prod, folder_name, y_axis_scale, _exp, _seat_out):\n",
        "    _root = f'regression/postProcessing/{folder_name}{y_axis_scale}'\n",
        "    if not os.path.exists(_root):\n",
        "        os.makedirs(_root)\n",
        "    # ---------------------------------\n",
        "    fig, ax = plt.subplots(1, figsize=(10, 9))\n",
        "    # ---------------------------------\n",
        "    df2 = df_all.copy(deep=True)\n",
        "    df2 = df2.loc[df2['Experiment'] == _exp]\n",
        "    replicas = df2['Description'].unique()\n",
        "    n = 1\n",
        "    for rep in replicas:\n",
        "        df3 = df2.loc[df_all['Description'] == rep]\n",
        "        _X = df3['time_hrs']\n",
        "        _y = 10 ** (df3['corrosion_mm_yr'])\n",
        "        _color, _zorder = 'gray', 0\n",
        "        plt.scatter(_X, _y, c=_color, label=f'Replica {n}', zorder=_zorder)\n",
        "        n += 1\n",
        "    # ---------------------------------\n",
        "\n",
        "    # Plot predictions (dark red color)\n",
        "    df2 = df_selected.copy(deep=True)\n",
        "    df2 = df2.loc[df2['Experiment'] == _exp]\n",
        "    # _X_pred = df2['time_hrs_original']\n",
        "    _X_pred = df2['time_hrs']\n",
        "    _X_pred = _X_pred[:int(len(_X_pred) / len(replicas))]\n",
        "\n",
        "    # Debug: Check lengths of _X_pred and _y_pred\n",
        "    print(f\"_exp: {str(_exp)}\")\n",
        "    print(f\"Length of _X_pred: {len(_X_pred)}\")\n",
        "    print(f\"Length of _y_pred: {len(_y_pred)}\")\n",
        "\n",
        "    # plt.scatter(_X_pred, 10 ** _y_pred, c='darkred', marker='^', s=[75], label='Prediction', zorder=7)\n",
        "    plt.scatter(_X_pred, 10 ** _y_pred, c='#6F2DA8', marker='^', s=[75], label='Prediction', zorder=7, edgecolor='k', linewidth=0.5)\n",
        "    # ---------------------------------\n",
        "    if y_axis_scale == 'Log':\n",
        "        plt.yscale('log')\n",
        "        ax.set_ylim(0.01, 100)\n",
        "    # ---------------------------------\n",
        "    _info = [i for i in _seat_out]\n",
        "    plt.grid(linewidth=0.5)\n",
        "    x_axis_max = 25 # 10 * (1 + int(np.max(_X_pred) / 10))\n",
        "    if _exp in [3,6,9, 11, 13, 17]:\n",
        "        x_axis_max = 25\n",
        "    elif _exp == 14:\n",
        "        x_axis_max = 30\n",
        "    elif _exp == 16:\n",
        "        x_axis_max = 28\n",
        "    x_axis_index = np.linspace(0, x_axis_max, num=6)\n",
        "    ax.set_xticks(x_axis_index)\n",
        "    ax.set_xlim(0, x_axis_max)\n",
        "    ax.set_xticklabels(x_axis_index, fontsize=30)\n",
        "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "    ax.set_xlabel('Time (hr)', fontsize=40, labelpad=20)\n",
        "    plt.yticks(fontsize=30)\n",
        "    ax.set_ylabel('Corrosion rate (mm/yr)', fontsize=40, labelpad=25)\n",
        "    n_col, legend_font_size = 1, 25\n",
        "    if _exp in [10, 14, 29]:\n",
        "        n_col = 2\n",
        "    if _exp == 14:\n",
        "        legend_font_size = 18\n",
        "    leg = plt.legend(loc='upper right', fontsize=legend_font_size, ncol=n_col, fancybox=True, shadow=True)\n",
        "    for handle, text in zip(leg.legendHandles, leg.get_texts()):\n",
        "        text.set_color(handle.get_facecolor()[0])\n",
        "        if text.get_text() == 'Prediction':\n",
        "            text.set_color('#6F2DA8')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{_root}/{_info} exp{_exp} mse({_mse_prod}).png')\n",
        "    plt.close()\n",
        "\n",
        "def sensitivity_plot(df, _exp, y_axis_scale, _feature):\n",
        "    _root = f'regression/sensitivityAnalysis/variation{_exp}{y_axis_scale}'\n",
        "    if not os.path.exists(_root):\n",
        "        os.makedirs(_root)\n",
        "\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "\n",
        "    # Define colors and markers\n",
        "    colors = ['black', 'blue', 'green', 'olive', 'brown', 'purple', 'orange']  # Extended color list\n",
        "    markers = ['o', 'x', '^', 's', 'D', 'P', '*']  # Extended marker list\n",
        "\n",
        "    # Plot each feature variation\n",
        "    _X = df['time_hrs']\n",
        "    for i, column in enumerate(df.columns):\n",
        "        if column == 'time_hrs':\n",
        "            continue\n",
        "        _y = 10 ** (df[column])\n",
        "\n",
        "        # Use edgecolor only for filled markers\n",
        "        if markers[i] in ['o', '^', 's', 'D', 'P', '*']:  # Filled markers\n",
        "            plt.scatter(_X, _y, c=colors[i], marker=markers[i], s=100, label=column, alpha=0.8, edgecolor='k', linewidth=0.5)\n",
        "        else:  # Unfilled markers (e.g., 'x')\n",
        "            plt.scatter(_X, _y, c=colors[i], marker=markers[i], s=100, label=column, alpha=0.8)\n",
        "\n",
        "    # Set y-axis scale\n",
        "    if y_axis_scale == 'Log':\n",
        "        plt.yscale('log')\n",
        "        ax.set_ylim(0.01, 100)\n",
        "\n",
        "    # Add grid and labels\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    x_axis_max = 10 * (1 + int(np.max(_X) / 10))\n",
        "    if _exp == 6:\n",
        "        x_axis_max = 40\n",
        "    elif _exp in [11, 13, 17, 18, 19]:\n",
        "        x_axis_max = 25\n",
        "    elif _exp == 14:\n",
        "        x_axis_max = 30\n",
        "    elif _exp == 16:\n",
        "        x_axis_max = 15\n",
        "\n",
        "    x_axis_index = np.linspace(0, x_axis_max, num=6)\n",
        "    ax.set_xticks(x_axis_index)\n",
        "    ax.set_xlim(0, x_axis_max)\n",
        "    ax.set_xticklabels(x_axis_index, fontsize=18)\n",
        "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "    ax.set_xlabel('Time (hr)', fontsize=22, labelpad=10)\n",
        "    ax.set_ylabel('Corrosion rate (mm/yr)', fontsize=22, labelpad=10)\n",
        "    plt.yticks(fontsize=18)\n",
        "\n",
        "    # Add legend\n",
        "    plt.legend(loc='upper right', fontsize=16, ncol=1, fancybox=True, shadow=True, framealpha=0.9)\n",
        "\n",
        "    # Add title\n",
        "    plt.title(f'Sensitivity Analysis: {_feature} (Experiment {_exp})', fontsize=24, pad=20)\n",
        "\n",
        "    # Save and close\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{_root}/{_feature}.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Save data to Excel\n",
        "    excel_output(df, _root, file_name=_feature, csv=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main (REGRESSION PROBLEM)"
      ],
      "metadata": {
        "id": "t1HQtS-koEcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading data\n",
        "dataAll, n_exp = read_data('SequentialDataInhibitor_large', new=False)\n",
        "\n",
        "# data summary (one-time output)\n",
        "if param['summary']:\n",
        "    summary_data(df=dataAll)\n",
        "\n",
        "\n",
        "# # pre-processing data\n",
        "dataSelected = select_features(dataAll)\n",
        "inhibitor = encode_data(dataSelected)\n",
        "\n",
        "\n",
        "# grid-search to find the best hyper-parameters of each algorithm (one-time output)\n",
        "if param['grid_search']:\n",
        "    root = 'regression/gridSearchModels'\n",
        "    if not os.path.exists(root):\n",
        "        os.makedirs(root)\n",
        "    # ---------------------------------\n",
        "    best_models = {}\n",
        "    df_scores = pd.DataFrame()\n",
        "    for algorithm in ['MLP','CDNN','SVM', 'RF','KNN']:  #\n",
        "        print(algorithm)\n",
        "        algorithms = grid_search(algorithm)\n",
        "        scores, best = compare_models(inhibitor, algorithms)\n",
        "        best_models[algorithm] = best\n",
        "        df_scores['{}_mean'.format(algorithm)], df_scores['{}_std'.format(algorithm)] = scores['mean'], scores['std']\n",
        "        printOut = pd.DataFrame(algorithms)\n",
        "        printOut['mean'], printOut['std'] = [-x for x in scores['mean']], scores['std']\n",
        "        excel_output(printOut, root, file_name='{}'.format(algorithm), csv=False)\n",
        "    models_reg = [\n",
        "                  ('MLP', best_models['MLP']),\n",
        "                  ('CDNN', best_models['CDNN']),\n",
        "                  ('SVM', best_models['SVM']),\n",
        "                  ('RF', best_models['RF']),\n",
        "                  ('KNN', best_models['KNN'])]\n",
        "else:\n",
        "    # The following models with the set hyper-parameters are found after the one time grid searching\n",
        "    models_reg = [\n",
        "                  ('MLP', MLPRegressor(hidden_layer_sizes=(8, 8, 8, 8), max_iter=10000)),\n",
        "                  ('CDNN', KerasRegressor(build_fn=create_complex_deep_learning_model, input_dim=17,\n",
        "                                     hidden_layers=(512, 256, 128), activation='relu',\n",
        "                                     dropout_rate=0.5,\n",
        "                                     epochs=100, batch_size=64, verbose=1)),\n",
        "                  ('SVM', cuSVR(C=1000, gamma=1)),\n",
        "                  ('RF', cuRF(max_features=0.5, n_estimators=300)),\n",
        "                  ('KNN', KNeighborsRegressor(n_neighbors=3, weights='distance'))]\n",
        "\n",
        "# comparing four ML models\n",
        "_best_reg = models_reg[2][1]   ##   _best_reg = models_reg[0][1]\n",
        "if param['compare']:\n",
        "    scores_reg, _best_reg = compare_models(inhibitor, models_reg)\n",
        "    compare_models_box_plot(scores_reg)\n",
        "    excel_output(scores_reg, 'regression/gridSearchModels', file_name='comparison', csv=False)\n",
        "best_reg = _best_reg\n",
        "\n",
        "# features importance\n",
        "if param['importance']:\n",
        "    X, y = split_xy(inhibitor, True)\n",
        "    best_reg.fit(X, y)\n",
        "    feature_importance, permute_importance = importance_plot(inhibitor, best_reg, X, y)\n",
        "\n",
        "# parity plot for 25% of the data\n",
        "if param['parity']:\n",
        "    training_reg, testing_reg = split_data_random(inhibitor)\n",
        "    X_train, y_train = split_xy(training_reg, True)\n",
        "    best_reg.fit(X_train, y_train)\n",
        "    X_test, y_test = split_xy(testing_reg, True)\n",
        "    y_pred = best_reg.predict(X_test)\n",
        "    scores_pred = prediction(inhibitor, best_reg)\n",
        "    parity_plot(y_test, y_pred, scores_pred)\n",
        "    excel_output(X_train, 'regression/bestModelPerformance', file_name='trainFeatureMatrixNorm', csv=False)\n",
        "\n",
        "# testing the model when 4 experiment randomly selected to seat out\n",
        "if param['production']:\n",
        "    experiments = inhibitor['Experiment'].unique()\n",
        "    seatOuts = [[int(i) for i in np.random.choice(a=experiments, size=20, replace=False)]]\n",
        "    for seatOut in seatOuts:\n",
        "        print(seatOut)\n",
        "        training_test, testing_test = split_data_exp(inhibitor, seatOut)\n",
        "        X_train, y_train = split_xy(training_test, True)\n",
        "        best_reg.fit(X_train, y_train)\n",
        "        for exp in seatOut:\n",
        "            testing_temp = testing_test.loc[testing_test['Experiment'] == exp].reset_index(drop=True)\n",
        "            X_test, y_test = split_xy(testing_temp, False)\n",
        "            X_prod, y_prod = production(X_test, testing_temp)\n",
        "            X_prod = X_prod[:int(len(testing_temp))]\n",
        "            mse_prod = 0\n",
        "            for iteration in range(param['iterations']):\n",
        "                y_pred = best_reg.predict(X_prod)\n",
        "                mse_prod += mean_squared_error(y_prod, y_pred)\n",
        "            y_pred = best_reg.predict(X_prod)\n",
        "            mse_prod = mse_prod / param['iterations']\n",
        "            production_plot(dataAll, dataSelected, y_prod, mse_prod, f'testingTheModel/{seatOut}', 'Log', exp, seatOut)\n",
        "            production_plot(dataAll, dataSelected, y_prod, mse_prod, f'testingTheModel/{seatOut}', 'Norm', exp, seatOut)\n",
        "\n",
        "\n",
        "# sensitivity analysis\n",
        "if param['sensitivity']:\n",
        "    experiments = [int(i) for i in inhibitor['Experiment'].unique()]\n",
        "    # experiments = [11]\n",
        "    experiment = [i for i in np.random.choice(a=experiments, size=1, replace=False)]\n",
        "    features_reg = {'CI': [['EC1612A', 'CORR12148SP'], [0.0, 0.0], 'Corrosion inhibitor', 'Inhibitor type', ''],\n",
        "                    'pH': [['Controlled=6', 'Uncontrolled'], [0.0, 0.0], 'pH', 'pH', ''],\n",
        "                    'Brine_Type': [['TH', 'Galapagos'], [0.0, 0.0], 'Brine type', 'Brine type', ''],\n",
        "                    'Pressure_bar_CO2': [[0.5, 5, 12, 50], [4.51, 3.15],\n",
        "                                         'CO2 partial pressure', '$p_{CO2}$', 'bar'],\n",
        "                    'Temperature_C': [[30, 90, 110, 132], [106.69, 19.34],\n",
        "                                      'Temperature', 'T', '$^oC$'],\n",
        "                    'Shear_Pa': [[20, 50, 100, 200, 300], [32.85, 56.01],\n",
        "                                 'Wall shear stress', '$\\u03C4$', 'Pa'],\n",
        "                    'Brine_Ionic_Strength': [[0.5, 1.5, 2.5, 5, 10], [0.87, 0.62],\n",
        "                                             'Brine ionic strength', '$C_{Brine}$', 'M'],\n",
        "                    'concentration_ppm': [[10, 100, 200, 300, 1000], [190.21, 131.99],\n",
        "                                          'CI concentration', '$C_{CI}$', 'ppm']}\n",
        "    for experiment in experiments:\n",
        "        print(experiment)\n",
        "        training_sens, testing_sens = split_data_exp(inhibitor, [experiment])\n",
        "        X_train, y_train = split_xy(inhibitor, True)\n",
        "\n",
        "        y_train = y_train.astype(float)\n",
        "\n",
        "\n",
        "        best_reg.fit(X_train, y_train)\n",
        "        testing_sens, time_sens = sensitivity(dataSelected, testing_sens, experiment)\n",
        "        for key in features_reg:\n",
        "            print(key)\n",
        "            first = True\n",
        "            sensitivity_df = pd.DataFrame(index=range(len(testing_sens)))\n",
        "            sensitivity_df['time_hrs'] = time_sens\n",
        "            for value in features_reg[key][0]:\n",
        "                testing_temp = testing_sens.copy(deep=True)\n",
        "                if first and key in ['CI', 'pH', 'Brine_Type']:\n",
        "                    testing_temp['{}_{}'.format(key, features_reg[key][0][0])] = [1.0] * len(testing_sens)\n",
        "                    testing_temp['{}_{}'.format(key, features_reg[key][0][1])] = [0.0] * len(testing_sens)\n",
        "                    first = False\n",
        "                elif key in ['CI', 'pH', 'Brine_Type']:\n",
        "                    testing_temp['{}_{}'.format(key, features_reg[key][0][0])] = [0.0] * len(testing_sens)\n",
        "                    testing_temp['{}_{}'.format(key, features_reg[key][0][1])] = [1.0] * len(testing_sens)\n",
        "                else:\n",
        "                    key_mean, key_std = np.mean(dataSelected[key]), np.std(dataSelected[key])\n",
        "                    zero_norm = (0 - key_mean) / float(key_std)\n",
        "                    value_norm = (value - key_mean) / float(key_std)\n",
        "                    if key != 'concentration_ppm':\n",
        "                        testing_temp[key] = [value_norm] * len(testing_sens)\n",
        "                    else:\n",
        "                        for v in range(len(testing_temp)):\n",
        "                            if testing_temp.loc[v, 'concentration_ppm'] != 0:\n",
        "                                testing_temp.loc[v, 'concentration_ppm'] = value\n",
        "                X_sens = testing_temp.drop(['Description', 'Experiment', 'corrosion_mm_yr'], axis=1)\n",
        "                y_sens = best_reg.predict(X_sens)\n",
        "                value = 130 if value == 132 else value\n",
        "                if value == 'EC1612A':\n",
        "                    sensitivity_df['{} = {} {}'.format(features_reg[key][3], 'CI-1', features_reg[key][4])] = y_sens\n",
        "                elif value == 'CORR12148SP':\n",
        "                    sensitivity_df['{} = {} {}'.format(features_reg[key][3], 'CI-2', features_reg[key][4])] = y_sens\n",
        "                elif value == 'Controlled=6':\n",
        "                    sensitivity_df['{} = {} {}'.format(features_reg[key][3], 'controlled 6',\n",
        "                                                       features_reg[key][4])] = y_sens\n",
        "                elif value == 'Uncontrolled':\n",
        "                    sensitivity_df['{} = {} {}'.format(features_reg[key][3], 'uncontrolled',\n",
        "                                                       features_reg[key][4])] = y_sens\n",
        "                elif value == 'TH':\n",
        "                    sensitivity_df['{} = {} {}'.format(features_reg[key][3], 'A', features_reg[key][4])] = y_sens\n",
        "                elif value == 'Galapagos':\n",
        "                    sensitivity_df['{} = {} {}'.format(features_reg[key][3], 'B', features_reg[key][4])] = y_sens\n",
        "                else:\n",
        "                    sensitivity_df['{} = {} {}'.format(features_reg[key][3], value, features_reg[key][4])] = y_sens\n",
        "            sensitivity_plot(sensitivity_df, experiment, 'Log', features_reg[key][2])\n",
        "            sensitivity_plot(sensitivity_df, experiment, 'Norm', features_reg[key][2])\n",
        "\n",
        "\n",
        "print('DONE!')"
      ],
      "metadata": {
        "id": "3ukR7QEDoFEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "KzF68hOTlS3T",
        "UHy-jvwrl0x3",
        "gw3OOeKml9nj",
        "Myzj2yqOmZvL",
        "jteuDJ8OnqGf"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}