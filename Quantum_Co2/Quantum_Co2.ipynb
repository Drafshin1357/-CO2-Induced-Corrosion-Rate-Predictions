{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuohVZEFImV-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL29hsuuI5p1",
        "outputId": "4bc83ea2-72a3-4f63-8853-e656365df701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/myDetection/ShapeRecogition/N5/OnePose_Plus_Plus/2024/CO2/NewApproach/Quantum_Data\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/gdrive/MyDrive/myDetection/ShapeRecogition/N5/OnePose_Plus_Plus/2024/CO2/NewApproach/Quantum_Data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqyEBdSFJoqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6251e1-e376-4f0a-a554-e6e97c7bc04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting pennylane\n",
            "  Downloading pennylane-0.42.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting custatevec-cu12\n",
            "  Downloading custatevec_cu12-1.9.0.post0-py3-none-manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting pennylane-lightning[gpu]\n",
            "  Downloading pennylane_lightning-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.0.2)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning[gpu])\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pennylane-lightning-gpu (from pennylane-lightning[gpu])\n",
            "  Downloading pennylane_lightning_gpu-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.1.3)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu->pennylane-lightning[gpu]) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.7.14)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.42.1-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading custatevec_cu12-1.9.0.post0-py3-none-manylinux2014_x86_64.whl (70.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.2/70.2 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m137.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning_gpu-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: custatevec-cu12, appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane, pennylane-lightning-gpu\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 custatevec-cu12-1.9.0.post0 diastatic-malt-2.15.2 pennylane-0.42.1 pennylane-lightning-0.42.0 pennylane-lightning-gpu-0.42.0 rustworkx-0.16.0 scipy-openblas32-0.3.30.0.2\n"
          ]
        }
      ],
      "source": [
        "# !pip install pennylane --upgrade\n",
        "!pip install openpyxl\n",
        "!pip install pennylane pennylane-lightning[gpu] custatevec-cu12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zrFgy2pJdFd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np_pennylane\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyCR1zuSJ_id"
      },
      "outputs": [],
      "source": [
        "from pennylane.optimize import AdamOptimizer\n",
        "@qml.qnode(dev)\n",
        "def quantum_circuit(inputs, weights):\n",
        "    # Feature encoding\n",
        "    for i in range(n_qubits):\n",
        "        qml.RX(np.pi * inputs[i], wires=i)\n",
        "\n",
        "    # Variational layers\n",
        "    for l in range(n_layers):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RZ(weights[l, i, 0], wires=i)\n",
        "            qml.RY(weights[l, i, 1], wires=i)\n",
        "            qml.RZ(weights[l, i, 2], wires=i)\n",
        "        for i in range(n_qubits-1):\n",
        "            qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "    # Measurement\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# 3. QNN Model (Algorithm 3)\n",
        "class QNN:\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Quantum parameters\n",
        "        self.q_weights = np.random.uniform(0, 2*np.pi, (n_layers, n_qubits, 3))\n",
        "\n",
        "        # Classical parameters\n",
        "        self.pre_weights = np.random.randn(n_qubits, n_qubits)\n",
        "        self.pre_bias = np.random.randn(n_qubits)\n",
        "        self.post_weights = np.random.randn(1, n_qubits)\n",
        "        self.post_bias = np.random.randn(1)\n",
        "\n",
        "        # Quantum device\n",
        "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "        # Quantum circuit\n",
        "        @qml.qnode(self.dev, interface=\"autograd\")\n",
        "        def quantum_circuit(inputs, weights):\n",
        "            # Encode input\n",
        "            for i in range(n_qubits):\n",
        "                qml.RY(inputs[i], wires=i)\n",
        "\n",
        "            # Variational layers\n",
        "            for layer in range(n_layers):\n",
        "                for i in range(n_qubits):\n",
        "                    qml.Rot(*weights[layer, i], wires=i)\n",
        "                for i in range(n_qubits-1):\n",
        "                    qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "        self.quantum_circuit = quantum_circuit\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Quantum part\n",
        "        q_out = self.quantum_circuit(x, self.q_weights)\n",
        "\n",
        "        # Classical part (using pennylane.numpy)\n",
        "        hidden = np.tanh(np.dot(self.pre_weights, q_out) + self.pre_bias)\n",
        "        output = np.dot(self.post_weights, hidden) + self.post_bias\n",
        "        return output[0]\n",
        "\n",
        "def train_qnn(X_train, y_train, X_val, y_val, epochs=50, lr=0.005, patience=10):\n",
        "    model = QNN(n_qubits=4, n_layers=2)  # Adjust dimensions as needed\n",
        "    opt = AdamOptimizer(stepsize=lr)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = patience\n",
        "    best_params = None\n",
        "\n",
        "    def cost_fn(params, x, y):\n",
        "        # Unpack all parameters\n",
        "        (q_weights, pre_weights, pre_bias,\n",
        "         post_weights, post_bias) = params\n",
        "\n",
        "        # Quantum part\n",
        "        q_out = model.quantum_circuit(x, q_weights)\n",
        "\n",
        "        # Classical part\n",
        "        hidden = np.tanh(np.dot(pre_weights, q_out) + pre_bias)\n",
        "        pred = np.dot(post_weights, hidden) + post_bias\n",
        "        return (pred[0] - y) ** 2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        train_loss = 0\n",
        "        for x, y in zip(X_train, y_train):\n",
        "            params = (model.q_weights, model.pre_weights, model.pre_bias,\n",
        "                     model.post_weights, model.post_bias)\n",
        "\n",
        "            params, loss = opt.step_and_cost(cost_fn, params, x=x, y=y)\n",
        "\n",
        "            # Update model parameters\n",
        "            (model.q_weights, model.pre_weights, model.pre_bias,\n",
        "             model.post_weights, model.post_bias) = params\n",
        "\n",
        "            train_loss += loss\n",
        "\n",
        "        train_loss /= len(X_train)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = 0\n",
        "        for x, y in zip(X_val, y_val):\n",
        "            pred = model.forward(x)\n",
        "            val_loss += (pred - y) ** 2\n",
        "        val_loss /= len(X_val)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_params = (model.q_weights.copy(), model.pre_weights.copy(),\n",
        "                          model.pre_bias.copy(), model.post_weights.copy(),\n",
        "                          model.post_bias.copy())\n",
        "            patience_counter = patience\n",
        "        else:\n",
        "            patience_counter -= 1\n",
        "            if patience_counter == 0:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "    # Restore best parameters\n",
        "    if best_params is not None:\n",
        "        (model.q_weights, model.pre_weights, model.pre_bias,\n",
        "         model.post_weights, model.post_bias) = best_params\n",
        "\n",
        "    return model\n",
        "\n",
        "# 4. Model Evaluation (Algorithm 4)\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = np.array([model.forward(x) for x in X_test])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(f\"MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, MAE: {mae:.4f}\")\n",
        "    return mse, rmse, r2, mae, y_pred\n",
        "\n",
        "# 5. Data Simulation for Visualization (Algorithm 5)\n",
        "def simulate_data():\n",
        "    C_0 = 15\n",
        "    beta = 0.15\n",
        "    gamma = 0.1\n",
        "    alpha = 0.5\n",
        "    T_0 = 100\n",
        "    P_CO2 = 10.408  # Fixed for Experiment 1 based on your data\n",
        "\n",
        "    time = np.arange(1, 4, 0.165829146)  # Based on your data range\n",
        "    temp = np.arange(104, 117.5, 0.5)\n",
        "    conc = np.arange(100, 310, 10)\n",
        "    shear = np.arange(210, 263, 1)\n",
        "\n",
        "    data_sim = []\n",
        "    for t in time:\n",
        "        for T in temp:\n",
        "            for I in conc:\n",
        "                for S in shear:\n",
        "                    C = C_0 * np.exp(-beta * t) * (1 + gamma * P_CO2) * \\\n",
        "                        (1 + alpha * (T - T_0) / T_0) * (1 - I / (I + 100))\n",
        "                    data_sim.append([t, T, I, S, C])\n",
        "\n",
        "    return pd.DataFrame(data_sim, columns=['time_hrs', 'Temperature_C',\n",
        "                                          'concentration_ppm', 'Shear_Pa', 'corrosion_mm_yr'])\n",
        "\n",
        "# 6. Visualization (Figures 1-3)\n",
        "def plot_results(sim_data, df_actual):\n",
        "    # Figure 1: 3D Surface Plot (Time vs Temperature vs Corrosion Rate)\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf_data = sim_data[sim_data['concentration_ppm'] == 200]  # Match your data\n",
        "    ax.plot_trisurf(surf_data['time_hrs'], surf_data['Temperature_C'],\n",
        "                    surf_data['corrosion_mm_yr'], cmap='Blues', alpha=0.8)\n",
        "    ax.scatter(df_actual['time_hrs'], df_actual['Temperature_C'],\n",
        "               df_actual['corrosion_mm_yr'], color='red', label='Actual Data')\n",
        "    ax.set_xlabel('Time (hrs)')\n",
        "    ax.set_ylabel('Temperature (°C)')\n",
        "    ax.set_zlabel('Corrosion Rate (mm/yr)')\n",
        "    ax.set_title('3D Surface Plot of Corrosion Rate')\n",
        "    ax.legend()\n",
        "    plt.savefig('3d_time_temp.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 2: Logarithmic Scatter Plot (Inhibitor Concentration vs Corrosion Rate)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x='concentration_ppm', y='corrosion_mm_yr',\n",
        "                    data=sim_data[sim_data['concentration_ppm'] == 200],\n",
        "                    label='Simulated Data (200 ppm)', color='blue', marker='s')\n",
        "    sns.scatterplot(x='concentration_ppm', y='corrosion_mm_yr',\n",
        "                    data=df_actual, label='Actual Data (200 ppm)', color='red', marker='o')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Inhibitor Concentration (ppm)')\n",
        "    plt.ylabel('Corrosion Rate (mm/yr)')\n",
        "    plt.title('Logarithmic Scatter Plot of Corrosion Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    plt.savefig('log_concentration.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 3: Bar Plot (Shear Stress vs Average Corrosion Rate)\n",
        "    shear_bins = pd.cut(sim_data['Shear_Pa'], bins=[210, 220, 230, 240, 250, 263],\n",
        "                        labels=['210-220', '220-230', '230-240', '240-250', '250-263'])\n",
        "    shear_avg = sim_data.groupby(shear_bins)['corrosion_mm_yr'].mean()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    shear_avg.plot(kind='bar', color='purple')\n",
        "    plt.xlabel('Shear Stress (Pa)')\n",
        "    plt.ylabel('Average Corrosion Rate (mm/yr)')\n",
        "    plt.title('Average Corrosion Rate by Shear Stress')\n",
        "    plt.grid(True, ls=\"--\")\n",
        "    plt.savefig('shear_bar.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkMWBnMwI_7w",
        "outputId": "cd6ee711-27c9-4291-be22-0aff7bb11512"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pennylane/_grad.py:310: UserWarning: Attempted to differentiate a function with no trainable parameters. If this is unintended, please add trainable parameters via the 'requires_grad' attribute or 'argnum' keyword.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss = 51.0475, Val Loss = 67.8121\n",
            "Early stopping at epoch 10\n",
            "MSE: 46.1713, RMSE: 6.7949, R²: -1.7311, MAE: 5.1780\n",
            "Output plots saved: 3d_time_temp.png, log_concentration.png, shear_bar.png\n",
            "Predictions saved in y_pred\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-42-4197100460.py:206: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  shear_avg = sim_data.groupby(shear_bins)['corrosion_mm_yr'].mean()\n"
          ]
        }
      ],
      "source": [
        "# Replace with your actual data path\n",
        "data_path = 'SequentialDataInhibitor_large.xlsx'\n",
        "\n",
        "# Load actual data for visualization\n",
        "df_actual = pd.read_excel(data_path, engine='openpyxl')\n",
        "# df_actual = pd.read_excel(data_path)\n",
        "\n",
        "# Preprocess data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, preprocessor = preprocess_data(data_path)\n",
        "\n",
        "# Train QNN\n",
        "model = train_qnn(X_train, y_train, X_val, y_val, epochs=50, lr=0.005, patience=10)\n",
        "\n",
        "# Evaluate model\n",
        "mse, rmse, r2, mae, y_pred = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "# Simulate data for visualization\n",
        "sim_data = simulate_data()\n",
        "\n",
        "# Generate plots\n",
        "plot_results(sim_data, df_actual)\n",
        "\n",
        "print(\"Output plots saved: 3d_time_temp.png, log_concentration.png, shear_bar.png\")\n",
        "print(\"Predictions saved in y_pred\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD19BtObM2wh",
        "outputId": "0f443c87-4396-4367-c28a-a7a441bf1c45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(30,)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3wosg9lg76p",
        "outputId": "884704d2-b3ff-4ed9-b587-694638ee4a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU device: lightning.gpu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np_pennylane\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "import time\n",
        "import torch\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Data Preprocessing (Algorithm 1)\n",
        "# Preprocess data with outlier removal and log transformation\n",
        "def preprocess_data(data_path):\n",
        "    print(\"Loading data from Excel sheets...\")\n",
        "    df = pd.concat(pd.read_excel(data_path, sheet_name=None), ignore_index=True)\n",
        "\n",
        "    # Remove outliers based on corrosion_mm_yr\n",
        "    mean = df['corrosion_mm_yr'].mean()\n",
        "    std = df['corrosion_mm_yr'].std()\n",
        "    df = df[(df['corrosion_mm_yr'] >= mean - 3 * std) & (df['corrosion_mm_yr'] <= mean + 3 * std)]\n",
        "\n",
        "    # Apply log transformation to corrosion_mm_yr\n",
        "    df['corrosion_mm_yr'] = np.log1p(df['corrosion_mm_yr'])\n",
        "\n",
        "    # Select features (add pH if numeric)\n",
        "    feature_columns = ['concentration_ppm', 'time_hrs', 'Pressure_bar_CO2', 'Temperature_C', 'Shear_Pa', 'Brine_Ionic_Strength']\n",
        "    if df['pH'].dtype in [np.float64, np.int64]:\n",
        "        feature_columns.append('pH')\n",
        "\n",
        "    X = df[feature_columns].values\n",
        "    y = df['corrosion_mm_yr'].values\n",
        "    df_actual = df.copy()\n",
        "\n",
        "    # Standardize features\n",
        "    preprocessor = StandardScaler()\n",
        "    X = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split data\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.833, random_state=42)\n",
        "\n",
        "    print(f\"Loaded {len(df)} samples after removing invalid entries.\")\n",
        "    print(f\"Preprocessing completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)} samples\")\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, preprocessor, df_actual\n",
        "\n",
        "# 2. Quantum Circuit Construction (Algorithm 2)\n",
        "# n_qubits = 8\n",
        "# n_layers = 3\n",
        "# dev = qml.device('default.qubit', wires=n_qubits)\n",
        "# 2. Quantum Circuit Construction (Algorithm 2)\n",
        "# 2. Quantum Circuit Construction (Algorithm 2)\n",
        "# 2. Quantum Circuit Construction (Algorithm 2)\n",
        "# 2. Quantum Circuit Construction (Algorithm 2)\n",
        "n_qubits = 6\n",
        "n_layers = 3\n",
        "try:\n",
        "    dev = qml.device('lightning.gpu', wires=n_qubits)\n",
        "    print(\"Using GPU device: lightning.gpu\")\n",
        "except Exception as e:\n",
        "    print(f\"GPU device not available: {e}. Falling back to CPU.\")\n",
        "    dev = qml.device('default.qubit', wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_circuit(inputs, weights):\n",
        "    # Feature encoding\n",
        "    for i in range(n_qubits):\n",
        "        qml.RX(np.pi * inputs[i], wires=i)\n",
        "\n",
        "    # Variational layers\n",
        "    for l in range(n_layers):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RZ(weights[l, i, 0], wires=i)\n",
        "            qml.RY(weights[l, i, 1], wires=i)\n",
        "            qml.RZ(weights[l, i, 2], wires=i)\n",
        "        for i in range(n_qubits-1):\n",
        "            qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "    # Measurement\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# 3. QNN Model (Algorithm 3)\n",
        "class QNN(torch.nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers, n_features, n_hidden=16, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        super().__init__()\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_layers = n_layers\n",
        "        self.n_features = n_features\n",
        "        self.n_hidden = n_hidden\n",
        "        self.device = device\n",
        "        # Initialize parameters as torch tensors\n",
        "        self.weights = torch.nn.Parameter(torch.randn(n_layers, n_qubits, 3, device=self.device))\n",
        "        self.pre_weights = torch.nn.Parameter(torch.randn(n_hidden, n_qubits, device=self.device))\n",
        "        self.pre_bias = torch.nn.Parameter(torch.zeros(n_hidden, device=self.device))\n",
        "        self.post_weights = torch.nn.Parameter(torch.randn(1, n_hidden, device=self.device))\n",
        "        self.post_bias = torch.nn.Parameter(torch.zeros(1, device=self.device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure batch dimension\n",
        "        if x.ndim == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        # Ensure input matches n_qubits\n",
        "        if x.shape[1] > self.n_qubits:\n",
        "            x = x[:, :self.n_qubits]\n",
        "        elif x.shape[1] < self.n_qubits:\n",
        "            padding = torch.zeros(x.shape[0], self.n_qubits - x.shape[1], device=self.device)\n",
        "            x = torch.cat([x, padding], dim=1)\n",
        "        # Process batch\n",
        "        q_out = torch.empty(x.shape[0], self.n_qubits, device=self.device)\n",
        "        for i in range(x.shape[0]):\n",
        "            q_out[i] = torch.stack(quantum_circuit(x[i], self.weights))\n",
        "        # Classical layers\n",
        "        hidden = torch.matmul(self.pre_weights, q_out.T).T + self.pre_bias\n",
        "        hidden = torch.tanh(hidden)\n",
        "        output = torch.matmul(self.post_weights, hidden.T).T + self.post_bias\n",
        "        return output.squeeze(-1)\n",
        "\n",
        "def train_qnn(X_train, y_train, X_val, y_val, epochs=50, lr=0.0005, patience=10, batch_size=512):\n",
        "    print(\"Starting QNN training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    model = QNN(n_qubits, n_layers, n_features=X_train.shape[1])\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = patience\n",
        "    best_params = None\n",
        "\n",
        "    # Convert data to torch tensors upfront\n",
        "    X_train_torch = torch.as_tensor(X_train, dtype=torch.float32, device=model.device)\n",
        "    y_train_torch = torch.as_tensor(y_train, dtype=torch.float32, device=model.device)\n",
        "    X_val_torch = torch.as_tensor(X_val, dtype=torch.float32, device=model.device)\n",
        "    y_val_torch = torch.as_tensor(y_val, dtype=torch.float32, device=model.device)\n",
        "\n",
        "    n_batches = len(X_train) // batch_size + (1 if len(X_train) % batch_size else 0)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle data\n",
        "        indices = torch.randperm(len(X_train))\n",
        "        X_train_shuffled = X_train_torch[indices]\n",
        "        y_train_shuffled = y_train_torch[indices]\n",
        "\n",
        "        # Training\n",
        "        train_loss = 0\n",
        "        for batch in range(n_batches):\n",
        "            batch_start_time = time.time()\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = min((batch + 1) * batch_size, len(X_train))\n",
        "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
        "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
        "\n",
        "            def cost_fn():\n",
        "                preds = model.forward(X_batch)\n",
        "                return torch.mean((preds - y_batch) ** 2)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss = cost_fn()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss += loss.item()\n",
        "            print(f\"Epoch {epoch}, Batch {batch + 1}/{n_batches}: Loss = {loss.item():.4f}, Time = {time.time() - batch_start_time:.2f}s\")\n",
        "\n",
        "        train_loss /= n_batches\n",
        "\n",
        "        # Validation\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            preds = model.forward(X_val_torch)\n",
        "            val_loss = torch.mean((preds - y_val_torch) ** 2).item()\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_params = {k: v.clone().detach() for k, v in model.state_dict().items()}\n",
        "            patience_counter = patience\n",
        "        else:\n",
        "            patience_counter -= 1\n",
        "            if patience_counter == 0:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Time = {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    # Restore best parameters\n",
        "    model.load_state_dict(best_params)\n",
        "    print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return model\n",
        "\n",
        "# Function to analyze data distribution\n",
        "def analyze_data(data_path):\n",
        "    print(\"Analyzing data distribution...\")\n",
        "    df = pd.concat(pd.read_excel(data_path, sheet_name=None), ignore_index=True)\n",
        "    # Select only numeric columns for correlation\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    print(\"Data summary:\")\n",
        "    print(df[numeric_cols].describe())\n",
        "    print(\"\\nCorrelation with corrosion_mm_yr:\")\n",
        "    print(df[numeric_cols].corr()['corrosion_mm_yr'].sort_values())\n",
        "    print(\"\\nMissing values:\")\n",
        "    print(df.isnull().sum())\n",
        "    # Check for outliers in corrosion_mm_yr\n",
        "    mean = df['corrosion_mm_yr'].mean()\n",
        "    std = df['corrosion_mm_yr'].std()\n",
        "    outliers = df[(df['corrosion_mm_yr'] > mean + 3 * std) | (df['corrosion_mm_yr'] < mean - 3 * std)]\n",
        "    print(f\"\\nOutliers in corrosion_mm_yr (>{mean + 3 * std:.2f} or <{mean - 3 * std:.2f}):\")\n",
        "    print(f\"Number of outliers: {len(outliers)}\")\n",
        "    return df\n",
        "\n",
        "# Update evaluate_model for vectorized input\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    X_test_torch = torch.tensor(X_test, dtype=torch.float32, device=model.device)\n",
        "    y_test_torch = torch.tensor(y_test, dtype=torch.float32, device=model.device)\n",
        "    with torch.no_grad():\n",
        "        y_pred = model.forward(X_test_torch).cpu().numpy()\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(f\"QNN Test Results: MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, MAE: {mae:.4f}\")\n",
        "    return mse, rmse, r2, mae, y_pred\n",
        "\n",
        "\n",
        "# 5. Data Simulation for Visualization (Algorithm 5)\n",
        "def simulate_data():\n",
        "    C_0 = 15\n",
        "    beta = 0.15\n",
        "    gamma = 0.1\n",
        "    alpha = 0.5\n",
        "    T_0 = 100\n",
        "    P_CO2 = 10.408  # Fixed for Experiment 1\n",
        "\n",
        "    time = np.arange(1, 4, 0.165829146)\n",
        "    temp = np.arange(104, 117.5, 0.5)\n",
        "    conc = np.arange(100, 310, 10)\n",
        "    shear = np.arange(210, 263, 1)\n",
        "\n",
        "    data_sim = []\n",
        "    for t in time:\n",
        "        for T in temp:\n",
        "            for I in conc:\n",
        "                for S in shear:\n",
        "                    C = C_0 * np.exp(-beta * t) * (1 + gamma * P_CO2) * \\\n",
        "                        (1 + alpha * (T - T_0) / T_0) * (1 - I / (I + 100))\n",
        "                    data_sim.append([t, T, I, S, C])\n",
        "\n",
        "    return pd.DataFrame(data_sim, columns=['time_hrs', 'Temperature_C',\n",
        "                                          'concentration_ppm', 'Shear_Pa', 'corrosion_mm_yr'])\n",
        "\n",
        "# 6. Visualization (Figures 1-3)\n",
        "def plot_results(df_actual, sim_data=None):\n",
        "    print(\"Generating plots...\")\n",
        "    # Subsample actual data for visualization to avoid overcrowding\n",
        "    df_sample = df_actual.sample(n=min(1000, len(df_actual)), random_state=42)\n",
        "\n",
        "    # Figure 1: 3D Surface Plot (Time vs Temperature vs Corrosion Rate)\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    if sim_data is not None:\n",
        "        surf_data = sim_data[sim_data['concentration_ppm'] == 200]\n",
        "        ax.plot_trisurf(surf_data['time_hrs'], surf_data['Temperature_C'],\n",
        "                        surf_data['corrosion_mm_yr'], cmap='Blues', alpha=0.8, label='Simulated')\n",
        "    ax.scatter(df_sample['time_hrs'], df_sample['Temperature_C'],\n",
        "               df_sample['corrosion_mm_yr'], color='red', label='Actual Data')\n",
        "    ax.set_xlabel('Time (hrs)')\n",
        "    ax.set_ylabel('Temperature (°C)')\n",
        "    ax.set_zlabel('Corrosion Rate (mm/yr)')\n",
        "    ax.set_title('3D Surface Plot of Corrosion Rate')\n",
        "    ax.legend()\n",
        "    plt.savefig('3d_time_temp.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 2: Logarithmic Scatter Plot (Inhibitor Concentration vs Corrosion Rate)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    if sim_data is not None:\n",
        "        sns.scatterplot(x='concentration_ppm', y='corrosion_mm_yr',\n",
        "                        data=sim_data[sim_data['concentration_ppm'] == 200],\n",
        "                        label='Simulated Data (200 ppm)', color='blue', marker='s')\n",
        "    sns.scatterplot(x='concentration_ppm', y='corrosion_mm_yr',\n",
        "                    data=df_sample, label='Actual Data', color='red', marker='o')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Inhibitor Concentration (ppm)')\n",
        "    plt.ylabel('Corrosion Rate (mm/yr)')\n",
        "    plt.title('Logarithmic Scatter Plot of Corrosion Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    plt.savefig('log_concentration.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 3: Bar Plot (Shear Stress vs Average Corrosion Rate)\n",
        "    shear_bins = pd.cut(df_sample['Shear_Pa'], bins=[210, 220, 230, 240, 250, 263],\n",
        "                        labels=['210-220', '220-230', '230-240', '240-250', '250-263'])\n",
        "    shear_avg = df_sample.groupby(shear_bins)['corrosion_mm_yr'].mean()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    shear_avg.plot(kind='bar', color='purple')\n",
        "    plt.xlabel('Shear Stress (Pa)')\n",
        "    plt.ylabel('Average Corrosion Rate (mm/yr)')\n",
        "    plt.title('Average Corrosion Rate by Shear Stress')\n",
        "    plt.grid(True, ls=\"--\")\n",
        "    plt.savefig('shear_bar.png')\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m2XE3wnhHuS",
        "outputId": "470b53f7-2165-420a-c8be-61d97f8a5ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing data distribution...\n",
            "Data summary:\n",
            "         Experiment       Replica  concentration_ppm      time_hrs  \\\n",
            "count  15400.000000  15400.000000       15400.000000  15400.000000   \n",
            "mean      12.831169      2.714286         244.935065     19.980519   \n",
            "std        6.042047      1.536255         179.185937     12.618280   \n",
            "min        1.000000      1.000000          10.000000      1.000000   \n",
            "25%        8.000000      1.000000         100.000000      9.618090   \n",
            "50%       14.000000      2.000000         200.000000     18.286432   \n",
            "75%       18.000000      4.000000         500.000000     29.080402   \n",
            "max       22.000000      6.000000         500.000000     59.000000   \n",
            "\n",
            "       time_hrs_original  Pressure_bar_CO2  Temperature_C      Shear_Pa  \\\n",
            "count       15400.000000      15400.000000   15400.000000  15400.000000   \n",
            "mean           19.980519          6.699337     103.081152    157.629989   \n",
            "std            12.618280          3.313848      14.046774     71.886944   \n",
            "min             1.000000          0.633661      80.358610     23.510453   \n",
            "25%             9.618090          3.988765      92.157923     95.067783   \n",
            "50%            18.286432          6.622724     101.742489    158.006339   \n",
            "75%            29.080402          9.646998     115.720700    216.129332   \n",
            "max            59.000000         11.968530     127.984546    275.769310   \n",
            "\n",
            "       Brine_Ionic_Strength  corrosion_mm_yr  \n",
            "count          15400.000000     15400.000000  \n",
            "mean               1.244026         3.645974  \n",
            "std                0.852659         4.934856  \n",
            "min                0.510000         0.002273  \n",
            "25%                0.510000         0.268124  \n",
            "50%                0.615000         1.193541  \n",
            "75%                2.310000         5.082317  \n",
            "max                2.310000        19.593556  \n",
            "\n",
            "Correlation with corrosion_mm_yr:\n",
            "time_hrs               -7.235507e-01\n",
            "time_hrs_original      -7.235507e-01\n",
            "concentration_ppm      -1.080024e-02\n",
            "Pressure_bar_CO2       -2.916716e-03\n",
            "Shear_Pa                3.964973e-07\n",
            "Brine_Ionic_Strength    3.025443e-03\n",
            "Temperature_C           5.305063e-03\n",
            "Experiment              2.619245e-02\n",
            "Replica                 3.024453e-02\n",
            "corrosion_mm_yr         1.000000e+00\n",
            "Name: corrosion_mm_yr, dtype: float64\n",
            "\n",
            "Missing values:\n",
            "Description             0\n",
            "Experiment              0\n",
            "Replica                 0\n",
            "concentration_ppm       0\n",
            "time_hrs                0\n",
            "time_hrs_original       0\n",
            "Pressure_bar_CO2        0\n",
            "Temperature_C           0\n",
            "CI                      0\n",
            "Shear_Pa                0\n",
            "Brine_Ionic_Strength    0\n",
            "pH                      0\n",
            "Brine_Type              0\n",
            "Type_of_test            0\n",
            "Lab                     0\n",
            "corrosion_mm_yr         0\n",
            "dtype: int64\n",
            "\n",
            "Outliers in corrosion_mm_yr (>18.45 or <-11.16):\n",
            "Number of outliers: 115\n",
            "Loading data from Excel sheets...\n",
            "Loaded 15285 samples after removing invalid entries.\n",
            "Preprocessing completed in 4.75 seconds.\n",
            "Train: 10699, Validation: 765, Test: 3821 samples\n",
            "Starting QNN training...\n",
            "Epoch 0, Batch 1/21: Loss = 2.7120, Time = 57.48s\n",
            "Epoch 0, Batch 2/21: Loss = 2.8595, Time = 58.18s\n",
            "Epoch 0, Batch 3/21: Loss = 2.4334, Time = 56.91s\n",
            "Epoch 0, Batch 4/21: Loss = 2.8596, Time = 57.36s\n",
            "Epoch 0, Batch 5/21: Loss = 2.5633, Time = 56.74s\n",
            "Epoch 0, Batch 6/21: Loss = 2.4371, Time = 56.81s\n",
            "Epoch 0, Batch 7/21: Loss = 2.4623, Time = 56.96s\n",
            "Epoch 0, Batch 8/21: Loss = 2.4988, Time = 56.92s\n",
            "Epoch 0, Batch 9/21: Loss = 2.5178, Time = 56.03s\n",
            "Epoch 0, Batch 10/21: Loss = 2.6543, Time = 56.49s\n",
            "Epoch 0, Batch 11/21: Loss = 2.4125, Time = 56.08s\n",
            "Epoch 0, Batch 12/21: Loss = 2.4364, Time = 56.87s\n",
            "Epoch 0, Batch 13/21: Loss = 2.3128, Time = 56.44s\n",
            "Epoch 0, Batch 14/21: Loss = 2.4053, Time = 57.35s\n",
            "Epoch 0, Batch 15/21: Loss = 2.4818, Time = 57.00s\n",
            "Epoch 0, Batch 16/21: Loss = 2.3483, Time = 57.27s\n",
            "Epoch 0, Batch 17/21: Loss = 2.5053, Time = 56.86s\n",
            "Epoch 0, Batch 18/21: Loss = 2.4340, Time = 56.82s\n",
            "Epoch 0, Batch 19/21: Loss = 2.6317, Time = 57.03s\n",
            "Epoch 0, Batch 20/21: Loss = 2.2652, Time = 56.50s\n",
            "Epoch 0, Batch 21/21: Loss = 2.4624, Time = 51.29s\n",
            "Epoch 0: Train Loss = 2.5092, Val Loss = 2.2371, Time = 1213.67s\n",
            "Epoch 1, Batch 1/21: Loss = 2.2857, Time = 57.72s\n",
            "Epoch 1, Batch 2/21: Loss = 2.2498, Time = 57.55s\n",
            "Epoch 1, Batch 3/21: Loss = 2.3276, Time = 57.55s\n",
            "Epoch 1, Batch 4/21: Loss = 2.1253, Time = 57.41s\n",
            "Epoch 1, Batch 5/21: Loss = 2.4284, Time = 56.30s\n",
            "Epoch 1, Batch 6/21: Loss = 2.2361, Time = 56.63s\n",
            "Epoch 1, Batch 7/21: Loss = 2.0578, Time = 56.40s\n",
            "Epoch 1, Batch 8/21: Loss = 2.2256, Time = 56.70s\n",
            "Epoch 1, Batch 9/21: Loss = 2.5571, Time = 56.39s\n",
            "Epoch 1, Batch 10/21: Loss = 1.8957, Time = 56.80s\n",
            "Epoch 1, Batch 11/21: Loss = 2.0414, Time = 56.50s\n",
            "Epoch 1, Batch 12/21: Loss = 2.0817, Time = 56.69s\n",
            "Epoch 1, Batch 13/21: Loss = 2.2943, Time = 56.42s\n",
            "Epoch 1, Batch 14/21: Loss = 2.3124, Time = 56.87s\n",
            "Epoch 1, Batch 15/21: Loss = 2.2027, Time = 56.44s\n",
            "Epoch 1, Batch 16/21: Loss = 2.0100, Time = 56.54s\n",
            "Epoch 1, Batch 17/21: Loss = 2.0736, Time = 57.31s\n",
            "Epoch 1, Batch 18/21: Loss = 1.8006, Time = 56.38s\n",
            "Epoch 1, Batch 19/21: Loss = 2.0420, Time = 56.48s\n",
            "Epoch 1, Batch 20/21: Loss = 2.1853, Time = 56.79s\n",
            "Epoch 1, Batch 21/21: Loss = 2.1012, Time = 50.69s\n",
            "Epoch 1: Train Loss = 2.1683, Val Loss = 1.9332, Time = 2424.16s\n",
            "Epoch 2, Batch 1/21: Loss = 2.1145, Time = 56.90s\n",
            "Epoch 2, Batch 2/21: Loss = 2.1513, Time = 56.57s\n",
            "Epoch 2, Batch 3/21: Loss = 2.0433, Time = 57.04s\n",
            "Epoch 2, Batch 4/21: Loss = 1.9462, Time = 58.12s\n",
            "Epoch 2, Batch 5/21: Loss = 2.0092, Time = 58.43s\n",
            "Epoch 2, Batch 6/21: Loss = 1.9666, Time = 57.67s\n",
            "Epoch 2, Batch 7/21: Loss = 1.8229, Time = 56.50s\n",
            "Epoch 2, Batch 8/21: Loss = 1.6451, Time = 57.04s\n",
            "Epoch 2, Batch 9/21: Loss = 1.8437, Time = 56.34s\n",
            "Epoch 2, Batch 10/21: Loss = 1.8723, Time = 56.76s\n",
            "Epoch 2, Batch 11/21: Loss = 2.0372, Time = 56.60s\n",
            "Epoch 2, Batch 12/21: Loss = 1.9057, Time = 57.23s\n",
            "Epoch 2, Batch 13/21: Loss = 1.8966, Time = 56.74s\n",
            "Epoch 2, Batch 14/21: Loss = 1.7568, Time = 56.85s\n",
            "Epoch 2, Batch 15/21: Loss = 1.9393, Time = 57.99s\n",
            "Epoch 2, Batch 16/21: Loss = 1.6153, Time = 57.10s\n",
            "Epoch 2, Batch 17/21: Loss = 1.6779, Time = 57.01s\n",
            "Epoch 2, Batch 18/21: Loss = 1.9987, Time = 56.94s\n",
            "Epoch 2, Batch 19/21: Loss = 1.8555, Time = 56.74s\n",
            "Epoch 2, Batch 20/21: Loss = 1.7455, Time = 57.30s\n",
            "Epoch 2, Batch 21/21: Loss = 1.6975, Time = 51.16s\n",
            "Epoch 2: Train Loss = 1.8829, Val Loss = 1.6792, Time = 3641.21s\n",
            "Epoch 3, Batch 1/21: Loss = 1.6096, Time = 57.13s\n",
            "Epoch 3, Batch 2/21: Loss = 1.8338, Time = 56.76s\n",
            "Epoch 3, Batch 3/21: Loss = 1.6565, Time = 56.72s\n",
            "Epoch 3, Batch 4/21: Loss = 1.8469, Time = 57.42s\n",
            "Epoch 3, Batch 5/21: Loss = 1.5188, Time = 56.75s\n",
            "Epoch 3, Batch 6/21: Loss = 1.7047, Time = 56.77s\n",
            "Epoch 3, Batch 7/21: Loss = 1.6223, Time = 57.24s\n",
            "Epoch 3, Batch 8/21: Loss = 1.7843, Time = 56.92s\n",
            "Epoch 3, Batch 9/21: Loss = 1.6492, Time = 57.43s\n",
            "Epoch 3, Batch 10/21: Loss = 1.6800, Time = 56.71s\n",
            "Epoch 3, Batch 11/21: Loss = 1.7173, Time = 57.46s\n",
            "Epoch 3, Batch 12/21: Loss = 1.5743, Time = 56.82s\n",
            "Epoch 3, Batch 13/21: Loss = 1.7095, Time = 56.75s\n",
            "Epoch 3, Batch 14/21: Loss = 1.6463, Time = 57.24s\n",
            "Epoch 3, Batch 15/21: Loss = 1.6205, Time = 56.87s\n",
            "Epoch 3, Batch 16/21: Loss = 1.5667, Time = 56.81s\n",
            "Epoch 3, Batch 17/21: Loss = 1.6202, Time = 57.37s\n",
            "Epoch 3, Batch 18/21: Loss = 1.6454, Time = 56.99s\n",
            "Epoch 3, Batch 19/21: Loss = 1.4315, Time = 57.59s\n",
            "Epoch 3, Batch 20/21: Loss = 1.4637, Time = 56.68s\n",
            "Epoch 3, Batch 21/21: Loss = 1.6786, Time = 51.10s\n",
            "Epoch 3: Train Loss = 1.6467, Val Loss = 1.4731, Time = 4856.71s\n",
            "Epoch 4, Batch 1/21: Loss = 1.4666, Time = 57.24s\n",
            "Epoch 4, Batch 2/21: Loss = 1.5575, Time = 57.11s\n",
            "Epoch 4, Batch 3/21: Loss = 1.4414, Time = 57.33s\n",
            "Epoch 4, Batch 4/21: Loss = 1.3785, Time = 56.88s\n",
            "Epoch 4, Batch 5/21: Loss = 1.3735, Time = 57.45s\n",
            "Epoch 4, Batch 6/21: Loss = 1.4858, Time = 56.71s\n",
            "Epoch 4, Batch 7/21: Loss = 1.4196, Time = 57.19s\n",
            "Epoch 4, Batch 8/21: Loss = 1.3714, Time = 57.04s\n",
            "Epoch 4, Batch 9/21: Loss = 1.3247, Time = 56.99s\n",
            "Epoch 4, Batch 10/21: Loss = 1.5191, Time = 56.65s\n",
            "Epoch 4, Batch 11/21: Loss = 1.5396, Time = 56.88s\n",
            "Epoch 4, Batch 12/21: Loss = 1.6805, Time = 57.09s\n",
            "Epoch 4, Batch 13/21: Loss = 1.5728, Time = 56.96s\n",
            "Epoch 4, Batch 14/21: Loss = 1.3797, Time = 57.19s\n",
            "Epoch 4, Batch 15/21: Loss = 1.6365, Time = 56.95s\n",
            "Epoch 4, Batch 16/21: Loss = 1.5181, Time = 57.18s\n",
            "Epoch 4, Batch 17/21: Loss = 1.3342, Time = 56.91s\n",
            "Epoch 4, Batch 18/21: Loss = 1.2644, Time = 57.31s\n",
            "Epoch 4, Batch 19/21: Loss = 1.4051, Time = 56.85s\n",
            "Epoch 4, Batch 20/21: Loss = 1.3206, Time = 57.27s\n",
            "Epoch 4, Batch 21/21: Loss = 1.5501, Time = 51.18s\n",
            "Epoch 4: Train Loss = 1.4543, Val Loss = 1.3072, Time = 6073.15s\n",
            "Epoch 5, Batch 1/21: Loss = 1.4234, Time = 57.13s\n",
            "Epoch 5, Batch 2/21: Loss = 1.4194, Time = 56.82s\n",
            "Epoch 5, Batch 3/21: Loss = 1.3385, Time = 56.69s\n",
            "Epoch 5, Batch 4/21: Loss = 1.3107, Time = 57.09s\n",
            "Epoch 5, Batch 5/21: Loss = 1.3650, Time = 56.76s\n",
            "Epoch 5, Batch 6/21: Loss = 1.4326, Time = 57.39s\n",
            "Epoch 5, Batch 7/21: Loss = 1.3125, Time = 57.21s\n",
            "Epoch 5, Batch 8/21: Loss = 1.3276, Time = 56.93s\n",
            "Epoch 5, Batch 9/21: Loss = 1.3008, Time = 57.45s\n",
            "Epoch 5, Batch 10/21: Loss = 1.3471, Time = 57.06s\n",
            "Epoch 5, Batch 11/21: Loss = 1.1621, Time = 56.89s\n",
            "Epoch 5, Batch 12/21: Loss = 1.1839, Time = 57.37s\n",
            "Epoch 5, Batch 13/21: Loss = 1.2314, Time = 56.95s\n",
            "Epoch 5, Batch 14/21: Loss = 1.2908, Time = 57.48s\n",
            "Epoch 5, Batch 15/21: Loss = 1.4542, Time = 56.80s\n",
            "Epoch 5, Batch 16/21: Loss = 1.1856, Time = 56.86s\n",
            "Epoch 5, Batch 17/21: Loss = 1.2221, Time = 57.15s\n",
            "Epoch 5, Batch 18/21: Loss = 1.3286, Time = 56.83s\n",
            "Epoch 5, Batch 19/21: Loss = 1.1294, Time = 56.76s\n",
            "Epoch 5, Batch 20/21: Loss = 1.1942, Time = 57.11s\n",
            "Epoch 5, Batch 21/21: Loss = 1.3519, Time = 50.76s\n",
            "Epoch 5: Train Loss = 1.3006, Val Loss = 1.1737, Time = 7288.69s\n",
            "Epoch 6, Batch 1/21: Loss = 1.1709, Time = 57.24s\n",
            "Epoch 6, Batch 2/21: Loss = 1.2827, Time = 56.54s\n",
            "Epoch 6, Batch 3/21: Loss = 1.2503, Time = 57.03s\n",
            "Epoch 6, Batch 4/21: Loss = 1.2936, Time = 56.57s\n",
            "Epoch 6, Batch 5/21: Loss = 1.1848, Time = 56.49s\n",
            "Epoch 6, Batch 6/21: Loss = 1.3569, Time = 57.27s\n",
            "Epoch 6, Batch 7/21: Loss = 1.0463, Time = 56.61s\n",
            "Epoch 6, Batch 8/21: Loss = 1.2849, Time = 56.75s\n",
            "Epoch 6, Batch 9/21: Loss = 1.2778, Time = 56.96s\n",
            "Epoch 6, Batch 10/21: Loss = 1.0893, Time = 57.21s\n",
            "Epoch 6, Batch 11/21: Loss = 1.1952, Time = 56.95s\n",
            "Epoch 6, Batch 12/21: Loss = 1.0644, Time = 57.46s\n",
            "Epoch 6, Batch 13/21: Loss = 1.2377, Time = 56.71s\n",
            "Epoch 6, Batch 14/21: Loss = 1.0922, Time = 56.87s\n",
            "Epoch 6, Batch 15/21: Loss = 1.2520, Time = 57.63s\n",
            "Epoch 6, Batch 16/21: Loss = 1.1009, Time = 57.01s\n",
            "Epoch 6, Batch 17/21: Loss = 1.1618, Time = 57.11s\n",
            "Epoch 6, Batch 18/21: Loss = 1.1392, Time = 57.52s\n",
            "Epoch 6, Batch 19/21: Loss = 1.1436, Time = 57.23s\n",
            "Epoch 6, Batch 20/21: Loss = 0.9911, Time = 58.26s\n",
            "Epoch 6, Batch 21/21: Loss = 1.1069, Time = 52.00s\n",
            "Epoch 6: Train Loss = 1.1773, Val Loss = 1.0700, Time = 8506.17s\n",
            "Epoch 7, Batch 1/21: Loss = 1.2446, Time = 57.92s\n",
            "Epoch 7, Batch 2/21: Loss = 1.0572, Time = 57.89s\n",
            "Epoch 7, Batch 3/21: Loss = 1.1165, Time = 57.84s\n",
            "Epoch 7, Batch 4/21: Loss = 0.9930, Time = 58.41s\n",
            "Epoch 7, Batch 5/21: Loss = 1.1069, Time = 57.12s\n",
            "Epoch 7, Batch 6/21: Loss = 1.1893, Time = 56.96s\n",
            "Epoch 7, Batch 7/21: Loss = 1.2283, Time = 57.29s\n",
            "Epoch 7, Batch 8/21: Loss = 1.0993, Time = 57.98s\n",
            "Epoch 7, Batch 9/21: Loss = 1.0829, Time = 58.02s\n",
            "Epoch 7, Batch 10/21: Loss = 1.0456, Time = 57.41s\n",
            "Epoch 7, Batch 11/21: Loss = 1.1100, Time = 57.87s\n",
            "Epoch 7, Batch 12/21: Loss = 1.1150, Time = 58.21s\n",
            "Epoch 7, Batch 13/21: Loss = 1.1417, Time = 58.33s\n",
            "Epoch 7, Batch 14/21: Loss = 0.9814, Time = 58.59s\n",
            "Epoch 7, Batch 15/21: Loss = 1.0421, Time = 58.03s\n",
            "Epoch 7, Batch 16/21: Loss = 0.9229, Time = 57.22s\n",
            "Epoch 7, Batch 17/21: Loss = 1.0365, Time = 56.82s\n",
            "Epoch 7, Batch 18/21: Loss = 1.0874, Time = 57.70s\n",
            "Epoch 7, Batch 19/21: Loss = 1.0247, Time = 57.95s\n",
            "Epoch 7, Batch 20/21: Loss = 1.0410, Time = 57.11s\n",
            "Epoch 7, Batch 21/21: Loss = 1.0649, Time = 51.51s\n",
            "Epoch 7: Train Loss = 1.0825, Val Loss = 0.9910, Time = 9736.39s\n",
            "Epoch 8, Batch 1/21: Loss = 1.0377, Time = 56.94s\n",
            "Epoch 8, Batch 2/21: Loss = 0.9936, Time = 57.80s\n",
            "Epoch 8, Batch 3/21: Loss = 0.9941, Time = 56.79s\n",
            "Epoch 8, Batch 4/21: Loss = 1.0672, Time = 56.99s\n",
            "Epoch 8, Batch 5/21: Loss = 1.0883, Time = 56.82s\n",
            "Epoch 8, Batch 6/21: Loss = 1.0092, Time = 57.53s\n",
            "Epoch 8, Batch 7/21: Loss = 0.9495, Time = 57.03s\n",
            "Epoch 8, Batch 8/21: Loss = 1.1826, Time = 56.87s\n",
            "Epoch 8, Batch 9/21: Loss = 0.9033, Time = 57.38s\n",
            "Epoch 8, Batch 10/21: Loss = 1.0683, Time = 57.16s\n",
            "Epoch 8, Batch 11/21: Loss = 1.0360, Time = 58.27s\n",
            "Epoch 8, Batch 12/21: Loss = 0.8546, Time = 57.84s\n",
            "Epoch 8, Batch 13/21: Loss = 1.0161, Time = 57.61s\n",
            "Epoch 8, Batch 14/21: Loss = 0.9560, Time = 57.96s\n",
            "Epoch 8, Batch 15/21: Loss = 0.9854, Time = 57.52s\n",
            "Epoch 8, Batch 16/21: Loss = 1.0563, Time = 57.91s\n",
            "Epoch 8, Batch 17/21: Loss = 1.0357, Time = 57.59s\n",
            "Epoch 8, Batch 18/21: Loss = 0.9781, Time = 57.62s\n",
            "Epoch 8, Batch 19/21: Loss = 0.9241, Time = 58.04s\n",
            "Epoch 8, Batch 20/21: Loss = 1.0560, Time = 57.73s\n",
            "Epoch 8, Batch 21/21: Loss = 0.9971, Time = 51.98s\n",
            "Epoch 8: Train Loss = 1.0090, Val Loss = 0.9328, Time = 10962.57s\n",
            "Epoch 9, Batch 1/21: Loss = 1.0438, Time = 57.49s\n",
            "Epoch 9, Batch 2/21: Loss = 0.9208, Time = 57.85s\n",
            "Epoch 9, Batch 3/21: Loss = 0.9808, Time = 57.32s\n",
            "Epoch 9, Batch 4/21: Loss = 0.9406, Time = 58.10s\n",
            "Epoch 9, Batch 5/21: Loss = 0.9356, Time = 57.75s\n",
            "Epoch 9, Batch 6/21: Loss = 1.0291, Time = 57.50s\n",
            "Epoch 9, Batch 7/21: Loss = 0.9034, Time = 58.22s\n",
            "Epoch 9, Batch 8/21: Loss = 1.0600, Time = 57.95s\n",
            "Epoch 9, Batch 9/21: Loss = 0.9812, Time = 58.17s\n",
            "Epoch 9, Batch 10/21: Loss = 0.9585, Time = 57.61s\n",
            "Epoch 9, Batch 11/21: Loss = 0.9769, Time = 58.05s\n",
            "Epoch 9, Batch 12/21: Loss = 1.0091, Time = 57.78s\n",
            "Epoch 9, Batch 13/21: Loss = 0.9915, Time = 57.14s\n",
            "Epoch 9, Batch 14/21: Loss = 0.9291, Time = 58.03s\n",
            "Epoch 9, Batch 15/21: Loss = 0.8956, Time = 57.76s\n",
            "Epoch 9, Batch 16/21: Loss = 0.9425, Time = 58.10s\n",
            "Epoch 9, Batch 17/21: Loss = 0.9267, Time = 57.50s\n",
            "Epoch 9, Batch 18/21: Loss = 0.9425, Time = 57.36s\n",
            "Epoch 9, Batch 19/21: Loss = 0.8501, Time = 58.12s\n",
            "Epoch 9, Batch 20/21: Loss = 0.9110, Time = 57.56s\n",
            "Epoch 9, Batch 21/21: Loss = 0.9164, Time = 52.24s\n",
            "Epoch 9: Train Loss = 0.9545, Val Loss = 0.8891, Time = 12194.82s\n",
            "Epoch 10, Batch 1/21: Loss = 0.9532, Time = 57.38s\n",
            "Epoch 10, Batch 2/21: Loss = 0.8280, Time = 57.83s\n",
            "Epoch 10, Batch 3/21: Loss = 0.8443, Time = 57.65s\n",
            "Epoch 10, Batch 4/21: Loss = 0.9077, Time = 58.17s\n",
            "Epoch 10, Batch 5/21: Loss = 0.9058, Time = 57.59s\n",
            "Epoch 10, Batch 6/21: Loss = 0.9202, Time = 57.52s\n",
            "Epoch 10, Batch 7/21: Loss = 0.9294, Time = 58.07s\n",
            "Epoch 10, Batch 8/21: Loss = 0.9074, Time = 57.09s\n",
            "Epoch 10, Batch 9/21: Loss = 0.9336, Time = 57.72s\n",
            "Epoch 10, Batch 10/21: Loss = 0.8968, Time = 57.66s\n",
            "Epoch 10, Batch 11/21: Loss = 0.9198, Time = 58.24s\n",
            "Epoch 10, Batch 12/21: Loss = 0.9417, Time = 57.96s\n",
            "Epoch 10, Batch 13/21: Loss = 0.9732, Time = 57.83s\n",
            "Epoch 10, Batch 14/21: Loss = 0.8554, Time = 58.34s\n",
            "Epoch 10, Batch 15/21: Loss = 0.8699, Time = 57.83s\n",
            "Epoch 10, Batch 16/21: Loss = 0.9038, Time = 58.20s\n",
            "Epoch 10, Batch 17/21: Loss = 0.8807, Time = 57.85s\n",
            "Epoch 10, Batch 18/21: Loss = 0.9660, Time = 57.61s\n",
            "Epoch 10, Batch 19/21: Loss = 0.9187, Time = 58.18s\n",
            "Epoch 10, Batch 20/21: Loss = 0.9683, Time = 57.46s\n",
            "Epoch 10, Batch 21/21: Loss = 0.9682, Time = 51.93s\n",
            "Epoch 10: Train Loss = 0.9139, Val Loss = 0.8584, Time = 13427.60s\n",
            "Epoch 11, Batch 1/21: Loss = 0.9253, Time = 57.50s\n",
            "Epoch 11, Batch 2/21: Loss = 0.7809, Time = 57.97s\n",
            "Epoch 11, Batch 3/21: Loss = 0.9251, Time = 57.81s\n",
            "Epoch 11, Batch 4/21: Loss = 0.9085, Time = 57.85s\n",
            "Epoch 11, Batch 5/21: Loss = 0.8574, Time = 57.60s\n",
            "Epoch 11, Batch 6/21: Loss = 0.8439, Time = 57.58s\n",
            "Epoch 11, Batch 7/21: Loss = 0.9275, Time = 58.32s\n",
            "Epoch 11, Batch 8/21: Loss = 0.9340, Time = 56.78s\n",
            "Epoch 11, Batch 9/21: Loss = 1.0243, Time = 58.09s\n",
            "Epoch 11, Batch 10/21: Loss = 0.8473, Time = 57.50s\n",
            "Epoch 11, Batch 11/21: Loss = 0.8627, Time = 57.97s\n",
            "Epoch 11, Batch 12/21: Loss = 0.9078, Time = 57.36s\n",
            "Epoch 11, Batch 13/21: Loss = 0.8924, Time = 57.44s\n",
            "Epoch 11, Batch 14/21: Loss = 0.8263, Time = 57.99s\n",
            "Epoch 11, Batch 15/21: Loss = 0.8993, Time = 57.36s\n",
            "Epoch 11, Batch 16/21: Loss = 0.8919, Time = 58.04s\n",
            "Epoch 11, Batch 17/21: Loss = 0.9771, Time = 57.36s\n",
            "Epoch 11, Batch 18/21: Loss = 0.7922, Time = 57.38s\n",
            "Epoch 11, Batch 19/21: Loss = 0.9045, Time = 58.15s\n",
            "Epoch 11, Batch 20/21: Loss = 0.7920, Time = 57.71s\n",
            "Epoch 11, Batch 21/21: Loss = 0.8443, Time = 52.26s\n",
            "Epoch 11: Train Loss = 0.8840, Val Loss = 0.8362, Time = 14658.30s\n",
            "Epoch 12, Batch 1/21: Loss = 0.8663, Time = 58.04s\n",
            "Epoch 12, Batch 2/21: Loss = 0.8965, Time = 58.51s\n",
            "Epoch 12, Batch 3/21: Loss = 0.8458, Time = 58.03s\n",
            "Epoch 12, Batch 4/21: Loss = 0.8957, Time = 58.02s\n",
            "Epoch 12, Batch 5/21: Loss = 0.8782, Time = 57.40s\n",
            "Epoch 12, Batch 6/21: Loss = 0.9109, Time = 57.42s\n",
            "Epoch 12, Batch 7/21: Loss = 0.9304, Time = 58.16s\n",
            "Epoch 12, Batch 8/21: Loss = 0.8281, Time = 57.34s\n",
            "Epoch 12, Batch 9/21: Loss = 0.8815, Time = 57.90s\n",
            "Epoch 12, Batch 10/21: Loss = 0.7840, Time = 58.01s\n",
            "Epoch 12, Batch 11/21: Loss = 0.8847, Time = 58.33s\n",
            "Epoch 12, Batch 12/21: Loss = 0.7919, Time = 57.69s\n",
            "Epoch 12, Batch 13/21: Loss = 0.8421, Time = 57.86s\n",
            "Epoch 12, Batch 14/21: Loss = 0.8021, Time = 57.95s\n",
            "Epoch 12, Batch 15/21: Loss = 0.8310, Time = 57.74s\n",
            "Epoch 12, Batch 16/21: Loss = 0.9214, Time = 58.24s\n",
            "Epoch 12, Batch 17/21: Loss = 0.8754, Time = 58.17s\n",
            "Epoch 12, Batch 18/21: Loss = 0.8693, Time = 57.79s\n",
            "Epoch 12, Batch 19/21: Loss = 0.8936, Time = 58.42s\n",
            "Epoch 12, Batch 20/21: Loss = 0.8645, Time = 57.58s\n",
            "Epoch 12, Batch 21/21: Loss = 0.8184, Time = 51.94s\n",
            "Epoch 12: Train Loss = 0.8625, Val Loss = 0.8210, Time = 15893.57s\n",
            "Epoch 13, Batch 1/21: Loss = 0.8574, Time = 57.44s\n",
            "Epoch 13, Batch 2/21: Loss = 0.8629, Time = 58.15s\n",
            "Epoch 13, Batch 3/21: Loss = 0.9369, Time = 57.61s\n",
            "Epoch 13, Batch 4/21: Loss = 0.9011, Time = 57.98s\n",
            "Epoch 13, Batch 5/21: Loss = 0.8364, Time = 57.40s\n",
            "Epoch 13, Batch 6/21: Loss = 0.8336, Time = 57.77s\n",
            "Epoch 13, Batch 7/21: Loss = 0.8170, Time = 57.89s\n",
            "Epoch 13, Batch 8/21: Loss = 0.8746, Time = 56.89s\n",
            "Epoch 13, Batch 9/21: Loss = 0.8882, Time = 57.28s\n",
            "Epoch 13, Batch 10/21: Loss = 0.8498, Time = 57.77s\n",
            "Epoch 13, Batch 11/21: Loss = 0.8923, Time = 58.16s\n",
            "Epoch 13, Batch 12/21: Loss = 0.7742, Time = 57.73s\n",
            "Epoch 13, Batch 13/21: Loss = 0.8118, Time = 57.53s\n",
            "Epoch 13, Batch 14/21: Loss = 0.7737, Time = 58.07s\n",
            "Epoch 13, Batch 15/21: Loss = 0.8642, Time = 57.39s\n",
            "Epoch 13, Batch 16/21: Loss = 0.7748, Time = 57.96s\n",
            "Epoch 13, Batch 17/21: Loss = 0.8706, Time = 57.49s\n",
            "Epoch 13, Batch 18/21: Loss = 0.7781, Time = 57.53s\n",
            "Epoch 13, Batch 19/21: Loss = 0.8330, Time = 58.09s\n",
            "Epoch 13, Batch 20/21: Loss = 0.8785, Time = 57.60s\n",
            "Epoch 13, Batch 21/21: Loss = 0.8863, Time = 52.13s\n",
            "Epoch 13: Train Loss = 0.8474, Val Loss = 0.8104, Time = 17124.50s\n",
            "Epoch 14, Batch 1/21: Loss = 0.8703, Time = 57.60s\n",
            "Epoch 14, Batch 2/21: Loss = 0.8594, Time = 57.96s\n",
            "Epoch 14, Batch 3/21: Loss = 0.8300, Time = 57.68s\n",
            "Epoch 14, Batch 4/21: Loss = 0.8285, Time = 58.06s\n",
            "Epoch 14, Batch 5/21: Loss = 0.7940, Time = 57.53s\n",
            "Epoch 14, Batch 6/21: Loss = 0.9064, Time = 57.53s\n",
            "Epoch 14, Batch 7/21: Loss = 0.8773, Time = 58.03s\n",
            "Epoch 14, Batch 8/21: Loss = 0.8229, Time = 56.91s\n",
            "Epoch 14, Batch 9/21: Loss = 0.8843, Time = 57.35s\n",
            "Epoch 14, Batch 10/21: Loss = 0.8362, Time = 57.75s\n",
            "Epoch 14, Batch 11/21: Loss = 0.8624, Time = 58.02s\n",
            "Epoch 14, Batch 12/21: Loss = 0.8191, Time = 57.82s\n",
            "Epoch 14, Batch 13/21: Loss = 0.8631, Time = 57.64s\n",
            "Epoch 14, Batch 14/21: Loss = 0.8737, Time = 57.93s\n",
            "Epoch 14, Batch 15/21: Loss = 0.7663, Time = 57.62s\n",
            "Epoch 14, Batch 16/21: Loss = 0.7799, Time = 57.93s\n",
            "Epoch 14, Batch 17/21: Loss = 0.7733, Time = 57.82s\n",
            "Epoch 14, Batch 18/21: Loss = 0.8697, Time = 57.67s\n",
            "Epoch 14, Batch 19/21: Loss = 0.8397, Time = 58.21s\n",
            "Epoch 14, Batch 20/21: Loss = 0.7836, Time = 57.54s\n",
            "Epoch 14, Batch 21/21: Loss = 0.8122, Time = 52.17s\n",
            "Epoch 14: Train Loss = 0.8358, Val Loss = 0.8028, Time = 18356.44s\n",
            "Epoch 15, Batch 1/21: Loss = 0.8262, Time = 57.92s\n",
            "Epoch 15, Batch 2/21: Loss = 0.9179, Time = 58.09s\n",
            "Epoch 15, Batch 3/21: Loss = 0.7674, Time = 57.62s\n",
            "Epoch 15, Batch 4/21: Loss = 0.8366, Time = 58.21s\n",
            "Epoch 15, Batch 5/21: Loss = 0.8776, Time = 57.61s\n",
            "Epoch 15, Batch 6/21: Loss = 0.9007, Time = 57.51s\n",
            "Epoch 15, Batch 7/21: Loss = 0.8045, Time = 58.29s\n",
            "Epoch 15, Batch 8/21: Loss = 0.8805, Time = 57.53s\n",
            "Epoch 15, Batch 9/21: Loss = 0.8189, Time = 57.55s\n",
            "Epoch 15, Batch 10/21: Loss = 0.7966, Time = 57.60s\n",
            "Epoch 15, Batch 11/21: Loss = 0.8039, Time = 58.07s\n",
            "Epoch 15, Batch 12/21: Loss = 0.7674, Time = 57.66s\n",
            "Epoch 15, Batch 13/21: Loss = 0.8549, Time = 57.45s\n",
            "Epoch 15, Batch 14/21: Loss = 0.8223, Time = 58.09s\n",
            "Epoch 15, Batch 15/21: Loss = 0.8005, Time = 57.43s\n",
            "Epoch 15, Batch 16/21: Loss = 0.7753, Time = 58.21s\n",
            "Epoch 15, Batch 17/21: Loss = 0.7725, Time = 57.22s\n",
            "Epoch 15, Batch 18/21: Loss = 0.7969, Time = 57.41s\n",
            "Epoch 15, Batch 19/21: Loss = 0.8404, Time = 57.90s\n",
            "Epoch 15, Batch 20/21: Loss = 0.8619, Time = 57.69s\n",
            "Epoch 15, Batch 21/21: Loss = 0.8604, Time = 52.03s\n",
            "Epoch 15: Train Loss = 0.8278, Val Loss = 0.7974, Time = 19588.24s\n",
            "Epoch 16, Batch 1/21: Loss = 0.8140, Time = 57.73s\n",
            "Epoch 16, Batch 2/21: Loss = 0.8615, Time = 57.87s\n",
            "Epoch 16, Batch 3/21: Loss = 0.8897, Time = 57.78s\n",
            "Epoch 16, Batch 4/21: Loss = 0.8606, Time = 58.19s\n",
            "Epoch 16, Batch 5/21: Loss = 0.8814, Time = 57.70s\n",
            "Epoch 16, Batch 6/21: Loss = 0.8498, Time = 57.66s\n",
            "Epoch 16, Batch 7/21: Loss = 0.7845, Time = 58.39s\n",
            "Epoch 16, Batch 8/21: Loss = 0.7808, Time = 57.31s\n",
            "Epoch 16, Batch 9/21: Loss = 0.8217, Time = 57.50s\n",
            "Epoch 16, Batch 10/21: Loss = 0.7775, Time = 57.50s\n",
            "Epoch 16, Batch 11/21: Loss = 0.8485, Time = 58.15s\n",
            "Epoch 16, Batch 12/21: Loss = 0.8468, Time = 57.59s\n",
            "Epoch 16, Batch 13/21: Loss = 0.8090, Time = 57.46s\n",
            "Epoch 16, Batch 14/21: Loss = 0.7919, Time = 57.97s\n",
            "Epoch 16, Batch 15/21: Loss = 0.7846, Time = 57.88s\n",
            "Epoch 16, Batch 16/21: Loss = 0.8038, Time = 58.05s\n",
            "Epoch 16, Batch 17/21: Loss = 0.7698, Time = 57.73s\n",
            "Epoch 16, Batch 18/21: Loss = 0.7801, Time = 57.74s\n",
            "Epoch 16, Batch 19/21: Loss = 0.8670, Time = 58.16s\n",
            "Epoch 16, Batch 20/21: Loss = 0.8120, Time = 57.69s\n",
            "Epoch 16, Batch 21/21: Loss = 0.8128, Time = 52.26s\n",
            "Epoch 16: Train Loss = 0.8213, Val Loss = 0.7935, Time = 20821.40s\n",
            "Epoch 17, Batch 1/21: Loss = 0.8334, Time = 57.71s\n",
            "Epoch 17, Batch 2/21: Loss = 0.7738, Time = 58.13s\n",
            "Epoch 17, Batch 3/21: Loss = 0.8556, Time = 57.64s\n",
            "Epoch 17, Batch 4/21: Loss = 0.8466, Time = 58.39s\n",
            "Epoch 17, Batch 5/21: Loss = 0.8117, Time = 58.01s\n",
            "Epoch 17, Batch 6/21: Loss = 0.8143, Time = 57.60s\n",
            "Epoch 17, Batch 7/21: Loss = 0.8441, Time = 58.15s\n",
            "Epoch 17, Batch 8/21: Loss = 0.7661, Time = 58.22s\n",
            "Epoch 17, Batch 9/21: Loss = 0.7590, Time = 57.71s\n",
            "Epoch 17, Batch 10/21: Loss = 0.8344, Time = 57.60s\n",
            "Epoch 17, Batch 11/21: Loss = 0.8684, Time = 58.34s\n",
            "Epoch 17, Batch 12/21: Loss = 0.8486, Time = 57.79s\n",
            "Epoch 17, Batch 13/21: Loss = 0.7973, Time = 57.71s\n",
            "Epoch 17, Batch 14/21: Loss = 0.8198, Time = 58.36s\n",
            "Epoch 17, Batch 15/21: Loss = 0.7817, Time = 57.94s\n",
            "Epoch 17, Batch 16/21: Loss = 0.8847, Time = 58.52s\n",
            "Epoch 17, Batch 17/21: Loss = 0.8114, Time = 57.84s\n",
            "Epoch 17, Batch 18/21: Loss = 0.7983, Time = 57.83s\n",
            "Epoch 17, Batch 19/21: Loss = 0.9013, Time = 58.17s\n",
            "Epoch 17, Batch 20/21: Loss = 0.7604, Time = 57.70s\n",
            "Epoch 17, Batch 21/21: Loss = 0.7271, Time = 52.18s\n",
            "Epoch 17: Train Loss = 0.8161, Val Loss = 0.7905, Time = 22057.62s\n",
            "Epoch 18, Batch 1/21: Loss = 0.7632, Time = 57.89s\n",
            "Epoch 18, Batch 2/21: Loss = 0.8478, Time = 58.22s\n",
            "Epoch 18, Batch 3/21: Loss = 0.7503, Time = 57.80s\n",
            "Epoch 18, Batch 4/21: Loss = 0.7713, Time = 57.89s\n",
            "Epoch 18, Batch 5/21: Loss = 0.7882, Time = 57.67s\n",
            "Epoch 18, Batch 6/21: Loss = 0.8239, Time = 57.60s\n",
            "Epoch 18, Batch 7/21: Loss = 0.7314, Time = 58.21s\n",
            "Epoch 18, Batch 8/21: Loss = 0.8153, Time = 57.82s\n",
            "Epoch 18, Batch 9/21: Loss = 0.8480, Time = 58.17s\n",
            "Epoch 18, Batch 10/21: Loss = 0.8574, Time = 57.73s\n",
            "Epoch 18, Batch 11/21: Loss = 0.8522, Time = 58.30s\n",
            "Epoch 18, Batch 12/21: Loss = 0.8537, Time = 57.84s\n",
            "Epoch 18, Batch 13/21: Loss = 0.7744, Time = 57.86s\n",
            "Epoch 18, Batch 14/21: Loss = 0.7970, Time = 58.21s\n",
            "Epoch 18, Batch 15/21: Loss = 0.8799, Time = 57.66s\n",
            "Epoch 18, Batch 16/21: Loss = 0.7690, Time = 58.13s\n",
            "Epoch 18, Batch 17/21: Loss = 0.8828, Time = 57.42s\n",
            "Epoch 18, Batch 18/21: Loss = 0.7917, Time = 57.45s\n",
            "Epoch 18, Batch 19/21: Loss = 0.8041, Time = 58.08s\n",
            "Epoch 18, Batch 20/21: Loss = 0.8050, Time = 57.88s\n",
            "Epoch 18, Batch 21/21: Loss = 0.8600, Time = 52.12s\n",
            "Epoch 18: Train Loss = 0.8127, Val Loss = 0.7879, Time = 23292.26s\n",
            "Epoch 19, Batch 1/21: Loss = 0.8482, Time = 57.71s\n",
            "Epoch 19, Batch 2/21: Loss = 0.7762, Time = 58.37s\n",
            "Epoch 19, Batch 3/21: Loss = 0.8095, Time = 57.72s\n",
            "Epoch 19, Batch 4/21: Loss = 0.8030, Time = 58.04s\n",
            "Epoch 19, Batch 5/21: Loss = 0.8560, Time = 57.87s\n",
            "Epoch 19, Batch 6/21: Loss = 0.8222, Time = 57.93s\n",
            "Epoch 19, Batch 7/21: Loss = 0.8507, Time = 57.98s\n",
            "Epoch 19, Batch 8/21: Loss = 0.8108, Time = 57.63s\n",
            "Epoch 19, Batch 9/21: Loss = 0.8252, Time = 58.03s\n",
            "Epoch 19, Batch 10/21: Loss = 0.8814, Time = 57.48s\n",
            "Epoch 19, Batch 11/21: Loss = 0.7626, Time = 58.33s\n",
            "Epoch 19, Batch 12/21: Loss = 0.8524, Time = 57.50s\n",
            "Epoch 19, Batch 13/21: Loss = 0.7641, Time = 57.32s\n",
            "Epoch 19, Batch 14/21: Loss = 0.8037, Time = 57.71s\n",
            "Epoch 19, Batch 15/21: Loss = 0.7694, Time = 57.66s\n",
            "Epoch 19, Batch 16/21: Loss = 0.7854, Time = 57.98s\n",
            "Epoch 19, Batch 17/21: Loss = 0.7522, Time = 57.76s\n",
            "Epoch 19, Batch 18/21: Loss = 0.7690, Time = 57.55s\n",
            "Epoch 19, Batch 19/21: Loss = 0.7713, Time = 58.05s\n",
            "Epoch 19, Batch 20/21: Loss = 0.8163, Time = 57.92s\n",
            "Epoch 19, Batch 21/21: Loss = 0.8699, Time = 52.17s\n",
            "Epoch 19: Train Loss = 0.8095, Val Loss = 0.7854, Time = 24525.57s\n",
            "Epoch 20, Batch 1/21: Loss = 0.7730, Time = 57.50s\n",
            "Epoch 20, Batch 2/21: Loss = 0.8262, Time = 57.84s\n",
            "Epoch 20, Batch 3/21: Loss = 0.7748, Time = 57.52s\n",
            "Epoch 20, Batch 4/21: Loss = 0.8161, Time = 58.26s\n",
            "Epoch 20, Batch 5/21: Loss = 0.8164, Time = 56.96s\n",
            "Epoch 20, Batch 6/21: Loss = 0.8175, Time = 57.13s\n",
            "Epoch 20, Batch 7/21: Loss = 0.7809, Time = 58.32s\n",
            "Epoch 20, Batch 8/21: Loss = 0.7867, Time = 57.35s\n",
            "Epoch 20, Batch 9/21: Loss = 0.8923, Time = 57.73s\n",
            "Epoch 20, Batch 10/21: Loss = 0.8332, Time = 57.51s\n",
            "Epoch 20, Batch 11/21: Loss = 0.7276, Time = 57.96s\n",
            "Epoch 20, Batch 12/21: Loss = 0.7788, Time = 57.56s\n",
            "Epoch 20, Batch 13/21: Loss = 0.7885, Time = 57.50s\n",
            "Epoch 20, Batch 14/21: Loss = 0.7680, Time = 57.88s\n",
            "Epoch 20, Batch 15/21: Loss = 0.8180, Time = 57.38s\n",
            "Epoch 20, Batch 16/21: Loss = 0.8225, Time = 58.20s\n",
            "Epoch 20, Batch 17/21: Loss = 0.8793, Time = 57.56s\n",
            "Epoch 20, Batch 18/21: Loss = 0.8119, Time = 57.82s\n",
            "Epoch 20, Batch 19/21: Loss = 0.8268, Time = 58.08s\n",
            "Epoch 20, Batch 20/21: Loss = 0.8045, Time = 57.77s\n",
            "Epoch 20, Batch 21/21: Loss = 0.7882, Time = 51.98s\n",
            "Epoch 20: Train Loss = 0.8062, Val Loss = 0.7831, Time = 25756.03s\n",
            "Epoch 21, Batch 1/21: Loss = 0.8168, Time = 57.56s\n",
            "Epoch 21, Batch 2/21: Loss = 0.8422, Time = 57.82s\n",
            "Epoch 21, Batch 3/21: Loss = 0.8266, Time = 57.48s\n",
            "Epoch 21, Batch 4/21: Loss = 0.8186, Time = 58.03s\n",
            "Epoch 21, Batch 5/21: Loss = 0.8152, Time = 57.97s\n",
            "Epoch 21, Batch 6/21: Loss = 0.7824, Time = 56.80s\n",
            "Epoch 21, Batch 7/21: Loss = 0.8155, Time = 57.47s\n",
            "Epoch 21, Batch 8/21: Loss = 0.7934, Time = 57.11s\n",
            "Epoch 21, Batch 9/21: Loss = 0.7826, Time = 57.31s\n",
            "Epoch 21, Batch 10/21: Loss = 0.8259, Time = 56.85s\n",
            "Epoch 21, Batch 11/21: Loss = 0.7663, Time = 57.62s\n",
            "Epoch 21, Batch 12/21: Loss = 0.8147, Time = 57.06s\n",
            "Epoch 21, Batch 13/21: Loss = 0.7858, Time = 57.62s\n",
            "Epoch 21, Batch 14/21: Loss = 0.8146, Time = 58.00s\n",
            "Epoch 21, Batch 15/21: Loss = 0.7845, Time = 57.56s\n",
            "Epoch 21, Batch 16/21: Loss = 0.8103, Time = 57.93s\n",
            "Epoch 21, Batch 17/21: Loss = 0.8255, Time = 57.55s\n",
            "Epoch 21, Batch 18/21: Loss = 0.7772, Time = 57.55s\n",
            "Epoch 21, Batch 19/21: Loss = 0.8593, Time = 57.82s\n",
            "Epoch 21, Batch 20/21: Loss = 0.8055, Time = 57.56s\n",
            "Epoch 21, Batch 21/21: Loss = 0.7031, Time = 51.92s\n",
            "Epoch 21: Train Loss = 0.8031, Val Loss = 0.7812, Time = 26982.52s\n",
            "Epoch 22, Batch 1/21: Loss = 0.7880, Time = 56.54s\n",
            "Epoch 22, Batch 2/21: Loss = 0.8420, Time = 57.13s\n",
            "Epoch 22, Batch 3/21: Loss = 0.7738, Time = 56.87s\n",
            "Epoch 22, Batch 4/21: Loss = 0.7266, Time = 57.23s\n",
            "Epoch 22, Batch 5/21: Loss = 0.8357, Time = 57.30s\n",
            "Epoch 22, Batch 6/21: Loss = 0.7985, Time = 56.97s\n",
            "Epoch 22, Batch 7/21: Loss = 0.8123, Time = 57.70s\n",
            "Epoch 22, Batch 8/21: Loss = 0.7892, Time = 57.84s\n",
            "Epoch 22, Batch 9/21: Loss = 0.8197, Time = 58.71s\n",
            "Epoch 22, Batch 10/21: Loss = 0.7635, Time = 57.76s\n",
            "Epoch 22, Batch 11/21: Loss = 0.7978, Time = 58.07s\n",
            "Epoch 22, Batch 12/21: Loss = 0.9003, Time = 57.76s\n",
            "Epoch 22, Batch 13/21: Loss = 0.8281, Time = 58.08s\n",
            "Epoch 22, Batch 14/21: Loss = 0.7712, Time = 58.56s\n",
            "Epoch 22, Batch 15/21: Loss = 0.7817, Time = 58.84s\n",
            "Epoch 22, Batch 16/21: Loss = 0.7977, Time = 59.03s\n",
            "Epoch 22, Batch 17/21: Loss = 0.7930, Time = 58.51s\n",
            "Epoch 22, Batch 18/21: Loss = 0.8120, Time = 58.05s\n",
            "Epoch 22, Batch 19/21: Loss = 0.8290, Time = 58.94s\n",
            "Epoch 22, Batch 20/21: Loss = 0.7611, Time = 58.47s\n",
            "Epoch 22, Batch 21/21: Loss = 0.8045, Time = 52.98s\n",
            "Epoch 22: Train Loss = 0.8012, Val Loss = 0.7790, Time = 28218.14s\n",
            "Epoch 23, Batch 1/21: Loss = 0.8168, Time = 57.92s\n",
            "Epoch 23, Batch 2/21: Loss = 0.7508, Time = 58.95s\n",
            "Epoch 23, Batch 3/21: Loss = 0.7919, Time = 58.37s\n",
            "Epoch 23, Batch 4/21: Loss = 0.8540, Time = 58.47s\n",
            "Epoch 23, Batch 5/21: Loss = 0.7970, Time = 58.85s\n",
            "Epoch 23, Batch 6/21: Loss = 0.8276, Time = 58.91s\n",
            "Epoch 23, Batch 7/21: Loss = 0.8130, Time = 58.58s\n",
            "Epoch 23, Batch 8/21: Loss = 0.7839, Time = 58.51s\n",
            "Epoch 23, Batch 9/21: Loss = 0.8260, Time = 59.49s\n",
            "Epoch 23, Batch 10/21: Loss = 0.7787, Time = 58.37s\n",
            "Epoch 23, Batch 11/21: Loss = 0.7266, Time = 58.83s\n",
            "Epoch 23, Batch 12/21: Loss = 0.8399, Time = 58.30s\n",
            "Epoch 23, Batch 13/21: Loss = 0.8190, Time = 58.41s\n",
            "Epoch 23, Batch 14/21: Loss = 0.8286, Time = 57.76s\n",
            "Epoch 23, Batch 15/21: Loss = 0.8012, Time = 57.61s\n",
            "Epoch 23, Batch 16/21: Loss = 0.7778, Time = 58.05s\n",
            "Epoch 23, Batch 17/21: Loss = 0.7604, Time = 58.10s\n",
            "Epoch 23, Batch 18/21: Loss = 0.7975, Time = 57.45s\n",
            "Epoch 23, Batch 19/21: Loss = 0.7720, Time = 57.47s\n",
            "Epoch 23, Batch 20/21: Loss = 0.7784, Time = 57.11s\n",
            "Epoch 23, Batch 21/21: Loss = 0.8405, Time = 51.59s\n",
            "Epoch 23: Train Loss = 0.7991, Val Loss = 0.7772, Time = 29459.09s\n",
            "Epoch 24, Batch 1/21: Loss = 0.8152, Time = 57.19s\n",
            "Epoch 24, Batch 2/21: Loss = 0.7970, Time = 57.53s\n",
            "Epoch 24, Batch 3/21: Loss = 0.8228, Time = 57.02s\n",
            "Epoch 24, Batch 4/21: Loss = 0.8170, Time = 58.15s\n",
            "Epoch 24, Batch 5/21: Loss = 0.8362, Time = 57.69s\n",
            "Epoch 24, Batch 6/21: Loss = 0.7864, Time = 58.09s\n",
            "Epoch 24, Batch 7/21: Loss = 0.8397, Time = 57.98s\n",
            "Epoch 24, Batch 8/21: Loss = 0.8420, Time = 57.40s\n",
            "Epoch 24, Batch 9/21: Loss = 0.7424, Time = 57.83s\n",
            "Epoch 24, Batch 10/21: Loss = 0.7355, Time = 57.67s\n",
            "Epoch 24, Batch 11/21: Loss = 0.7477, Time = 58.20s\n",
            "Epoch 24, Batch 12/21: Loss = 0.8129, Time = 57.29s\n",
            "Epoch 24, Batch 13/21: Loss = 0.8364, Time = 57.44s\n",
            "Epoch 24, Batch 14/21: Loss = 0.7776, Time = 58.01s\n",
            "Epoch 24, Batch 15/21: Loss = 0.7995, Time = 58.03s\n",
            "Epoch 24, Batch 16/21: Loss = 0.7702, Time = 58.50s\n",
            "Epoch 24, Batch 17/21: Loss = 0.7945, Time = 57.84s\n",
            "Epoch 24, Batch 18/21: Loss = 0.7865, Time = 57.93s\n",
            "Epoch 24, Batch 19/21: Loss = 0.8549, Time = 58.27s\n",
            "Epoch 24, Batch 20/21: Loss = 0.7607, Time = 57.92s\n",
            "Epoch 24, Batch 21/21: Loss = 0.7530, Time = 52.48s\n",
            "Epoch 24: Train Loss = 0.7966, Val Loss = 0.7750, Time = 30691.56s\n",
            "Epoch 25, Batch 1/21: Loss = 0.7614, Time = 58.29s\n",
            "Epoch 25, Batch 2/21: Loss = 0.7208, Time = 59.24s\n",
            "Epoch 25, Batch 3/21: Loss = 0.7540, Time = 57.95s\n",
            "Epoch 25, Batch 4/21: Loss = 0.8136, Time = 58.72s\n",
            "Epoch 25, Batch 5/21: Loss = 0.8263, Time = 57.74s\n",
            "Epoch 25, Batch 6/21: Loss = 0.7936, Time = 57.88s\n",
            "Epoch 25, Batch 7/21: Loss = 0.8159, Time = 58.97s\n",
            "Epoch 25, Batch 8/21: Loss = 0.7566, Time = 58.27s\n",
            "Epoch 25, Batch 9/21: Loss = 0.7700, Time = 58.50s\n",
            "Epoch 25, Batch 10/21: Loss = 0.7714, Time = 58.59s\n",
            "Epoch 25, Batch 11/21: Loss = 0.8266, Time = 58.22s\n",
            "Epoch 25, Batch 12/21: Loss = 0.8764, Time = 58.30s\n",
            "Epoch 25, Batch 13/21: Loss = 0.7583, Time = 58.78s\n",
            "Epoch 25, Batch 14/21: Loss = 0.8449, Time = 58.82s\n",
            "Epoch 25, Batch 15/21: Loss = 0.8169, Time = 58.74s\n",
            "Epoch 25, Batch 16/21: Loss = 0.7685, Time = 58.33s\n",
            "Epoch 25, Batch 17/21: Loss = 0.7799, Time = 57.59s\n",
            "Epoch 25, Batch 18/21: Loss = 0.8556, Time = 58.41s\n",
            "Epoch 25, Batch 19/21: Loss = 0.8201, Time = 59.23s\n",
            "Epoch 25, Batch 20/21: Loss = 0.7627, Time = 57.98s\n",
            "Epoch 25, Batch 21/21: Loss = 0.7958, Time = 52.43s\n",
            "Epoch 25: Train Loss = 0.7947, Val Loss = 0.7733, Time = 31937.03s\n",
            "Epoch 26, Batch 1/21: Loss = 0.7498, Time = 57.87s\n",
            "Epoch 26, Batch 2/21: Loss = 0.8084, Time = 58.00s\n",
            "Epoch 26, Batch 3/21: Loss = 0.7829, Time = 56.88s\n",
            "Epoch 26, Batch 4/21: Loss = 0.8295, Time = 57.24s\n",
            "Epoch 26, Batch 5/21: Loss = 0.7804, Time = 56.71s\n",
            "Epoch 26, Batch 6/21: Loss = 0.7780, Time = 56.58s\n",
            "Epoch 26, Batch 7/21: Loss = 0.7854, Time = 57.11s\n",
            "Epoch 26, Batch 8/21: Loss = 0.8087, Time = 56.52s\n",
            "Epoch 26, Batch 9/21: Loss = 0.8209, Time = 57.03s\n",
            "Epoch 26, Batch 10/21: Loss = 0.7521, Time = 56.60s\n",
            "Epoch 26, Batch 11/21: Loss = 0.8797, Time = 57.27s\n",
            "Epoch 26, Batch 12/21: Loss = 0.7685, Time = 56.87s\n",
            "Epoch 26, Batch 13/21: Loss = 0.7911, Time = 56.84s\n",
            "Epoch 26, Batch 14/21: Loss = 0.7895, Time = 57.20s\n",
            "Epoch 26, Batch 15/21: Loss = 0.8137, Time = 56.73s\n",
            "Epoch 26, Batch 16/21: Loss = 0.7732, Time = 57.15s\n",
            "Epoch 26, Batch 17/21: Loss = 0.8149, Time = 56.88s\n",
            "Epoch 26, Batch 18/21: Loss = 0.8129, Time = 56.63s\n",
            "Epoch 26, Batch 19/21: Loss = 0.7818, Time = 57.29s\n",
            "Epoch 26, Batch 20/21: Loss = 0.7632, Time = 56.90s\n",
            "Epoch 26, Batch 21/21: Loss = 0.7589, Time = 51.50s\n",
            "Epoch 26: Train Loss = 0.7925, Val Loss = 0.7711, Time = 33152.82s\n",
            "Epoch 27, Batch 1/21: Loss = 0.8440, Time = 57.47s\n",
            "Epoch 27, Batch 2/21: Loss = 0.8407, Time = 57.53s\n",
            "Epoch 27, Batch 3/21: Loss = 0.7954, Time = 57.23s\n",
            "Epoch 27, Batch 4/21: Loss = 0.7692, Time = 57.34s\n",
            "Epoch 27, Batch 5/21: Loss = 0.8239, Time = 56.69s\n",
            "Epoch 27, Batch 6/21: Loss = 0.7601, Time = 56.98s\n",
            "Epoch 27, Batch 7/21: Loss = 0.8166, Time = 57.78s\n",
            "Epoch 27, Batch 8/21: Loss = 0.8014, Time = 56.90s\n",
            "Epoch 27, Batch 9/21: Loss = 0.7880, Time = 57.06s\n",
            "Epoch 27, Batch 10/21: Loss = 0.8266, Time = 57.18s\n",
            "Epoch 27, Batch 11/21: Loss = 0.7527, Time = 57.96s\n",
            "Epoch 27, Batch 12/21: Loss = 0.7418, Time = 57.08s\n",
            "Epoch 27, Batch 13/21: Loss = 0.8088, Time = 57.08s\n",
            "Epoch 27, Batch 14/21: Loss = 0.6929, Time = 57.68s\n",
            "Epoch 27, Batch 15/21: Loss = 0.7903, Time = 57.11s\n",
            "Epoch 27, Batch 16/21: Loss = 0.7901, Time = 57.19s\n",
            "Epoch 27, Batch 17/21: Loss = 0.7727, Time = 57.03s\n",
            "Epoch 27, Batch 18/21: Loss = 0.7722, Time = 57.09s\n",
            "Epoch 27, Batch 19/21: Loss = 0.7910, Time = 57.66s\n",
            "Epoch 27, Batch 20/21: Loss = 0.7893, Time = 57.36s\n",
            "Epoch 27, Batch 21/21: Loss = 0.8437, Time = 51.54s\n",
            "Epoch 27: Train Loss = 0.7910, Val Loss = 0.7694, Time = 34373.62s\n",
            "Epoch 28, Batch 1/21: Loss = 0.7933, Time = 56.87s\n",
            "Epoch 28, Batch 2/21: Loss = 0.8413, Time = 57.13s\n",
            "Epoch 28, Batch 3/21: Loss = 0.7719, Time = 57.13s\n",
            "Epoch 28, Batch 4/21: Loss = 0.7597, Time = 57.46s\n",
            "Epoch 28, Batch 5/21: Loss = 0.7582, Time = 56.81s\n",
            "Epoch 28, Batch 6/21: Loss = 0.8192, Time = 56.87s\n",
            "Epoch 28, Batch 7/21: Loss = 0.7966, Time = 57.25s\n",
            "Epoch 28, Batch 8/21: Loss = 0.8125, Time = 57.10s\n",
            "Epoch 28, Batch 9/21: Loss = 0.7912, Time = 57.72s\n",
            "Epoch 28, Batch 10/21: Loss = 0.7610, Time = 57.13s\n",
            "Epoch 28, Batch 11/21: Loss = 0.7590, Time = 57.57s\n",
            "Epoch 28, Batch 12/21: Loss = 0.7934, Time = 56.86s\n",
            "Epoch 28, Batch 13/21: Loss = 0.8013, Time = 56.83s\n",
            "Epoch 28, Batch 14/21: Loss = 0.8223, Time = 57.30s\n",
            "Epoch 28, Batch 15/21: Loss = 0.7577, Time = 57.13s\n",
            "Epoch 28, Batch 16/21: Loss = 0.7982, Time = 57.18s\n",
            "Epoch 28, Batch 17/21: Loss = 0.6993, Time = 57.03s\n",
            "Epoch 28, Batch 18/21: Loss = 0.8144, Time = 57.03s\n",
            "Epoch 28, Batch 19/21: Loss = 0.7638, Time = 57.32s\n",
            "Epoch 28, Batch 20/21: Loss = 0.8421, Time = 56.87s\n",
            "Epoch 28, Batch 21/21: Loss = 0.8105, Time = 51.32s\n",
            "Epoch 28: Train Loss = 0.7889, Val Loss = 0.7675, Time = 35591.53s\n",
            "Epoch 29, Batch 1/21: Loss = 0.7930, Time = 56.61s\n",
            "Epoch 29, Batch 2/21: Loss = 0.8212, Time = 57.27s\n",
            "Epoch 29, Batch 3/21: Loss = 0.7738, Time = 56.85s\n",
            "Epoch 29, Batch 4/21: Loss = 0.8085, Time = 57.14s\n",
            "Epoch 29, Batch 5/21: Loss = 0.7519, Time = 57.19s\n",
            "Epoch 29, Batch 6/21: Loss = 0.8308, Time = 57.04s\n",
            "Epoch 29, Batch 7/21: Loss = 0.8178, Time = 57.43s\n",
            "Epoch 29, Batch 8/21: Loss = 0.7681, Time = 56.82s\n",
            "Epoch 29, Batch 9/21: Loss = 0.8343, Time = 57.35s\n",
            "Epoch 29, Batch 10/21: Loss = 0.7952, Time = 57.22s\n",
            "Epoch 29, Batch 11/21: Loss = 0.7601, Time = 57.59s\n",
            "Epoch 29, Batch 12/21: Loss = 0.7214, Time = 57.21s\n",
            "Epoch 29, Batch 13/21: Loss = 0.7942, Time = 57.01s\n",
            "Epoch 29, Batch 14/21: Loss = 0.8157, Time = 57.40s\n",
            "Epoch 29, Batch 15/21: Loss = 0.7584, Time = 56.75s\n",
            "Epoch 29, Batch 16/21: Loss = 0.8049, Time = 57.38s\n",
            "Epoch 29, Batch 17/21: Loss = 0.7788, Time = 57.20s\n",
            "Epoch 29, Batch 18/21: Loss = 0.8492, Time = 56.93s\n",
            "Epoch 29, Batch 19/21: Loss = 0.7732, Time = 57.41s\n",
            "Epoch 29, Batch 20/21: Loss = 0.7050, Time = 57.12s\n",
            "Epoch 29, Batch 21/21: Loss = 0.7677, Time = 51.62s\n",
            "Epoch 29: Train Loss = 0.7868, Val Loss = 0.7657, Time = 36810.02s\n",
            "Epoch 30, Batch 1/21: Loss = 0.7932, Time = 56.80s\n",
            "Epoch 30, Batch 2/21: Loss = 0.8276, Time = 57.44s\n",
            "Epoch 30, Batch 3/21: Loss = 0.8009, Time = 56.89s\n",
            "Epoch 30, Batch 4/21: Loss = 0.7707, Time = 57.37s\n",
            "Epoch 30, Batch 5/21: Loss = 0.7544, Time = 56.66s\n",
            "Epoch 30, Batch 6/21: Loss = 0.7929, Time = 56.77s\n",
            "Epoch 30, Batch 7/21: Loss = 0.8178, Time = 58.02s\n",
            "Epoch 30, Batch 8/21: Loss = 0.8084, Time = 56.81s\n",
            "Epoch 30, Batch 9/21: Loss = 0.7787, Time = 57.40s\n",
            "Epoch 30, Batch 10/21: Loss = 0.7674, Time = 57.37s\n",
            "Epoch 30, Batch 11/21: Loss = 0.7750, Time = 57.54s\n",
            "Epoch 30, Batch 12/21: Loss = 0.7970, Time = 57.14s\n",
            "Epoch 30, Batch 13/21: Loss = 0.7454, Time = 56.98s\n",
            "Epoch 30, Batch 14/21: Loss = 0.8018, Time = 57.25s\n",
            "Epoch 30, Batch 15/21: Loss = 0.7552, Time = 56.95s\n",
            "Epoch 30, Batch 16/21: Loss = 0.7268, Time = 57.42s\n",
            "Epoch 30, Batch 17/21: Loss = 0.8034, Time = 57.37s\n",
            "Epoch 30, Batch 18/21: Loss = 0.7942, Time = 56.94s\n",
            "Epoch 30, Batch 19/21: Loss = 0.8270, Time = 57.43s\n",
            "Epoch 30, Batch 20/21: Loss = 0.7898, Time = 57.02s\n",
            "Epoch 30, Batch 21/21: Loss = 0.7552, Time = 52.01s\n",
            "Epoch 30: Train Loss = 0.7849, Val Loss = 0.7638, Time = 38029.56s\n",
            "Epoch 31, Batch 1/21: Loss = 0.7581, Time = 56.85s\n",
            "Epoch 31, Batch 2/21: Loss = 0.8308, Time = 57.62s\n",
            "Epoch 31, Batch 3/21: Loss = 0.7395, Time = 56.93s\n",
            "Epoch 31, Batch 4/21: Loss = 0.6984, Time = 57.45s\n",
            "Epoch 31, Batch 5/21: Loss = 0.7934, Time = 57.25s\n",
            "Epoch 31, Batch 6/21: Loss = 0.7679, Time = 57.23s\n",
            "Epoch 31, Batch 7/21: Loss = 0.7069, Time = 57.47s\n",
            "Epoch 31, Batch 8/21: Loss = 0.7928, Time = 57.15s\n",
            "Epoch 31, Batch 9/21: Loss = 0.8173, Time = 57.41s\n",
            "Epoch 31, Batch 10/21: Loss = 0.8123, Time = 56.91s\n",
            "Epoch 31, Batch 11/21: Loss = 0.7592, Time = 57.54s\n",
            "Epoch 31, Batch 12/21: Loss = 0.8059, Time = 57.22s\n",
            "Epoch 31, Batch 13/21: Loss = 0.7884, Time = 56.81s\n",
            "Epoch 31, Batch 14/21: Loss = 0.7516, Time = 57.33s\n",
            "Epoch 31, Batch 15/21: Loss = 0.8044, Time = 57.07s\n",
            "Epoch 31, Batch 16/21: Loss = 0.8309, Time = 57.33s\n",
            "Epoch 31, Batch 17/21: Loss = 0.8396, Time = 57.50s\n",
            "Epoch 31, Batch 18/21: Loss = 0.8365, Time = 56.84s\n",
            "Epoch 31, Batch 19/21: Loss = 0.7289, Time = 57.81s\n",
            "Epoch 31, Batch 20/21: Loss = 0.7717, Time = 57.38s\n",
            "Epoch 31, Batch 21/21: Loss = 0.8169, Time = 51.53s\n",
            "Epoch 31: Train Loss = 0.7834, Val Loss = 0.7620, Time = 39250.28s\n",
            "Epoch 32, Batch 1/21: Loss = 0.8048, Time = 57.04s\n",
            "Epoch 32, Batch 2/21: Loss = 0.7725, Time = 57.43s\n",
            "Epoch 32, Batch 3/21: Loss = 0.7765, Time = 56.79s\n",
            "Epoch 32, Batch 4/21: Loss = 0.8172, Time = 57.21s\n",
            "Epoch 32, Batch 5/21: Loss = 0.7819, Time = 56.74s\n",
            "Epoch 32, Batch 6/21: Loss = 0.7997, Time = 57.13s\n",
            "Epoch 32, Batch 7/21: Loss = 0.7825, Time = 57.62s\n",
            "Epoch 32, Batch 8/21: Loss = 0.7520, Time = 57.02s\n",
            "Epoch 32, Batch 9/21: Loss = 0.7861, Time = 57.42s\n",
            "Epoch 32, Batch 10/21: Loss = 0.7885, Time = 57.29s\n",
            "Epoch 32, Batch 11/21: Loss = 0.7228, Time = 57.63s\n",
            "Epoch 32, Batch 12/21: Loss = 0.7447, Time = 57.31s\n",
            "Epoch 32, Batch 13/21: Loss = 0.7625, Time = 57.16s\n",
            "Epoch 32, Batch 14/21: Loss = 0.7676, Time = 57.82s\n",
            "Epoch 32, Batch 15/21: Loss = 0.8360, Time = 57.18s\n",
            "Epoch 32, Batch 16/21: Loss = 0.8417, Time = 57.53s\n",
            "Epoch 32, Batch 17/21: Loss = 0.7935, Time = 57.15s\n",
            "Epoch 32, Batch 18/21: Loss = 0.7814, Time = 57.10s\n",
            "Epoch 32, Batch 19/21: Loss = 0.7853, Time = 57.23s\n",
            "Epoch 32, Batch 20/21: Loss = 0.6950, Time = 56.93s\n",
            "Epoch 32, Batch 21/21: Loss = 0.8205, Time = 51.47s\n",
            "Epoch 32: Train Loss = 0.7816, Val Loss = 0.7605, Time = 40471.19s\n",
            "Epoch 33, Batch 1/21: Loss = 0.7741, Time = 57.32s\n",
            "Epoch 33, Batch 2/21: Loss = 0.8078, Time = 57.78s\n",
            "Epoch 33, Batch 3/21: Loss = 0.7607, Time = 57.01s\n",
            "Epoch 33, Batch 4/21: Loss = 0.7658, Time = 57.29s\n",
            "Epoch 33, Batch 5/21: Loss = 0.7131, Time = 56.70s\n",
            "Epoch 33, Batch 6/21: Loss = 0.7902, Time = 56.73s\n",
            "Epoch 33, Batch 7/21: Loss = 0.8404, Time = 57.02s\n",
            "Epoch 33, Batch 8/21: Loss = 0.8412, Time = 56.78s\n",
            "Epoch 33, Batch 9/21: Loss = 0.8102, Time = 57.39s\n",
            "Epoch 33, Batch 10/21: Loss = 0.8364, Time = 56.72s\n",
            "Epoch 33, Batch 11/21: Loss = 0.7067, Time = 57.69s\n",
            "Epoch 33, Batch 12/21: Loss = 0.8101, Time = 57.04s\n",
            "Epoch 33, Batch 13/21: Loss = 0.7778, Time = 57.21s\n",
            "Epoch 33, Batch 14/21: Loss = 0.7729, Time = 57.29s\n",
            "Epoch 33, Batch 15/21: Loss = 0.7438, Time = 56.76s\n",
            "Epoch 33, Batch 16/21: Loss = 0.7680, Time = 57.42s\n",
            "Epoch 33, Batch 17/21: Loss = 0.7961, Time = 56.95s\n",
            "Epoch 33, Batch 18/21: Loss = 0.7794, Time = 56.59s\n",
            "Epoch 33, Batch 19/21: Loss = 0.7048, Time = 57.40s\n",
            "Epoch 33, Batch 20/21: Loss = 0.8032, Time = 57.29s\n",
            "Epoch 33, Batch 21/21: Loss = 0.7670, Time = 51.79s\n",
            "Epoch 33: Train Loss = 0.7795, Val Loss = 0.7588, Time = 41689.25s\n",
            "Epoch 34, Batch 1/21: Loss = 0.7902, Time = 57.03s\n",
            "Epoch 34, Batch 2/21: Loss = 0.7602, Time = 57.32s\n",
            "Epoch 34, Batch 3/21: Loss = 0.7475, Time = 56.65s\n",
            "Epoch 34, Batch 4/21: Loss = 0.7773, Time = 57.33s\n",
            "Epoch 34, Batch 5/21: Loss = 0.8080, Time = 56.90s\n",
            "Epoch 34, Batch 6/21: Loss = 0.8076, Time = 56.85s\n",
            "Epoch 34, Batch 7/21: Loss = 0.7804, Time = 57.32s\n",
            "Epoch 34, Batch 8/21: Loss = 0.7450, Time = 56.83s\n",
            "Epoch 34, Batch 9/21: Loss = 0.8031, Time = 57.03s\n",
            "Epoch 34, Batch 10/21: Loss = 0.8182, Time = 57.20s\n",
            "Epoch 34, Batch 11/21: Loss = 0.7765, Time = 57.63s\n",
            "Epoch 34, Batch 12/21: Loss = 0.8114, Time = 57.19s\n",
            "Epoch 34, Batch 13/21: Loss = 0.7567, Time = 56.96s\n",
            "Epoch 34, Batch 14/21: Loss = 0.7345, Time = 57.12s\n",
            "Epoch 34, Batch 15/21: Loss = 0.7616, Time = 56.70s\n",
            "Epoch 34, Batch 16/21: Loss = 0.7952, Time = 57.28s\n",
            "Epoch 34, Batch 17/21: Loss = 0.7856, Time = 56.85s\n",
            "Epoch 34, Batch 18/21: Loss = 0.7393, Time = 56.90s\n",
            "Epoch 34, Batch 19/21: Loss = 0.7536, Time = 57.43s\n",
            "Epoch 34, Batch 20/21: Loss = 0.7715, Time = 57.04s\n",
            "Epoch 34, Batch 21/21: Loss = 0.8125, Time = 51.44s\n",
            "Epoch 34: Train Loss = 0.7779, Val Loss = 0.7570, Time = 42906.22s\n",
            "Epoch 35, Batch 1/21: Loss = 0.7478, Time = 56.82s\n",
            "Epoch 35, Batch 2/21: Loss = 0.8045, Time = 57.24s\n",
            "Epoch 35, Batch 3/21: Loss = 0.7907, Time = 56.92s\n",
            "Epoch 35, Batch 4/21: Loss = 0.7711, Time = 57.16s\n",
            "Epoch 35, Batch 5/21: Loss = 0.6939, Time = 57.27s\n",
            "Epoch 35, Batch 6/21: Loss = 0.8292, Time = 56.66s\n",
            "Epoch 35, Batch 7/21: Loss = 0.7833, Time = 57.52s\n",
            "Epoch 35, Batch 8/21: Loss = 0.7680, Time = 56.90s\n",
            "Epoch 35, Batch 9/21: Loss = 0.8083, Time = 57.34s\n",
            "Epoch 35, Batch 10/21: Loss = 0.8081, Time = 56.81s\n",
            "Epoch 35, Batch 11/21: Loss = 0.7618, Time = 57.38s\n",
            "Epoch 35, Batch 12/21: Loss = 0.7897, Time = 56.81s\n",
            "Epoch 35, Batch 13/21: Loss = 0.7719, Time = 56.89s\n",
            "Epoch 35, Batch 14/21: Loss = 0.8155, Time = 57.33s\n",
            "Epoch 35, Batch 15/21: Loss = 0.7971, Time = 56.84s\n",
            "Epoch 35, Batch 16/21: Loss = 0.7339, Time = 57.23s\n",
            "Epoch 35, Batch 17/21: Loss = 0.7766, Time = 56.91s\n",
            "Epoch 35, Batch 18/21: Loss = 0.7725, Time = 56.76s\n",
            "Epoch 35, Batch 19/21: Loss = 0.7662, Time = 57.17s\n",
            "Epoch 35, Batch 20/21: Loss = 0.7404, Time = 56.87s\n",
            "Epoch 35, Batch 21/21: Loss = 0.7630, Time = 51.53s\n",
            "Epoch 35: Train Loss = 0.7759, Val Loss = 0.7553, Time = 44122.46s\n",
            "Epoch 36, Batch 1/21: Loss = 0.7415, Time = 56.86s\n",
            "Epoch 36, Batch 2/21: Loss = 0.7849, Time = 57.22s\n",
            "Epoch 36, Batch 3/21: Loss = 0.7306, Time = 56.68s\n",
            "Epoch 36, Batch 4/21: Loss = 0.7906, Time = 57.31s\n",
            "Epoch 36, Batch 5/21: Loss = 0.7074, Time = 56.88s\n",
            "Epoch 36, Batch 6/21: Loss = 0.7547, Time = 56.81s\n",
            "Epoch 36, Batch 7/21: Loss = 0.8218, Time = 57.45s\n",
            "Epoch 36, Batch 8/21: Loss = 0.8054, Time = 57.80s\n",
            "Epoch 36, Batch 9/21: Loss = 0.8502, Time = 57.59s\n",
            "Epoch 36, Batch 10/21: Loss = 0.8039, Time = 57.02s\n",
            "Epoch 36, Batch 11/21: Loss = 0.7934, Time = 57.44s\n",
            "Epoch 36, Batch 12/21: Loss = 0.7498, Time = 57.32s\n",
            "Epoch 36, Batch 13/21: Loss = 0.7237, Time = 57.29s\n",
            "Epoch 36, Batch 14/21: Loss = 0.7960, Time = 57.76s\n",
            "Epoch 36, Batch 15/21: Loss = 0.8161, Time = 57.68s\n",
            "Epoch 36, Batch 16/21: Loss = 0.7736, Time = 58.11s\n",
            "Epoch 36, Batch 17/21: Loss = 0.7992, Time = 57.61s\n",
            "Epoch 36, Batch 18/21: Loss = 0.7509, Time = 58.24s\n",
            "Epoch 36, Batch 19/21: Loss = 0.7168, Time = 57.75s\n",
            "Epoch 36, Batch 20/21: Loss = 0.7784, Time = 57.99s\n",
            "Epoch 36, Batch 21/21: Loss = 0.7688, Time = 52.14s\n",
            "Epoch 36: Train Loss = 0.7742, Val Loss = 0.7533, Time = 45347.32s\n",
            "Epoch 37, Batch 1/21: Loss = 0.7144, Time = 57.65s\n",
            "Epoch 37, Batch 2/21: Loss = 0.7935, Time = 58.16s\n",
            "Epoch 37, Batch 3/21: Loss = 0.7676, Time = 57.69s\n",
            "Epoch 37, Batch 4/21: Loss = 0.8405, Time = 57.70s\n",
            "Epoch 37, Batch 5/21: Loss = 0.7573, Time = 57.49s\n",
            "Epoch 37, Batch 6/21: Loss = 0.7206, Time = 57.40s\n",
            "Epoch 37, Batch 7/21: Loss = 0.7509, Time = 58.09s\n",
            "Epoch 37, Batch 8/21: Loss = 0.8164, Time = 58.23s\n",
            "Epoch 37, Batch 9/21: Loss = 0.7599, Time = 58.50s\n",
            "Epoch 37, Batch 10/21: Loss = 0.8638, Time = 57.48s\n",
            "Epoch 37, Batch 11/21: Loss = 0.7979, Time = 58.30s\n",
            "Epoch 37, Batch 12/21: Loss = 0.7764, Time = 57.91s\n",
            "Epoch 37, Batch 13/21: Loss = 0.6476, Time = 58.01s\n",
            "Epoch 37, Batch 14/21: Loss = 0.7733, Time = 58.10s\n",
            "Epoch 37, Batch 15/21: Loss = 0.7931, Time = 57.96s\n",
            "Epoch 37, Batch 16/21: Loss = 0.7720, Time = 58.96s\n",
            "Epoch 37, Batch 17/21: Loss = 0.7706, Time = 58.12s\n",
            "Epoch 37, Batch 18/21: Loss = 0.8049, Time = 57.80s\n",
            "Epoch 37, Batch 19/21: Loss = 0.7810, Time = 59.40s\n",
            "Epoch 37, Batch 20/21: Loss = 0.7285, Time = 58.29s\n",
            "Epoch 37, Batch 21/21: Loss = 0.7911, Time = 52.14s\n",
            "Epoch 37: Train Loss = 0.7725, Val Loss = 0.7517, Time = 46584.88s\n",
            "Epoch 38, Batch 1/21: Loss = 0.7377, Time = 58.04s\n",
            "Epoch 38, Batch 2/21: Loss = 0.7531, Time = 58.92s\n",
            "Epoch 38, Batch 3/21: Loss = 0.8001, Time = 58.80s\n",
            "Epoch 38, Batch 4/21: Loss = 0.7869, Time = 58.64s\n",
            "Epoch 38, Batch 5/21: Loss = 0.7691, Time = 58.37s\n",
            "Epoch 38, Batch 6/21: Loss = 0.7785, Time = 58.31s\n",
            "Epoch 38, Batch 7/21: Loss = 0.7821, Time = 59.42s\n",
            "Epoch 38, Batch 8/21: Loss = 0.7805, Time = 58.18s\n",
            "Epoch 38, Batch 9/21: Loss = 0.7940, Time = 58.40s\n",
            "Epoch 38, Batch 10/21: Loss = 0.8141, Time = 57.65s\n",
            "Epoch 38, Batch 11/21: Loss = 0.7718, Time = 59.71s\n",
            "Epoch 38, Batch 12/21: Loss = 0.7540, Time = 58.22s\n",
            "Epoch 38, Batch 13/21: Loss = 0.7600, Time = 57.98s\n",
            "Epoch 38, Batch 14/21: Loss = 0.8009, Time = 58.65s\n",
            "Epoch 38, Batch 15/21: Loss = 0.8010, Time = 57.99s\n",
            "Epoch 38, Batch 16/21: Loss = 0.7550, Time = 57.82s\n",
            "Epoch 38, Batch 17/21: Loss = 0.7975, Time = 57.10s\n",
            "Epoch 38, Batch 18/21: Loss = 0.6843, Time = 57.59s\n",
            "Epoch 38, Batch 19/21: Loss = 0.7595, Time = 58.06s\n",
            "Epoch 38, Batch 20/21: Loss = 0.7510, Time = 57.87s\n",
            "Epoch 38, Batch 21/21: Loss = 0.7497, Time = 51.91s\n",
            "Epoch 38: Train Loss = 0.7705, Val Loss = 0.7501, Time = 47826.59s\n",
            "Epoch 39, Batch 1/21: Loss = 0.7458, Time = 57.35s\n",
            "Epoch 39, Batch 2/21: Loss = 0.8158, Time = 57.69s\n",
            "Epoch 39, Batch 3/21: Loss = 0.7333, Time = 57.29s\n",
            "Epoch 39, Batch 4/21: Loss = 0.7175, Time = 57.44s\n",
            "Epoch 39, Batch 5/21: Loss = 0.7444, Time = 57.72s\n",
            "Epoch 39, Batch 6/21: Loss = 0.7434, Time = 57.09s\n",
            "Epoch 39, Batch 7/21: Loss = 0.7502, Time = 58.27s\n",
            "Epoch 39, Batch 8/21: Loss = 0.8269, Time = 58.39s\n",
            "Epoch 39, Batch 9/21: Loss = 0.7778, Time = 58.37s\n",
            "Epoch 39, Batch 10/21: Loss = 0.8149, Time = 57.44s\n",
            "Epoch 39, Batch 11/21: Loss = 0.7594, Time = 58.35s\n",
            "Epoch 39, Batch 12/21: Loss = 0.7519, Time = 57.27s\n",
            "Epoch 39, Batch 13/21: Loss = 0.8071, Time = 57.60s\n",
            "Epoch 39, Batch 14/21: Loss = 0.7833, Time = 58.14s\n",
            "Epoch 39, Batch 15/21: Loss = 0.8054, Time = 58.06s\n",
            "Epoch 39, Batch 16/21: Loss = 0.7735, Time = 58.08s\n",
            "Epoch 39, Batch 17/21: Loss = 0.7397, Time = 57.80s\n",
            "Epoch 39, Batch 18/21: Loss = 0.7608, Time = 57.51s\n",
            "Epoch 39, Batch 19/21: Loss = 0.7142, Time = 58.42s\n",
            "Epoch 39, Batch 20/21: Loss = 0.7850, Time = 58.24s\n",
            "Epoch 39, Batch 21/21: Loss = 0.7980, Time = 52.16s\n",
            "Epoch 39: Train Loss = 0.7690, Val Loss = 0.7488, Time = 49059.38s\n",
            "Epoch 40, Batch 1/21: Loss = 0.8651, Time = 58.09s\n",
            "Epoch 40, Batch 2/21: Loss = 0.8217, Time = 57.94s\n",
            "Epoch 40, Batch 3/21: Loss = 0.7596, Time = 57.60s\n",
            "Epoch 40, Batch 4/21: Loss = 0.8095, Time = 58.23s\n",
            "Epoch 40, Batch 5/21: Loss = 0.7728, Time = 58.60s\n",
            "Epoch 40, Batch 6/21: Loss = 0.7600, Time = 57.47s\n",
            "Epoch 40, Batch 7/21: Loss = 0.7338, Time = 57.43s\n",
            "Epoch 40, Batch 8/21: Loss = 0.7751, Time = 56.95s\n",
            "Epoch 40, Batch 9/21: Loss = 0.7506, Time = 57.49s\n",
            "Epoch 40, Batch 10/21: Loss = 0.7758, Time = 57.35s\n",
            "Epoch 40, Batch 11/21: Loss = 0.7380, Time = 57.85s\n",
            "Epoch 40, Batch 12/21: Loss = 0.7594, Time = 57.35s\n",
            "Epoch 40, Batch 13/21: Loss = 0.7522, Time = 56.94s\n",
            "Epoch 40, Batch 14/21: Loss = 0.7659, Time = 57.65s\n",
            "Epoch 40, Batch 15/21: Loss = 0.7415, Time = 57.29s\n",
            "Epoch 40, Batch 16/21: Loss = 0.7498, Time = 57.22s\n",
            "Epoch 40, Batch 17/21: Loss = 0.6885, Time = 56.91s\n",
            "Epoch 40, Batch 18/21: Loss = 0.7389, Time = 57.06s\n",
            "Epoch 40, Batch 19/21: Loss = 0.8314, Time = 57.31s\n",
            "Epoch 40, Batch 20/21: Loss = 0.7679, Time = 56.73s\n",
            "Epoch 40, Batch 21/21: Loss = 0.7496, Time = 51.45s\n",
            "Epoch 40: Train Loss = 0.7670, Val Loss = 0.7472, Time = 50284.27s\n",
            "Epoch 41, Batch 1/21: Loss = 0.8270, Time = 56.67s\n",
            "Epoch 41, Batch 2/21: Loss = 0.6905, Time = 57.03s\n",
            "Epoch 41, Batch 3/21: Loss = 0.7815, Time = 56.61s\n",
            "Epoch 41, Batch 4/21: Loss = 0.7719, Time = 57.37s\n",
            "Epoch 41, Batch 5/21: Loss = 0.7254, Time = 56.90s\n",
            "Epoch 41, Batch 6/21: Loss = 0.7703, Time = 56.64s\n",
            "Epoch 41, Batch 7/21: Loss = 0.8387, Time = 57.42s\n",
            "Epoch 41, Batch 8/21: Loss = 0.7936, Time = 56.96s\n",
            "Epoch 41, Batch 9/21: Loss = 0.7615, Time = 57.41s\n",
            "Epoch 41, Batch 10/21: Loss = 0.6903, Time = 56.66s\n",
            "Epoch 41, Batch 11/21: Loss = 0.8102, Time = 57.35s\n",
            "Epoch 41, Batch 12/21: Loss = 0.7785, Time = 56.79s\n",
            "Epoch 41, Batch 13/21: Loss = 0.7883, Time = 56.71s\n",
            "Epoch 41, Batch 14/21: Loss = 0.7327, Time = 57.28s\n",
            "Epoch 41, Batch 15/21: Loss = 0.7455, Time = 56.86s\n",
            "Epoch 41, Batch 16/21: Loss = 0.7927, Time = 57.42s\n",
            "Epoch 41, Batch 17/21: Loss = 0.7313, Time = 57.05s\n",
            "Epoch 41, Batch 18/21: Loss = 0.7561, Time = 56.96s\n",
            "Epoch 41, Batch 19/21: Loss = 0.7452, Time = 57.42s\n",
            "Epoch 41, Batch 20/21: Loss = 0.7595, Time = 56.91s\n",
            "Epoch 41, Batch 21/21: Loss = 0.7837, Time = 51.47s\n",
            "Epoch 41: Train Loss = 0.7655, Val Loss = 0.7457, Time = 51500.35s\n",
            "Epoch 42, Batch 1/21: Loss = 0.7270, Time = 56.99s\n",
            "Epoch 42, Batch 2/21: Loss = 0.8020, Time = 57.06s\n",
            "Epoch 42, Batch 3/21: Loss = 0.8022, Time = 56.76s\n",
            "Epoch 42, Batch 4/21: Loss = 0.8025, Time = 57.18s\n",
            "Epoch 42, Batch 5/21: Loss = 0.7513, Time = 57.17s\n",
            "Epoch 42, Batch 6/21: Loss = 0.8005, Time = 56.82s\n",
            "Epoch 42, Batch 7/21: Loss = 0.7898, Time = 57.11s\n",
            "Epoch 42, Batch 8/21: Loss = 0.7755, Time = 56.65s\n",
            "Epoch 42, Batch 9/21: Loss = 0.7311, Time = 57.32s\n",
            "Epoch 42, Batch 10/21: Loss = 0.7993, Time = 57.24s\n",
            "Epoch 42, Batch 11/21: Loss = 0.7608, Time = 57.06s\n",
            "Epoch 42, Batch 12/21: Loss = 0.7485, Time = 56.78s\n",
            "Epoch 42, Batch 13/21: Loss = 0.8082, Time = 56.78s\n",
            "Epoch 42, Batch 14/21: Loss = 0.6667, Time = 57.37s\n",
            "Epoch 42, Batch 15/21: Loss = 0.7411, Time = 56.73s\n",
            "Epoch 42, Batch 16/21: Loss = 0.7323, Time = 57.41s\n",
            "Epoch 42, Batch 17/21: Loss = 0.7537, Time = 57.31s\n",
            "Epoch 42, Batch 18/21: Loss = 0.7227, Time = 57.48s\n",
            "Epoch 42, Batch 19/21: Loss = 0.7878, Time = 57.69s\n",
            "Epoch 42, Batch 20/21: Loss = 0.7686, Time = 57.14s\n",
            "Epoch 42, Batch 21/21: Loss = 0.7650, Time = 51.55s\n",
            "Epoch 42: Train Loss = 0.7637, Val Loss = 0.7441, Time = 52718.05s\n",
            "Epoch 43, Batch 1/21: Loss = 0.7473, Time = 56.81s\n",
            "Epoch 43, Batch 2/21: Loss = 0.8017, Time = 57.48s\n",
            "Epoch 43, Batch 3/21: Loss = 0.8141, Time = 56.95s\n",
            "Epoch 43, Batch 4/21: Loss = 0.7230, Time = 57.53s\n",
            "Epoch 43, Batch 5/21: Loss = 0.7475, Time = 57.29s\n",
            "Epoch 43, Batch 6/21: Loss = 0.7676, Time = 56.85s\n",
            "Epoch 43, Batch 7/21: Loss = 0.7137, Time = 57.53s\n",
            "Epoch 43, Batch 8/21: Loss = 0.7136, Time = 56.77s\n",
            "Epoch 43, Batch 9/21: Loss = 0.7511, Time = 56.99s\n",
            "Epoch 43, Batch 10/21: Loss = 0.7772, Time = 56.74s\n",
            "Epoch 43, Batch 11/21: Loss = 0.8156, Time = 56.97s\n",
            "Epoch 43, Batch 12/21: Loss = 0.7307, Time = 56.60s\n",
            "Epoch 43, Batch 13/21: Loss = 0.7413, Time = 56.77s\n",
            "Epoch 43, Batch 14/21: Loss = 0.7305, Time = 57.48s\n",
            "Epoch 43, Batch 15/21: Loss = 0.7790, Time = 57.01s\n",
            "Epoch 43, Batch 16/21: Loss = 0.7567, Time = 57.37s\n",
            "Epoch 43, Batch 17/21: Loss = 0.7620, Time = 57.16s\n",
            "Epoch 43, Batch 18/21: Loss = 0.7698, Time = 56.95s\n",
            "Epoch 43, Batch 19/21: Loss = 0.7780, Time = 57.39s\n",
            "Epoch 43, Batch 20/21: Loss = 0.7706, Time = 56.85s\n",
            "Epoch 43, Batch 21/21: Loss = 0.8154, Time = 51.33s\n",
            "Epoch 43: Train Loss = 0.7622, Val Loss = 0.7423, Time = 53934.96s\n",
            "Epoch 44, Batch 1/21: Loss = 0.7543, Time = 56.82s\n",
            "Epoch 44, Batch 2/21: Loss = 0.8142, Time = 57.35s\n",
            "Epoch 44, Batch 3/21: Loss = 0.8306, Time = 56.82s\n",
            "Epoch 44, Batch 4/21: Loss = 0.7369, Time = 57.26s\n",
            "Epoch 44, Batch 5/21: Loss = 0.7632, Time = 56.81s\n",
            "Epoch 44, Batch 6/21: Loss = 0.7992, Time = 56.41s\n",
            "Epoch 44, Batch 7/21: Loss = 0.7467, Time = 57.18s\n",
            "Epoch 44, Batch 8/21: Loss = 0.7804, Time = 56.94s\n",
            "Epoch 44, Batch 9/21: Loss = 0.7530, Time = 57.23s\n",
            "Epoch 44, Batch 10/21: Loss = 0.7385, Time = 56.90s\n",
            "Epoch 44, Batch 11/21: Loss = 0.7571, Time = 57.32s\n",
            "Epoch 44, Batch 12/21: Loss = 0.7722, Time = 57.00s\n",
            "Epoch 44, Batch 13/21: Loss = 0.7636, Time = 57.13s\n",
            "Epoch 44, Batch 14/21: Loss = 0.7951, Time = 57.24s\n",
            "Epoch 44, Batch 15/21: Loss = 0.7619, Time = 56.62s\n",
            "Epoch 44, Batch 16/21: Loss = 0.7219, Time = 57.37s\n",
            "Epoch 44, Batch 17/21: Loss = 0.7333, Time = 56.83s\n",
            "Epoch 44, Batch 18/21: Loss = 0.7401, Time = 56.93s\n",
            "Epoch 44, Batch 19/21: Loss = 0.7632, Time = 57.41s\n",
            "Epoch 44, Batch 20/21: Loss = 0.7158, Time = 57.37s\n",
            "Epoch 44, Batch 21/21: Loss = 0.7210, Time = 51.53s\n",
            "Epoch 44: Train Loss = 0.7601, Val Loss = 0.7412, Time = 55151.41s\n",
            "Epoch 45, Batch 1/21: Loss = 0.7107, Time = 57.23s\n",
            "Epoch 45, Batch 2/21: Loss = 0.7368, Time = 57.50s\n",
            "Epoch 45, Batch 3/21: Loss = 0.7664, Time = 56.85s\n",
            "Epoch 45, Batch 4/21: Loss = 0.7664, Time = 57.58s\n",
            "Epoch 45, Batch 5/21: Loss = 0.7902, Time = 57.11s\n",
            "Epoch 45, Batch 6/21: Loss = 0.7402, Time = 57.02s\n",
            "Epoch 45, Batch 7/21: Loss = 0.7821, Time = 57.66s\n",
            "Epoch 45, Batch 8/21: Loss = 0.7837, Time = 56.99s\n",
            "Epoch 45, Batch 9/21: Loss = 0.7218, Time = 57.25s\n",
            "Epoch 45, Batch 10/21: Loss = 0.8005, Time = 56.93s\n",
            "Epoch 45, Batch 11/21: Loss = 0.7527, Time = 57.43s\n",
            "Epoch 45, Batch 12/21: Loss = 0.7698, Time = 56.94s\n",
            "Epoch 45, Batch 13/21: Loss = 0.7766, Time = 56.98s\n",
            "Epoch 45, Batch 14/21: Loss = 0.7391, Time = 57.56s\n",
            "Epoch 45, Batch 15/21: Loss = 0.7559, Time = 56.90s\n",
            "Epoch 45, Batch 16/21: Loss = 0.7319, Time = 57.41s\n",
            "Epoch 45, Batch 17/21: Loss = 0.7442, Time = 57.01s\n",
            "Epoch 45, Batch 18/21: Loss = 0.7208, Time = 56.65s\n",
            "Epoch 45, Batch 19/21: Loss = 0.8184, Time = 57.23s\n",
            "Epoch 45, Batch 20/21: Loss = 0.7591, Time = 56.72s\n",
            "Epoch 45, Batch 21/21: Loss = 0.7640, Time = 51.34s\n",
            "Epoch 45: Train Loss = 0.7586, Val Loss = 0.7397, Time = 56369.90s\n",
            "Epoch 46, Batch 1/21: Loss = 0.8092, Time = 56.66s\n",
            "Epoch 46, Batch 2/21: Loss = 0.7786, Time = 57.17s\n",
            "Epoch 46, Batch 3/21: Loss = 0.7923, Time = 56.80s\n",
            "Epoch 46, Batch 4/21: Loss = 0.7282, Time = 57.24s\n",
            "Epoch 46, Batch 5/21: Loss = 0.7351, Time = 56.98s\n",
            "Epoch 46, Batch 6/21: Loss = 0.8219, Time = 56.78s\n",
            "Epoch 46, Batch 7/21: Loss = 0.6944, Time = 57.29s\n",
            "Epoch 46, Batch 8/21: Loss = 0.7262, Time = 56.97s\n",
            "Epoch 46, Batch 9/21: Loss = 0.7675, Time = 57.60s\n",
            "Epoch 46, Batch 10/21: Loss = 0.8470, Time = 57.00s\n",
            "Epoch 46, Batch 11/21: Loss = 0.7285, Time = 57.52s\n",
            "Epoch 46, Batch 12/21: Loss = 0.7748, Time = 56.78s\n",
            "Epoch 46, Batch 13/21: Loss = 0.7413, Time = 57.22s\n",
            "Epoch 46, Batch 14/21: Loss = 0.7408, Time = 57.32s\n",
            "Epoch 46, Batch 15/21: Loss = 0.7376, Time = 56.61s\n",
            "Epoch 46, Batch 16/21: Loss = 0.7828, Time = 57.19s\n",
            "Epoch 46, Batch 17/21: Loss = 0.7449, Time = 56.69s\n",
            "Epoch 46, Batch 18/21: Loss = 0.6881, Time = 56.58s\n",
            "Epoch 46, Batch 19/21: Loss = 0.7522, Time = 57.28s\n",
            "Epoch 46, Batch 20/21: Loss = 0.7605, Time = 56.73s\n",
            "Epoch 46, Batch 21/21: Loss = 0.7436, Time = 51.31s\n",
            "Epoch 46: Train Loss = 0.7569, Val Loss = 0.7385, Time = 57585.64s\n",
            "Epoch 47, Batch 1/21: Loss = 0.7100, Time = 56.78s\n",
            "Epoch 47, Batch 2/21: Loss = 0.7702, Time = 57.32s\n",
            "Epoch 47, Batch 3/21: Loss = 0.7254, Time = 56.73s\n",
            "Epoch 47, Batch 4/21: Loss = 0.7231, Time = 57.13s\n",
            "Epoch 47, Batch 5/21: Loss = 0.7453, Time = 56.87s\n",
            "Epoch 47, Batch 6/21: Loss = 0.7669, Time = 56.99s\n",
            "Epoch 47, Batch 7/21: Loss = 0.7777, Time = 57.61s\n",
            "Epoch 47, Batch 8/21: Loss = 0.7659, Time = 56.76s\n",
            "Epoch 47, Batch 9/21: Loss = 0.7594, Time = 57.21s\n",
            "Epoch 47, Batch 10/21: Loss = 0.8259, Time = 56.81s\n",
            "Epoch 47, Batch 11/21: Loss = 0.7263, Time = 57.21s\n",
            "Epoch 47, Batch 12/21: Loss = 0.7389, Time = 57.15s\n",
            "Epoch 47, Batch 13/21: Loss = 0.8210, Time = 57.08s\n",
            "Epoch 47, Batch 14/21: Loss = 0.7747, Time = 57.50s\n",
            "Epoch 47, Batch 15/21: Loss = 0.7792, Time = 57.36s\n",
            "Epoch 47, Batch 16/21: Loss = 0.7337, Time = 57.48s\n",
            "Epoch 47, Batch 17/21: Loss = 0.7418, Time = 56.81s\n",
            "Epoch 47, Batch 18/21: Loss = 0.7138, Time = 57.03s\n",
            "Epoch 47, Batch 19/21: Loss = 0.7895, Time = 57.63s\n",
            "Epoch 47, Batch 20/21: Loss = 0.7436, Time = 57.10s\n",
            "Epoch 47, Batch 21/21: Loss = 0.7297, Time = 51.64s\n",
            "Epoch 47: Train Loss = 0.7553, Val Loss = 0.7370, Time = 58803.89s\n",
            "Epoch 48, Batch 1/21: Loss = 0.7706, Time = 56.80s\n",
            "Epoch 48, Batch 2/21: Loss = 0.8127, Time = 57.34s\n",
            "Epoch 48, Batch 3/21: Loss = 0.7586, Time = 56.98s\n",
            "Epoch 48, Batch 4/21: Loss = 0.7931, Time = 57.51s\n",
            "Epoch 48, Batch 5/21: Loss = 0.7784, Time = 56.84s\n",
            "Epoch 48, Batch 6/21: Loss = 0.7227, Time = 56.82s\n",
            "Epoch 48, Batch 7/21: Loss = 0.7806, Time = 57.13s\n",
            "Epoch 48, Batch 8/21: Loss = 0.7560, Time = 56.79s\n",
            "Epoch 48, Batch 9/21: Loss = 0.7384, Time = 57.34s\n",
            "Epoch 48, Batch 10/21: Loss = 0.7259, Time = 57.10s\n",
            "Epoch 48, Batch 11/21: Loss = 0.7957, Time = 57.31s\n",
            "Epoch 48, Batch 12/21: Loss = 0.6877, Time = 56.89s\n",
            "Epoch 48, Batch 13/21: Loss = 0.7084, Time = 56.85s\n",
            "Epoch 48, Batch 14/21: Loss = 0.7316, Time = 57.29s\n",
            "Epoch 48, Batch 15/21: Loss = 0.6492, Time = 56.85s\n",
            "Epoch 48, Batch 16/21: Loss = 0.7577, Time = 57.54s\n",
            "Epoch 48, Batch 17/21: Loss = 0.7486, Time = 56.74s\n",
            "Epoch 48, Batch 18/21: Loss = 0.7784, Time = 56.84s\n",
            "Epoch 48, Batch 19/21: Loss = 0.7461, Time = 57.29s\n",
            "Epoch 48, Batch 20/21: Loss = 0.7975, Time = 56.84s\n",
            "Epoch 48, Batch 21/21: Loss = 0.7991, Time = 51.21s\n",
            "Epoch 48: Train Loss = 0.7541, Val Loss = 0.7358, Time = 60020.33s\n",
            "Epoch 49, Batch 1/21: Loss = 0.7880, Time = 56.84s\n",
            "Epoch 49, Batch 2/21: Loss = 0.7300, Time = 57.33s\n",
            "Epoch 49, Batch 3/21: Loss = 0.7688, Time = 57.00s\n",
            "Epoch 49, Batch 4/21: Loss = 0.7314, Time = 57.40s\n",
            "Epoch 49, Batch 5/21: Loss = 0.7418, Time = 57.05s\n",
            "Epoch 49, Batch 6/21: Loss = 0.7155, Time = 56.64s\n",
            "Epoch 49, Batch 7/21: Loss = 0.7921, Time = 57.40s\n",
            "Epoch 49, Batch 8/21: Loss = 0.7502, Time = 57.34s\n",
            "Epoch 49, Batch 9/21: Loss = 0.7799, Time = 57.54s\n",
            "Epoch 49, Batch 10/21: Loss = 0.7326, Time = 56.90s\n",
            "Epoch 49, Batch 11/21: Loss = 0.7326, Time = 57.46s\n",
            "Epoch 49, Batch 12/21: Loss = 0.7638, Time = 57.06s\n",
            "Epoch 49, Batch 13/21: Loss = 0.7458, Time = 57.30s\n",
            "Epoch 49, Batch 14/21: Loss = 0.8165, Time = 57.54s\n",
            "Epoch 49, Batch 15/21: Loss = 0.7376, Time = 57.21s\n",
            "Epoch 49, Batch 16/21: Loss = 0.6893, Time = 57.65s\n",
            "Epoch 49, Batch 17/21: Loss = 0.8016, Time = 57.14s\n",
            "Epoch 49, Batch 18/21: Loss = 0.7547, Time = 57.00s\n",
            "Epoch 49, Batch 19/21: Loss = 0.7760, Time = 57.50s\n",
            "Epoch 49, Batch 20/21: Loss = 0.7329, Time = 56.95s\n",
            "Epoch 49, Batch 21/21: Loss = 0.7166, Time = 51.85s\n",
            "Epoch 49: Train Loss = 0.7523, Val Loss = 0.7347, Time = 61240.53s\n",
            "Training completed in 61240.53 seconds.\n",
            "QNN Test Results: MSE: 0.7632, RMSE: 0.8736, R²: 0.0895, MAE: 0.7379\n",
            "QNN Original Scale - MSE: 23.8765, RMSE: 4.8864, R2: -0.0236, MAE: 3.1288\n",
            "Starting SVR training...\n",
            "SVR Results: MSE: 0.0140, RMSE: 0.1185, R²: 0.9833, MAE: 0.0905\n",
            "SVR training completed in 23.05 seconds.\n",
            "SVR Original Scale - MSE: 1.0212, RMSE: 1.0106, R2: 0.9562, MAE: 0.5008\n",
            "Generating plots...\n",
            "Output plots saved: 3d_time_temp.png, log_concentration.png, shear_bar.png\n",
            "Predictions saved to predictions.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-34-3643313008.py:301: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  shear_avg = df_sample.groupby(shear_bins)['corrosion_mm_yr'].mean()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Replace with your actual Excel file path\n",
        "data_path = 'SequentialDataInhibitor_large.xlsx'\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# Analyze data\n",
        "df = analyze_data(data_path)\n",
        "\n",
        "# Preprocess data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, preprocessor, df_actual = preprocess_data(data_path)\n",
        "\n",
        "# Option to run only SVR for quick baseline\n",
        "run_svr_only = False  # Set to True to skip QNN and run only SVR\n",
        "if run_svr_only:\n",
        "    svr, y_pred_svr = train_svr(X_train, y_train, X_test, y_test)\n",
        "    pd.DataFrame({\n",
        "        'y_test': np.expm1(y_test),  # Reverse log transformation\n",
        "        'y_pred_svr': np.expm1(y_pred_svr)\n",
        "    }).to_csv('predictions_svr.csv', index=False)\n",
        "else:\n",
        "    # Train QNN\n",
        "    model = train_qnn(X_train, y_train, X_val, y_val, batch_size=512)\n",
        "\n",
        "    # Evaluate QNN\n",
        "    mse, rmse, r2, mae, y_pred_qnn = evaluate_model(model, X_test, y_test)\n",
        "    # Reverse log transformation for metrics\n",
        "    y_test_orig = np.expm1(y_test)\n",
        "    y_pred_qnn_orig = np.expm1(y_pred_qnn)\n",
        "    mse_orig = mean_squared_error(y_test_orig, y_pred_qnn_orig)\n",
        "    rmse_orig = np.sqrt(mse_orig)\n",
        "    r2_orig = r2_score(y_test_orig, y_pred_qnn_orig)\n",
        "    mae_orig = mean_absolute_error(y_test_orig, y_pred_qnn_orig)\n",
        "    print(f\"QNN Original Scale - MSE: {mse_orig:.4f}, RMSE: {rmse_orig:.4f}, R2: {r2_orig:.4f}, MAE: {mae_orig:.4f}\")\n",
        "\n",
        "    # Train and evaluate SVR\n",
        "    svr, y_pred_svr = train_svr(X_train, y_train, X_test, y_test)\n",
        "    y_pred_svr_orig = np.expm1(y_pred_svr)\n",
        "    mse_svr_orig = mean_squared_error(y_test_orig, y_pred_svr_orig)\n",
        "    rmse_svr_orig = np.sqrt(mse_svr_orig)\n",
        "    r2_svr_orig = r2_score(y_test_orig, y_pred_svr_orig)\n",
        "    mae_svr_orig = mean_absolute_error(y_test_orig, y_pred_svr_orig)\n",
        "    print(f\"SVR Original Scale - MSE: {mse_svr_orig:.4f}, RMSE: {rmse_svr_orig:.4f}, R2: {r2_svr_orig:.4f}, MAE: {mae_svr_orig:.4f}\")\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    pd.DataFrame({\n",
        "        'y_test': y_test_orig,\n",
        "        'y_pred_qnn': y_pred_qnn_orig,\n",
        "        'y_pred_svr': y_pred_svr_orig\n",
        "    }).to_csv('predictions.csv', index=False)\n",
        "\n",
        "# Generate plots\n",
        "plot_results(df_actual, sim_data=None)\n",
        "\n",
        "print(\"Output plots saved: 3d_time_temp.png, log_concentration.png, shear_bar.png\")\n",
        "print(\"Predictions saved to predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import pennylane as qml\n",
        "import matplotlib.pyplot as plt\n",
        "def preprocess_data(data_path):\n",
        "    print(\"Loading data from Excel sheets...\")\n",
        "    df = pd.concat(pd.read_excel(data_path, sheet_name=None), ignore_index=True)\n",
        "\n",
        "    # Remove outliers based on corrosion_mm_yr\n",
        "    mean = df['corrosion_mm_yr'].mean()\n",
        "    std = df['corrosion_mm_yr'].std()\n",
        "    df = df[(df['corrosion_mm_yr'] >= mean - 3 * std) & (df['corrosion_mm_yr'] <= mean + 3 * std)]\n",
        "\n",
        "    # Apply log transformation to corrosion_mm_yr\n",
        "    df['corrosion_mm_yr'] = np.log1p(df['corrosion_mm_yr'])\n",
        "\n",
        "    # Select features (add pH if numeric)\n",
        "    feature_columns = ['concentration_ppm', 'time_hrs', 'Pressure_bar_CO2', 'Temperature_C', 'Shear_Pa', 'Brine_Ionic_Strength']\n",
        "    if df['pH'].dtype in [np.float64, np.int64]:\n",
        "        feature_columns.append('pH')\n",
        "\n",
        "    X = df[feature_columns].values\n",
        "    y = df['corrosion_mm_yr'].values\n",
        "    df_actual = df.copy()\n",
        "\n",
        "    # Standardize features\n",
        "    preprocessor = StandardScaler()\n",
        "    X = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split data\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.833, random_state=42)\n",
        "\n",
        "    print(f\"Loaded {len(df)} samples after removing invalid entries.\")\n",
        "    print(f\"Preprocessing completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)} samples\")\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, preprocessor, df_actual\n",
        "# 2. Quantum Circuit Construction (Algorithm 2)\n",
        "n_qubits = 4  # Reduced for speed\n",
        "n_layers = 2  # Reduced for speed\n",
        "try:\n",
        "    dev = qml.device('lightning.gpu', wires=n_qubits)\n",
        "    print(\"Using GPU device: lightning.gpu\")\n",
        "except Exception as e:\n",
        "    print(f\"GPU device not available: {e}. Falling back to CPU.\")\n",
        "    dev = qml.device('default.qubit', wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_circuit(inputs, weights):\n",
        "    # Feature encoding\n",
        "    for i in range(n_qubits):\n",
        "        qml.RX(np.pi * inputs[i], wires=i)\n",
        "\n",
        "    # Variational layers\n",
        "    for l in range(n_layers):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RZ(weights[l, i, 0], wires=i)\n",
        "            qml.RY(weights[l, i, 1], wires=i)\n",
        "            qml.RZ(weights[l, i, 2], wires=i)\n",
        "        for i in range(n_qubits-1):\n",
        "            qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "    # Measurement\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# 3. QNN Model (Algorithm 3)\n",
        "class QNN(torch.nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers, n_features, n_hidden=16, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        super().__init__()\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_layers = n_layers\n",
        "        self.n_features = n_features\n",
        "        self.n_hidden = n_hidden\n",
        "        self.device = device\n",
        "        # Initialize parameters as torch tensors\n",
        "        self.weights = torch.nn.Parameter(torch.randn(n_layers, n_qubits, 3, device=self.device))\n",
        "        self.pre_weights = torch.nn.Parameter(torch.randn(n_hidden, n_qubits, device=self.device))\n",
        "        self.pre_bias = torch.nn.Parameter(torch.zeros(n_hidden, device=self.device))\n",
        "        self.post_weights = torch.nn.Parameter(torch.randn(1, n_hidden, device=self.device))\n",
        "        self.post_bias = torch.nn.Parameter(torch.zeros(1, device=self.device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure batch dimension\n",
        "        if x.ndim == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        # Ensure input matches n_qubits\n",
        "        if x.shape[1] > self.n_qubits:\n",
        "            x = x[:, :self.n_qubits]\n",
        "        elif x.shape[1] < self.n_qubits:\n",
        "            padding = torch.zeros(x.shape[0], self.n_qubits - x.shape[1], device=self.device)\n",
        "            x = torch.cat([x, padding], dim=1)\n",
        "        # Process batch\n",
        "        q_out = torch.stack([torch.stack(quantum_circuit(x[i], self.weights)) for i in range(x.shape[0])])\n",
        "        # Classical layers\n",
        "        hidden = torch.matmul(self.pre_weights, q_out.T).T + self.pre_bias\n",
        "        hidden = torch.tanh(hidden)\n",
        "        output = torch.matmul(self.post_weights, hidden.T).T + self.post_bias\n",
        "        return output.squeeze(-1)\n",
        "\n",
        "def train_qnn(X_train, y_train, X_val, y_val, epochs=50, lr=0.001, patience=10, batch_size=256):\n",
        "    print(\"Starting QNN training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    model = QNN(n_qubits, n_layers, n_features=X_train.shape[1])\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = patience\n",
        "    best_params = None\n",
        "\n",
        "    # Convert data to torch tensors upfront\n",
        "    X_train_torch = torch.as_tensor(X_train, dtype=torch.float32, device=model.device)\n",
        "    y_train_torch = torch.as_tensor(y_train, dtype=torch.float32, device=model.device)\n",
        "    X_val_torch = torch.as_tensor(X_val, dtype=torch.float32, device=model.device)\n",
        "    y_val_torch = torch.as_tensor(y_val, dtype=torch.float32, device=model.device)\n",
        "\n",
        "    n_batches = len(X_train) // batch_size + (1 if len(X_train) % batch_size else 0)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle data\n",
        "        indices = torch.randperm(len(X_train))\n",
        "        X_train_shuffled = X_train_torch[indices]\n",
        "        y_train_shuffled = y_train_torch[indices]\n",
        "\n",
        "        # Training\n",
        "        train_loss = 0\n",
        "        for batch in range(n_batches):\n",
        "            batch_start_time = time.time()\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = min((batch + 1) * batch_size, len(X_train))\n",
        "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
        "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
        "\n",
        "            def cost_fn():\n",
        "                preds = model.forward(X_batch)\n",
        "                return torch.mean((preds - y_batch) ** 2)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss = cost_fn()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss += loss.item()\n",
        "            print(f\"Epoch {epoch}, Batch {batch + 1}/{n_batches}: Loss = {loss.item():.4f}, Time = {time.time() - batch_start_time:.2f}s\")\n",
        "\n",
        "        train_loss /= n_batches\n",
        "\n",
        "        # Validation\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            preds = model.forward(X_val_torch)\n",
        "            val_loss = torch.mean((preds - y_val_torch) ** 2).item()\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_params = {k: v.clone().detach() for k, v in model.state_dict().items()}\n",
        "            patience_counter = patience\n",
        "        else:\n",
        "            patience_counter -= 1\n",
        "            if patience_counter == 0:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Time = {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    # Restore best parameters\n",
        "    model.load_state_dict(best_params)\n",
        "    print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return model\n",
        "\n",
        "# Plotting function for comparison\n",
        "def plot_predictions(y_test, y_pred_qnn, y_pred_svr, filename='predictions_comparison.png'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred_qnn, label='QNN Predictions', alpha=0.5)\n",
        "    plt.scatter(y_test, y_pred_svr, label='SVR Predictions', alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
        "    plt.xlabel('True Corrosion Rate (mm/yr)')\n",
        "    plt.ylabel('Predicted Corrosion Rate (mm/yr)')\n",
        "    plt.title('QNN vs SVR Predictions')\n",
        "    plt.legend()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Function to analyze data distribution\n",
        "def analyze_data(data_path):\n",
        "    print(\"Analyzing data distribution...\")\n",
        "    df = pd.concat(pd.read_excel(data_path, sheet_name=None), ignore_index=True)\n",
        "    # Select only numeric columns for correlation\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    print(\"Data summary:\")\n",
        "    print(df[numeric_cols].describe())\n",
        "    print(\"\\nCorrelation with corrosion_mm_yr:\")\n",
        "    print(df[numeric_cols].corr()['corrosion_mm_yr'].sort_values())\n",
        "    print(\"\\nMissing values:\")\n",
        "    print(df.isnull().sum())\n",
        "    # Check for outliers in corrosion_mm_yr\n",
        "    mean = df['corrosion_mm_yr'].mean()\n",
        "    std = df['corrosion_mm_yr'].std()\n",
        "    outliers = df[(df['corrosion_mm_yr'] > mean + 3 * std) | (df['corrosion_mm_yr'] < mean - 3 * std)]\n",
        "    print(f\"\\nOutliers in corrosion_mm_yr (>{mean + 3 * std:.2f} or <{mean - 3 * std:.2f}):\")\n",
        "    print(f\"Number of outliers: {len(outliers)}\")\n",
        "    return df\n",
        "\n",
        "# Update evaluate_model for vectorized input\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    X_test_torch = torch.tensor(X_test, dtype=torch.float32, device=model.device)\n",
        "    y_test_torch = torch.tensor(y_test, dtype=torch.float32, device=model.device)\n",
        "    with torch.no_grad():\n",
        "        y_pred = model.forward(X_test_torch).cpu().numpy()\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(f\"QNN Test Results: MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}, MAE: {mae:.4f}\")\n",
        "    return mse, rmse, r2, mae, y_pred\n",
        "\n",
        "\n",
        "# 5. Data Simulation for Visualization (Algorithm 5)\n",
        "def simulate_data():\n",
        "    C_0 = 15\n",
        "    beta = 0.15\n",
        "    gamma = 0.1\n",
        "    alpha = 0.5\n",
        "    T_0 = 100\n",
        "    P_CO2 = 10.408  # Fixed for Experiment 1\n",
        "\n",
        "    time = np.arange(1, 4, 0.165829146)\n",
        "    temp = np.arange(104, 117.5, 0.5)\n",
        "    conc = np.arange(100, 310, 10)\n",
        "    shear = np.arange(210, 263, 1)\n",
        "\n",
        "    data_sim = []\n",
        "    for t in time:\n",
        "        for T in temp:\n",
        "            for I in conc:\n",
        "                for S in shear:\n",
        "                    C = C_0 * np.exp(-beta * t) * (1 + gamma * P_CO2) * \\\n",
        "                        (1 + alpha * (T - T_0) / T_0) * (1 - I / (I + 100))\n",
        "                    data_sim.append([t, T, I, S, C])\n",
        "\n",
        "    return pd.DataFrame(data_sim, columns=['time_hrs', 'Temperature_C',\n",
        "                                          'concentration_ppm', 'Shear_Pa', 'corrosion_mm_yr'])\n",
        "\n",
        "# 6. Visualization (Figures 1-3)\n",
        "def plot_results(df_actual, sim_data=None):\n",
        "    print(\"Generating plots...\")\n",
        "    # Subsample actual data for visualization to avoid overcrowding\n",
        "    df_sample = df_actual.sample(n=min(1000, len(df_actual)), random_state=42)\n",
        "\n",
        "    # Figure 1: 3D Surface Plot (Time vs Temperature vs Corrosion Rate)\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    if sim_data is not None:\n",
        "        surf_data = sim_data[sim_data['concentration_ppm'] == 200]\n",
        "        ax.plot_trisurf(surf_data['time_hrs'], surf_data['Temperature_C'],\n",
        "                        surf_data['corrosion_mm_yr'], cmap='Blues', alpha=0.8, label='Simulated')\n",
        "    ax.scatter(df_sample['time_hrs'], df_sample['Temperature_C'],\n",
        "               df_sample['corrosion_mm_yr'], color='red', label='Actual Data')\n",
        "    ax.set_xlabel('Time (hrs)')\n",
        "    ax.set_ylabel('Temperature (°C)')\n",
        "    ax.set_zlabel('Corrosion Rate (mm/yr)')\n",
        "    ax.set_title('3D Surface Plot of Corrosion Rate')\n",
        "    ax.legend()\n",
        "    plt.savefig('3d_time_temp.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 2: Logarithmic Scatter Plot (Inhibitor Concentration vs Corrosion Rate)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    if sim_data is not None:\n",
        "        sns.scatterplot(x='concentration_ppm', y='corrosion_mm_yr',\n",
        "                        data=sim_data[sim_data['concentration_ppm'] == 200],\n",
        "                        label='Simulated Data (200 ppm)', color='blue', marker='s')\n",
        "    sns.scatterplot(x='concentration_ppm', y='corrosion_mm_yr',\n",
        "                    data=df_sample, label='Actual Data', color='red', marker='o')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Inhibitor Concentration (ppm)')\n",
        "    plt.ylabel('Corrosion Rate (mm/yr)')\n",
        "    plt.title('Logarithmic Scatter Plot of Corrosion Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    plt.savefig('log_concentration.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Figure 3: Bar Plot (Shear Stress vs Average Corrosion Rate)\n",
        "    shear_bins = pd.cut(df_sample['Shear_Pa'], bins=[210, 220, 230, 240, 250, 263],\n",
        "                        labels=['210-220', '220-230', '230-240', '240-250', '250-263'])\n",
        "    shear_avg = df_sample.groupby(shear_bins)['corrosion_mm_yr'].mean()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    shear_avg.plot(kind='bar', color='purple')\n",
        "    plt.xlabel('Shear Stress (Pa)')\n",
        "    plt.ylabel('Average Corrosion Rate (mm/yr)')\n",
        "    plt.title('Average Corrosion Rate by Shear Stress')\n",
        "    plt.grid(True, ls=\"--\")\n",
        "    plt.savefig('shear_bar.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTihGKZEd1BK",
        "outputId": "c8a9ef4b-af6e-45ca-e81e-735154bb69f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU device: lightning.gpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'SingleDoesdataInhibitor_large.xlsx'\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# Analyze data\n",
        "df = analyze_data(data_path)\n",
        "\n",
        "# Preprocess data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, preprocessor, df_actual = preprocess_data(data_path)\n",
        "\n",
        "# Option to run only SVR for quick baseline\n",
        "run_svr_only = False  # Set to True to skip QNN and run only SVR\n",
        "if run_svr_only:\n",
        "    svr, y_pred_svr = train_svr(X_train, y_train, X_test, y_test)\n",
        "    pd.DataFrame({\n",
        "        'y_test': np.expm1(y_test),  # Reverse log transformation\n",
        "        'y_pred_svr': np.expm1(y_pred_svr)\n",
        "    }).to_csv('predictions_svr.csv', index=False)\n",
        "else:\n",
        "    # Train QNN\n",
        "    model = train_qnn(X_train, y_train, X_val, y_val, batch_size=512)\n",
        "\n",
        "    # Evaluate QNN\n",
        "    mse, rmse, r2, mae, y_pred_qnn = evaluate_model(model, X_test, y_test)\n",
        "    # Reverse log transformation for metrics\n",
        "    y_test_orig = np.expm1(y_test)\n",
        "    y_pred_qnn_orig = np.expm1(y_pred_qnn)\n",
        "    mse_orig = mean_squared_error(y_test_orig, y_pred_qnn_orig)\n",
        "    rmse_orig = np.sqrt(mse_orig)\n",
        "    r2_orig = r2_score(y_test_orig, y_pred_qnn_orig)\n",
        "    mae_orig = mean_absolute_error(y_test_orig, y_pred_qnn_orig)\n",
        "    print(f\"QNN Original Scale - MSE: {mse_orig:.4f}, RMSE: {rmse_orig:.4f}, R2: {r2_orig:.4f}, MAE: {mae_orig:.4f}\")\n",
        "\n",
        "    # Train and evaluate SVR\n",
        "    svr, y_pred_svr = train_svr(X_train, y_train, X_test, y_test)\n",
        "    y_pred_svr_orig = np.expm1(y_pred_svr)\n",
        "    mse_svr_orig = mean_squared_error(y_test_orig, y_pred_svr_orig)\n",
        "    rmse_svr_orig = np.sqrt(mse_svr_orig)\n",
        "    r2_svr_orig = r2_score(y_test_orig, y_pred_svr_orig)\n",
        "    mae_svr_orig = mean_absolute_error(y_test_orig, y_pred_svr_orig)\n",
        "    print(f\"SVR Original Scale - MSE: {mse_svr_orig:.4f}, RMSE: {rmse_svr_orig:.4f}, R2: {r2_svr_orig:.4f}, MAE: {mae_svr_orig:.4f}\")\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    pd.DataFrame({\n",
        "        'y_test': y_test_orig,\n",
        "        'y_pred_qnn': y_pred_qnn_orig,\n",
        "        'y_pred_svr': y_pred_svr_orig\n",
        "    }).to_csv('predictions.csv', index=False)\n",
        "\n",
        "# Generate plots\n",
        "plot_results(df_actual, sim_data=None)\n",
        "\n",
        "print(\"Output plots saved: 3d_time_temp.png, log_concentration.png, shear_bar.png\")\n",
        "print(\"Predictions saved to predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQVrViYGelWe",
        "outputId": "7999f6f9-6360-46db-e216-073c7b3a4e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing data distribution...\n",
            "Data summary:\n",
            "         Experiment       Replica  concentration_ppm      time_hrs  \\\n",
            "count  20750.000000  20750.000000       20750.000000  20750.000000   \n",
            "mean      11.253012      2.686747         215.301205     21.927711   \n",
            "std        6.168164      1.480248         168.779510     13.633354   \n",
            "min        1.000000      1.000000          10.000000      1.000000   \n",
            "25%        6.000000      1.000000         100.000000     10.614458   \n",
            "50%       12.000000      2.000000         200.000000     20.265060   \n",
            "75%       16.000000      4.000000         300.000000     31.722892   \n",
            "max       22.000000      6.000000         500.000000     59.000000   \n",
            "\n",
            "       time_hrs_original  Pressure_bar_CO2  Temperature_C      Shear_Pa  \\\n",
            "count       20750.000000      20750.000000   20750.000000  20750.000000   \n",
            "mean           21.927711          6.154896     105.315129    163.381931   \n",
            "std            13.633354          3.210114      15.622062     66.288953   \n",
            "min             1.000000          0.644502      80.214404     20.490902   \n",
            "25%            10.614458          3.533301      91.530068    113.672195   \n",
            "50%            20.265060          6.285362     102.727945    173.921666   \n",
            "75%            31.722892          8.811739     120.065107    207.710266   \n",
            "max            59.000000         11.916837     129.718778    275.550241   \n",
            "\n",
            "       Brine_Ionic_Strength  corrosion_mm_yr  \n",
            "count          20750.000000     20750.000000  \n",
            "mean               1.290542         3.012032  \n",
            "std                0.871386         5.303769  \n",
            "min                0.510000         0.107238  \n",
            "25%                0.510000         0.387058  \n",
            "50%                0.615000         0.627289  \n",
            "75%                2.310000         1.646830  \n",
            "max                2.310000        20.014591  \n",
            "\n",
            "Correlation with corrosion_mm_yr:\n",
            "time_hrs               -0.620186\n",
            "time_hrs_original      -0.620186\n",
            "Temperature_C          -0.013238\n",
            "Replica                -0.010265\n",
            "concentration_ppm      -0.000398\n",
            "Brine_Ionic_Strength    0.000925\n",
            "Pressure_bar_CO2        0.003326\n",
            "Shear_Pa                0.005131\n",
            "Experiment              0.007333\n",
            "corrosion_mm_yr         1.000000\n",
            "Name: corrosion_mm_yr, dtype: float64\n",
            "\n",
            "Missing values:\n",
            "Description             0\n",
            "Experiment              0\n",
            "Replica                 0\n",
            "concentration_ppm       0\n",
            "time_hrs                0\n",
            "time_hrs_original       0\n",
            "Pressure_bar_CO2        0\n",
            "Temperature_C           0\n",
            "CI                      0\n",
            "Shear_Pa                0\n",
            "Brine_Ionic_Strength    0\n",
            "pH                      0\n",
            "Brine_Type              0\n",
            "Type_of_test            0\n",
            "Lab                     0\n",
            "corrosion_mm_yr         0\n",
            "dtype: int64\n",
            "\n",
            "Outliers in corrosion_mm_yr (>18.92 or <-12.90):\n",
            "Number of outliers: 153\n",
            "Loading data from Excel sheets...\n",
            "Loaded 20597 samples after removing invalid entries.\n",
            "Preprocessing completed in 6.38 seconds.\n",
            "Train: 14417, Validation: 1032, Test: 5148 samples\n",
            "Starting QNN training...\n",
            "Epoch 0, Batch 1/29: Loss = 9.0947, Time = 21.34s\n",
            "Epoch 0, Batch 2/29: Loss = 9.4086, Time = 20.46s\n",
            "Epoch 0, Batch 3/29: Loss = 8.0944, Time = 20.30s\n",
            "Epoch 0, Batch 4/29: Loss = 8.0545, Time = 19.97s\n",
            "Epoch 0, Batch 5/29: Loss = 7.8941, Time = 20.26s\n",
            "Epoch 0, Batch 6/29: Loss = 8.0379, Time = 20.22s\n",
            "Epoch 0, Batch 7/29: Loss = 9.2439, Time = 20.28s\n",
            "Epoch 0, Batch 8/29: Loss = 8.8354, Time = 20.40s\n",
            "Epoch 0, Batch 9/29: Loss = 9.0000, Time = 20.16s\n",
            "Epoch 0, Batch 10/29: Loss = 8.4807, Time = 20.53s\n",
            "Epoch 0, Batch 11/29: Loss = 8.5347, Time = 20.38s\n",
            "Epoch 0, Batch 12/29: Loss = 7.9905, Time = 20.52s\n",
            "Epoch 0, Batch 13/29: Loss = 7.1189, Time = 20.45s\n",
            "Epoch 0, Batch 14/29: Loss = 7.8482, Time = 20.18s\n",
            "Epoch 0, Batch 15/29: Loss = 7.7202, Time = 20.34s\n",
            "Epoch 0, Batch 16/29: Loss = 7.2414, Time = 20.46s\n",
            "Epoch 0, Batch 17/29: Loss = 7.5899, Time = 20.60s\n",
            "Epoch 0, Batch 18/29: Loss = 7.6801, Time = 20.45s\n",
            "Epoch 0, Batch 19/29: Loss = 7.4165, Time = 20.47s\n",
            "Epoch 0, Batch 20/29: Loss = 6.8452, Time = 20.34s\n",
            "Epoch 0, Batch 21/29: Loss = 7.1784, Time = 20.40s\n",
            "Epoch 0, Batch 22/29: Loss = 7.2739, Time = 20.49s\n",
            "Epoch 0, Batch 23/29: Loss = 7.2637, Time = 20.48s\n",
            "Epoch 0, Batch 24/29: Loss = 6.6955, Time = 20.65s\n",
            "Epoch 0, Batch 25/29: Loss = 6.7861, Time = 20.35s\n",
            "Epoch 0, Batch 26/29: Loss = 7.1816, Time = 20.47s\n",
            "Epoch 0, Batch 27/29: Loss = 6.8017, Time = 20.50s\n",
            "Epoch 0, Batch 28/29: Loss = 6.9671, Time = 20.51s\n",
            "Epoch 0, Batch 29/29: Loss = 5.5334, Time = 3.27s\n",
            "Epoch 0: Train Loss = 7.7176, Val Loss = 5.9361, Time = 595.39s\n",
            "Epoch 1, Batch 1/29: Loss = 6.6081, Time = 20.36s\n",
            "Epoch 1, Batch 2/29: Loss = 6.6257, Time = 20.46s\n",
            "Epoch 1, Batch 3/29: Loss = 6.3276, Time = 20.30s\n",
            "Epoch 1, Batch 4/29: Loss = 6.8041, Time = 20.46s\n",
            "Epoch 1, Batch 5/29: Loss = 6.3458, Time = 20.49s\n",
            "Epoch 1, Batch 6/29: Loss = 6.4782, Time = 20.32s\n",
            "Epoch 1, Batch 7/29: Loss = 5.8364, Time = 20.58s\n",
            "Epoch 1, Batch 8/29: Loss = 6.9910, Time = 20.10s\n",
            "Epoch 1, Batch 9/29: Loss = 6.1139, Time = 20.44s\n",
            "Epoch 1, Batch 10/29: Loss = 5.4529, Time = 20.64s\n",
            "Epoch 1, Batch 11/29: Loss = 6.0266, Time = 20.65s\n",
            "Epoch 1, Batch 12/29: Loss = 5.0594, Time = 20.07s\n",
            "Epoch 1, Batch 13/29: Loss = 5.6098, Time = 20.47s\n",
            "Epoch 1, Batch 14/29: Loss = 5.4238, Time = 20.53s\n",
            "Epoch 1, Batch 15/29: Loss = 4.9744, Time = 20.46s\n",
            "Epoch 1, Batch 16/29: Loss = 6.0209, Time = 20.40s\n",
            "Epoch 1, Batch 17/29: Loss = 5.0944, Time = 20.43s\n",
            "Epoch 1, Batch 18/29: Loss = 5.5045, Time = 19.97s\n",
            "Epoch 1, Batch 19/29: Loss = 5.5421, Time = 20.33s\n",
            "Epoch 1, Batch 20/29: Loss = 6.0318, Time = 20.51s\n",
            "Epoch 1, Batch 21/29: Loss = 5.1406, Time = 20.38s\n",
            "Epoch 1, Batch 22/29: Loss = 5.0299, Time = 20.29s\n",
            "Epoch 1, Batch 23/29: Loss = 4.9444, Time = 20.40s\n",
            "Epoch 1, Batch 24/29: Loss = 4.9595, Time = 20.03s\n",
            "Epoch 1, Batch 25/29: Loss = 4.9974, Time = 20.38s\n",
            "Epoch 1, Batch 26/29: Loss = 5.2175, Time = 20.28s\n",
            "Epoch 1, Batch 27/29: Loss = 4.6847, Time = 20.43s\n",
            "Epoch 1, Batch 28/29: Loss = 5.0607, Time = 20.39s\n",
            "Epoch 1, Batch 29/29: Loss = 4.3035, Time = 3.26s\n",
            "Epoch 1: Train Loss = 5.6279, Val Loss = 4.3820, Time = 1184.19s\n",
            "Epoch 2, Batch 1/29: Loss = 4.9000, Time = 19.97s\n",
            "Epoch 2, Batch 2/29: Loss = 4.2432, Time = 20.45s\n",
            "Epoch 2, Batch 3/29: Loss = 5.0702, Time = 20.31s\n",
            "Epoch 2, Batch 4/29: Loss = 4.7163, Time = 20.24s\n",
            "Epoch 2, Batch 5/29: Loss = 4.4068, Time = 20.30s\n",
            "Epoch 2, Batch 6/29: Loss = 4.2520, Time = 20.29s\n",
            "Epoch 2, Batch 7/29: Loss = 4.5484, Time = 20.09s\n",
            "Epoch 2, Batch 8/29: Loss = 4.5877, Time = 20.40s\n",
            "Epoch 2, Batch 9/29: Loss = 4.7062, Time = 20.34s\n",
            "Epoch 2, Batch 10/29: Loss = 4.4708, Time = 20.37s\n",
            "Epoch 2, Batch 11/29: Loss = 4.5700, Time = 20.01s\n",
            "Epoch 2, Batch 12/29: Loss = 4.2635, Time = 20.25s\n",
            "Epoch 2, Batch 13/29: Loss = 4.6060, Time = 20.36s\n",
            "Epoch 2, Batch 14/29: Loss = 4.3956, Time = 20.33s\n",
            "Epoch 2, Batch 15/29: Loss = 3.3796, Time = 20.43s\n",
            "Epoch 2, Batch 16/29: Loss = 3.8791, Time = 20.47s\n",
            "Epoch 2, Batch 17/29: Loss = 3.7785, Time = 20.50s\n",
            "Epoch 2, Batch 18/29: Loss = 4.4886, Time = 20.02s\n",
            "Epoch 2, Batch 19/29: Loss = 4.0178, Time = 20.31s\n",
            "Epoch 2, Batch 20/29: Loss = 4.3206, Time = 20.31s\n",
            "Epoch 2, Batch 21/29: Loss = 3.8832, Time = 20.55s\n",
            "Epoch 2, Batch 22/29: Loss = 3.8271, Time = 20.15s\n",
            "Epoch 2, Batch 23/29: Loss = 4.1130, Time = 20.39s\n",
            "Epoch 2, Batch 24/29: Loss = 3.8586, Time = 20.35s\n",
            "Epoch 2, Batch 25/29: Loss = 4.1424, Time = 20.43s\n",
            "Epoch 2, Batch 26/29: Loss = 3.6252, Time = 20.33s\n",
            "Epoch 2, Batch 27/29: Loss = 3.8089, Time = 20.47s\n",
            "Epoch 2, Batch 28/29: Loss = 3.7535, Time = 20.07s\n",
            "Epoch 2, Batch 29/29: Loss = 4.6805, Time = 3.58s\n",
            "Epoch 2: Train Loss = 4.2515, Val Loss = 3.3257, Time = 1771.45s\n",
            "Epoch 3, Batch 1/29: Loss = 3.6960, Time = 19.88s\n",
            "Epoch 3, Batch 2/29: Loss = 3.3139, Time = 20.38s\n",
            "Epoch 3, Batch 3/29: Loss = 3.9815, Time = 20.27s\n",
            "Epoch 3, Batch 4/29: Loss = 3.6162, Time = 20.23s\n",
            "Epoch 3, Batch 5/29: Loss = 3.6009, Time = 20.43s\n",
            "Epoch 3, Batch 6/29: Loss = 3.5101, Time = 20.41s\n",
            "Epoch 3, Batch 7/29: Loss = 3.8179, Time = 19.97s\n",
            "Epoch 3, Batch 8/29: Loss = 3.4672, Time = 20.34s\n",
            "Epoch 3, Batch 9/29: Loss = 3.4652, Time = 20.34s\n",
            "Epoch 3, Batch 10/29: Loss = 3.1375, Time = 20.39s\n",
            "Epoch 3, Batch 11/29: Loss = 3.3794, Time = 20.06s\n",
            "Epoch 3, Batch 12/29: Loss = 3.2607, Time = 20.42s\n",
            "Epoch 3, Batch 13/29: Loss = 3.3771, Time = 20.39s\n",
            "Epoch 3, Batch 14/29: Loss = 3.3765, Time = 20.36s\n",
            "Epoch 3, Batch 15/29: Loss = 3.2278, Time = 20.44s\n",
            "Epoch 3, Batch 16/29: Loss = 3.4955, Time = 20.45s\n",
            "Epoch 3, Batch 17/29: Loss = 2.9231, Time = 20.03s\n",
            "Epoch 3, Batch 18/29: Loss = 3.0302, Time = 20.48s\n",
            "Epoch 3, Batch 19/29: Loss = 3.0166, Time = 20.36s\n",
            "Epoch 3, Batch 20/29: Loss = 3.0668, Time = 20.48s\n",
            "Epoch 3, Batch 21/29: Loss = 2.8242, Time = 20.63s\n",
            "Epoch 3, Batch 22/29: Loss = 3.0223, Time = 20.18s\n",
            "Epoch 3, Batch 23/29: Loss = 3.2687, Time = 21.03s\n",
            "Epoch 3, Batch 24/29: Loss = 2.9563, Time = 21.06s\n",
            "Epoch 3, Batch 25/29: Loss = 2.8769, Time = 20.87s\n",
            "Epoch 3, Batch 26/29: Loss = 2.8600, Time = 20.46s\n",
            "Epoch 3, Batch 27/29: Loss = 2.9151, Time = 20.41s\n",
            "Epoch 3, Batch 28/29: Loss = 2.7766, Time = 20.65s\n",
            "Epoch 3, Batch 29/29: Loss = 2.3948, Time = 3.30s\n",
            "Epoch 3: Train Loss = 3.2295, Val Loss = 2.5810, Time = 2361.27s\n",
            "Epoch 4, Batch 1/29: Loss = 3.1307, Time = 20.18s\n",
            "Epoch 4, Batch 2/29: Loss = 2.6969, Time = 20.09s\n",
            "Epoch 4, Batch 3/29: Loss = 2.9371, Time = 20.58s\n",
            "Epoch 4, Batch 4/29: Loss = 2.6604, Time = 20.50s\n",
            "Epoch 4, Batch 5/29: Loss = 3.0123, Time = 19.98s\n",
            "Epoch 4, Batch 6/29: Loss = 2.5178, Time = 20.46s\n",
            "Epoch 4, Batch 7/29: Loss = 2.5324, Time = 20.42s\n",
            "Epoch 4, Batch 8/29: Loss = 2.5409, Time = 20.34s\n",
            "Epoch 4, Batch 9/29: Loss = 2.5996, Time = 20.46s\n",
            "Epoch 4, Batch 10/29: Loss = 2.7244, Time = 20.54s\n",
            "Epoch 4, Batch 11/29: Loss = 2.6159, Time = 20.45s\n",
            "Epoch 4, Batch 12/29: Loss = 2.9835, Time = 20.17s\n",
            "Epoch 4, Batch 13/29: Loss = 2.5379, Time = 20.48s\n",
            "Epoch 4, Batch 14/29: Loss = 2.4011, Time = 20.65s\n",
            "Epoch 4, Batch 15/29: Loss = 2.3933, Time = 20.52s\n",
            "Epoch 4, Batch 16/29: Loss = 2.4801, Time = 20.23s\n",
            "Epoch 4, Batch 17/29: Loss = 2.5269, Time = 20.48s\n",
            "Epoch 4, Batch 18/29: Loss = 2.5268, Time = 20.62s\n",
            "Epoch 4, Batch 19/29: Loss = 2.2453, Time = 21.06s\n",
            "Epoch 4, Batch 20/29: Loss = 2.6816, Time = 21.01s\n",
            "Epoch 4, Batch 21/29: Loss = 2.4678, Time = 20.92s\n",
            "Epoch 4, Batch 22/29: Loss = 2.4794, Time = 20.29s\n",
            "Epoch 4, Batch 23/29: Loss = 2.3711, Time = 20.59s\n",
            "Epoch 4, Batch 24/29: Loss = 2.4302, Time = 20.50s\n",
            "Epoch 4, Batch 25/29: Loss = 2.2968, Time = 20.66s\n",
            "Epoch 4, Batch 26/29: Loss = 2.4631, Time = 20.48s\n",
            "Epoch 4, Batch 27/29: Loss = 2.2565, Time = 20.22s\n",
            "Epoch 4, Batch 28/29: Loss = 2.3573, Time = 20.50s\n",
            "Epoch 4, Batch 29/29: Loss = 2.8761, Time = 3.29s\n",
            "Epoch 4: Train Loss = 2.5773, Val Loss = 2.0669, Time = 2952.97s\n",
            "Epoch 5, Batch 1/29: Loss = 2.2775, Time = 20.26s\n",
            "Epoch 5, Batch 2/29: Loss = 2.1739, Time = 20.28s\n",
            "Epoch 5, Batch 3/29: Loss = 2.4161, Time = 20.34s\n",
            "Epoch 5, Batch 4/29: Loss = 2.1333, Time = 20.33s\n",
            "Epoch 5, Batch 5/29: Loss = 2.3062, Time = 20.38s\n",
            "Epoch 5, Batch 6/29: Loss = 2.3345, Time = 20.02s\n",
            "Epoch 5, Batch 7/29: Loss = 2.1591, Time = 20.38s\n",
            "Epoch 5, Batch 8/29: Loss = 2.3182, Time = 20.40s\n",
            "Epoch 5, Batch 9/29: Loss = 2.3201, Time = 20.37s\n",
            "Epoch 5, Batch 10/29: Loss = 1.9794, Time = 20.61s\n",
            "Epoch 5, Batch 11/29: Loss = 2.2619, Time = 20.56s\n",
            "Epoch 5, Batch 12/29: Loss = 1.9985, Time = 20.94s\n",
            "Epoch 5, Batch 13/29: Loss = 2.0848, Time = 20.77s\n",
            "Epoch 5, Batch 14/29: Loss = 2.3151, Time = 20.63s\n",
            "Epoch 5, Batch 15/29: Loss = 2.0323, Time = 20.57s\n",
            "Epoch 5, Batch 16/29: Loss = 1.9289, Time = 20.27s\n",
            "Epoch 5, Batch 17/29: Loss = 1.8387, Time = 20.60s\n",
            "Epoch 5, Batch 18/29: Loss = 2.0327, Time = 20.32s\n",
            "Epoch 5, Batch 19/29: Loss = 1.8720, Time = 20.57s\n",
            "Epoch 5, Batch 20/29: Loss = 2.0192, Time = 20.04s\n",
            "Epoch 5, Batch 21/29: Loss = 1.8785, Time = 20.28s\n",
            "Epoch 5, Batch 22/29: Loss = 2.0951, Time = 20.54s\n",
            "Epoch 5, Batch 23/29: Loss = 1.8906, Time = 20.65s\n",
            "Epoch 5, Batch 24/29: Loss = 2.2024, Time = 20.54s\n",
            "Epoch 5, Batch 25/29: Loss = 1.9357, Time = 20.58s\n",
            "Epoch 5, Batch 26/29: Loss = 1.8906, Time = 20.29s\n",
            "Epoch 5, Batch 27/29: Loss = 1.9351, Time = 20.47s\n",
            "Epoch 5, Batch 28/29: Loss = 1.7854, Time = 20.56s\n",
            "Epoch 5, Batch 29/29: Loss = 1.9914, Time = 3.26s\n",
            "Epoch 5: Train Loss = 2.0830, Val Loss = 1.6947, Time = 3543.98s\n",
            "Epoch 6, Batch 1/29: Loss = 1.8498, Time = 20.06s\n",
            "Epoch 6, Batch 2/29: Loss = 1.6881, Time = 20.23s\n",
            "Epoch 6, Batch 3/29: Loss = 1.7826, Time = 20.39s\n",
            "Epoch 6, Batch 4/29: Loss = 1.8213, Time = 20.08s\n",
            "Epoch 6, Batch 5/29: Loss = 2.0039, Time = 20.40s\n",
            "Epoch 6, Batch 6/29: Loss = 1.7225, Time = 20.38s\n",
            "Epoch 6, Batch 7/29: Loss = 1.8811, Time = 20.48s\n",
            "Epoch 6, Batch 8/29: Loss = 1.6668, Time = 20.41s\n",
            "Epoch 6, Batch 9/29: Loss = 1.7974, Time = 20.65s\n",
            "Epoch 6, Batch 10/29: Loss = 1.7557, Time = 20.57s\n",
            "Epoch 6, Batch 11/29: Loss = 1.7340, Time = 20.19s\n",
            "Epoch 6, Batch 12/29: Loss = 1.8942, Time = 20.52s\n",
            "Epoch 6, Batch 13/29: Loss = 1.9207, Time = 20.43s\n",
            "Epoch 6, Batch 14/29: Loss = 1.6199, Time = 20.46s\n",
            "Epoch 6, Batch 15/29: Loss = 1.7381, Time = 20.04s\n",
            "Epoch 6, Batch 16/29: Loss = 1.9570, Time = 20.41s\n",
            "Epoch 6, Batch 17/29: Loss = 1.6438, Time = 20.51s\n",
            "Epoch 6, Batch 18/29: Loss = 1.5507, Time = 20.33s\n",
            "Epoch 6, Batch 19/29: Loss = 1.6398, Time = 20.45s\n",
            "Epoch 6, Batch 20/29: Loss = 1.6282, Time = 20.39s\n",
            "Epoch 6, Batch 21/29: Loss = 2.0034, Time = 20.03s\n",
            "Epoch 6, Batch 22/29: Loss = 1.7875, Time = 20.31s\n",
            "Epoch 6, Batch 23/29: Loss = 1.6194, Time = 20.40s\n",
            "Epoch 6, Batch 24/29: Loss = 1.7286, Time = 20.41s\n",
            "Epoch 6, Batch 25/29: Loss = 1.4300, Time = 20.47s\n",
            "Epoch 6, Batch 26/29: Loss = 1.5877, Time = 20.09s\n",
            "Epoch 6, Batch 27/29: Loss = 1.6588, Time = 20.37s\n",
            "Epoch 6, Batch 28/29: Loss = 1.6849, Time = 20.46s\n",
            "Epoch 6, Batch 29/29: Loss = 1.3779, Time = 3.30s\n",
            "Epoch 6: Train Loss = 1.7301, Val Loss = 1.4326, Time = 4132.30s\n",
            "Epoch 7, Batch 1/29: Loss = 1.4071, Time = 20.13s\n",
            "Epoch 7, Batch 2/29: Loss = 1.6520, Time = 20.25s\n",
            "Epoch 7, Batch 3/29: Loss = 1.6646, Time = 20.26s\n",
            "Epoch 7, Batch 4/29: Loss = 1.7001, Time = 20.31s\n",
            "Epoch 7, Batch 5/29: Loss = 1.6324, Time = 20.37s\n",
            "Epoch 7, Batch 6/29: Loss = 1.6696, Time = 20.43s\n",
            "Epoch 7, Batch 7/29: Loss = 1.6984, Time = 20.08s\n",
            "Epoch 7, Batch 8/29: Loss = 1.3950, Time = 20.45s\n",
            "Epoch 7, Batch 9/29: Loss = 1.5048, Time = 20.46s\n",
            "Epoch 7, Batch 10/29: Loss = 1.5678, Time = 20.40s\n",
            "Epoch 7, Batch 11/29: Loss = 1.5196, Time = 20.29s\n",
            "Epoch 7, Batch 12/29: Loss = 1.6053, Time = 20.45s\n",
            "Epoch 7, Batch 13/29: Loss = 1.5030, Time = 20.70s\n",
            "Epoch 7, Batch 14/29: Loss = 1.4473, Time = 20.35s\n",
            "Epoch 7, Batch 15/29: Loss = 1.3590, Time = 20.38s\n",
            "Epoch 7, Batch 16/29: Loss = 1.5822, Time = 20.48s\n",
            "Epoch 7, Batch 17/29: Loss = 1.4474, Time = 20.26s\n",
            "Epoch 7, Batch 18/29: Loss = 1.4510, Time = 20.43s\n",
            "Epoch 7, Batch 19/29: Loss = 1.5498, Time = 20.57s\n",
            "Epoch 7, Batch 20/29: Loss = 1.3895, Time = 20.60s\n",
            "Epoch 7, Batch 21/29: Loss = 1.3263, Time = 20.50s\n",
            "Epoch 7, Batch 22/29: Loss = 1.5154, Time = 20.44s\n",
            "Epoch 7, Batch 23/29: Loss = 1.4159, Time = 20.19s\n",
            "Epoch 7, Batch 24/29: Loss = 1.4626, Time = 20.49s\n",
            "Epoch 7, Batch 25/29: Loss = 1.3995, Time = 20.38s\n",
            "Epoch 7, Batch 26/29: Loss = 1.4110, Time = 20.45s\n",
            "Epoch 7, Batch 27/29: Loss = 1.3405, Time = 20.57s\n",
            "Epoch 7, Batch 28/29: Loss = 1.2711, Time = 20.46s\n",
            "Epoch 7, Batch 29/29: Loss = 1.5347, Time = 3.28s\n",
            "Epoch 7: Train Loss = 1.4973, Val Loss = 1.2414, Time = 4721.93s\n",
            "Epoch 8, Batch 1/29: Loss = 1.4762, Time = 19.90s\n",
            "Epoch 8, Batch 2/29: Loss = 1.3604, Time = 20.29s\n",
            "Epoch 8, Batch 3/29: Loss = 1.4047, Time = 20.34s\n",
            "Epoch 8, Batch 4/29: Loss = 1.5646, Time = 20.32s\n",
            "Epoch 8, Batch 5/29: Loss = 1.3479, Time = 20.42s\n",
            "Epoch 8, Batch 6/29: Loss = 1.4111, Time = 20.23s\n",
            "Epoch 8, Batch 7/29: Loss = 1.2520, Time = 20.30s\n",
            "Epoch 8, Batch 8/29: Loss = 1.3281, Time = 20.26s\n",
            "Epoch 8, Batch 9/29: Loss = 1.3734, Time = 20.42s\n",
            "Epoch 8, Batch 10/29: Loss = 1.2235, Time = 20.45s\n",
            "Epoch 8, Batch 11/29: Loss = 1.2778, Time = 20.56s\n",
            "Epoch 8, Batch 12/29: Loss = 1.2201, Time = 20.15s\n",
            "Epoch 8, Batch 13/29: Loss = 1.2092, Time = 20.37s\n",
            "Epoch 8, Batch 14/29: Loss = 1.2485, Time = 20.40s\n",
            "Epoch 8, Batch 15/29: Loss = 1.2902, Time = 20.51s\n",
            "Epoch 8, Batch 16/29: Loss = 1.3236, Time = 20.33s\n",
            "Epoch 8, Batch 17/29: Loss = 1.3487, Time = 20.07s\n",
            "Epoch 8, Batch 18/29: Loss = 1.3046, Time = 20.54s\n",
            "Epoch 8, Batch 19/29: Loss = 1.2113, Time = 20.47s\n",
            "Epoch 8, Batch 20/29: Loss = 1.3081, Time = 20.40s\n",
            "Epoch 8, Batch 21/29: Loss = 1.3535, Time = 20.43s\n",
            "Epoch 8, Batch 22/29: Loss = 1.2416, Time = 20.03s\n",
            "Epoch 8, Batch 23/29: Loss = 1.4125, Time = 20.37s\n",
            "Epoch 8, Batch 24/29: Loss = 1.3007, Time = 20.41s\n",
            "Epoch 8, Batch 25/29: Loss = 1.3372, Time = 20.36s\n",
            "Epoch 8, Batch 26/29: Loss = 1.1836, Time = 20.53s\n",
            "Epoch 8, Batch 27/29: Loss = 1.2131, Time = 20.06s\n",
            "Epoch 8, Batch 28/29: Loss = 1.3817, Time = 20.41s\n",
            "Epoch 8, Batch 29/29: Loss = 0.9341, Time = 3.28s\n",
            "Epoch 8: Train Loss = 1.3049, Val Loss = 1.1020, Time = 5309.82s\n",
            "Epoch 9, Batch 1/29: Loss = 1.2980, Time = 20.08s\n",
            "Epoch 9, Batch 2/29: Loss = 1.3386, Time = 20.33s\n",
            "Epoch 9, Batch 3/29: Loss = 1.2012, Time = 20.51s\n",
            "Epoch 9, Batch 4/29: Loss = 1.2025, Time = 20.34s\n",
            "Epoch 9, Batch 5/29: Loss = 1.2258, Time = 20.09s\n",
            "Epoch 9, Batch 6/29: Loss = 1.2196, Time = 20.46s\n",
            "Epoch 9, Batch 7/29: Loss = 1.2130, Time = 20.42s\n",
            "Epoch 9, Batch 8/29: Loss = 1.2591, Time = 20.37s\n",
            "Epoch 9, Batch 9/29: Loss = 1.2144, Time = 20.45s\n",
            "Epoch 9, Batch 10/29: Loss = 1.0534, Time = 20.35s\n",
            "Epoch 9, Batch 11/29: Loss = 1.2377, Time = 19.96s\n",
            "Epoch 9, Batch 12/29: Loss = 1.2618, Time = 20.38s\n",
            "Epoch 9, Batch 13/29: Loss = 1.1965, Time = 20.26s\n",
            "Epoch 9, Batch 14/29: Loss = 1.1484, Time = 20.36s\n",
            "Epoch 9, Batch 15/29: Loss = 1.2321, Time = 20.35s\n",
            "Epoch 9, Batch 16/29: Loss = 1.2038, Time = 20.32s\n",
            "Epoch 9, Batch 17/29: Loss = 1.1579, Time = 20.08s\n",
            "Epoch 9, Batch 18/29: Loss = 1.0781, Time = 20.42s\n",
            "Epoch 9, Batch 19/29: Loss = 1.1765, Time = 20.29s\n",
            "Epoch 9, Batch 20/29: Loss = 1.0054, Time = 20.45s\n",
            "Epoch 9, Batch 21/29: Loss = 1.1121, Time = 20.36s\n",
            "Epoch 9, Batch 22/29: Loss = 1.0745, Time = 20.59s\n",
            "Epoch 9, Batch 23/29: Loss = 1.2875, Time = 20.28s\n",
            "Epoch 9, Batch 24/29: Loss = 1.2269, Time = 20.45s\n",
            "Epoch 9, Batch 25/29: Loss = 1.0948, Time = 20.51s\n",
            "Epoch 9, Batch 26/29: Loss = 1.1161, Time = 20.51s\n",
            "Epoch 9, Batch 27/29: Loss = 1.1422, Time = 20.08s\n",
            "Epoch 9, Batch 28/29: Loss = 1.2331, Time = 20.35s\n",
            "Epoch 9, Batch 29/29: Loss = 0.9342, Time = 3.28s\n",
            "Epoch 9: Train Loss = 1.1774, Val Loss = 0.9978, Time = 5897.57s\n",
            "Epoch 10, Batch 1/29: Loss = 1.1859, Time = 20.35s\n",
            "Epoch 10, Batch 2/29: Loss = 1.0913, Time = 19.86s\n",
            "Epoch 10, Batch 3/29: Loss = 1.1253, Time = 20.29s\n",
            "Epoch 10, Batch 4/29: Loss = 1.1041, Time = 20.30s\n",
            "Epoch 10, Batch 5/29: Loss = 1.1173, Time = 20.33s\n",
            "Epoch 10, Batch 6/29: Loss = 1.1702, Time = 20.39s\n",
            "Epoch 10, Batch 7/29: Loss = 1.2330, Time = 20.43s\n",
            "Epoch 10, Batch 8/29: Loss = 1.0464, Time = 20.11s\n",
            "Epoch 10, Batch 9/29: Loss = 1.0894, Time = 20.42s\n",
            "Epoch 10, Batch 10/29: Loss = 1.1461, Time = 20.47s\n",
            "Epoch 10, Batch 11/29: Loss = 1.0600, Time = 20.38s\n",
            "Epoch 10, Batch 12/29: Loss = 1.2507, Time = 20.33s\n",
            "Epoch 10, Batch 13/29: Loss = 1.0873, Time = 20.16s\n",
            "Epoch 10, Batch 14/29: Loss = 1.0816, Time = 20.54s\n",
            "Epoch 10, Batch 15/29: Loss = 1.0589, Time = 20.57s\n",
            "Epoch 10, Batch 16/29: Loss = 0.9527, Time = 20.55s\n",
            "Epoch 10, Batch 17/29: Loss = 1.0983, Time = 20.40s\n",
            "Epoch 10, Batch 18/29: Loss = 1.1365, Time = 20.03s\n",
            "Epoch 10, Batch 19/29: Loss = 1.1042, Time = 20.49s\n",
            "Epoch 10, Batch 20/29: Loss = 1.0193, Time = 20.37s\n",
            "Epoch 10, Batch 21/29: Loss = 0.9328, Time = 20.44s\n",
            "Epoch 10, Batch 22/29: Loss = 1.1327, Time = 19.99s\n",
            "Epoch 10, Batch 23/29: Loss = 1.0700, Time = 20.50s\n",
            "Epoch 10, Batch 24/29: Loss = 1.0652, Time = 20.47s\n",
            "Epoch 10, Batch 25/29: Loss = 1.0672, Time = 20.42s\n",
            "Epoch 10, Batch 26/29: Loss = 0.9735, Time = 20.35s\n",
            "Epoch 10, Batch 27/29: Loss = 0.9977, Time = 20.58s\n",
            "Epoch 10, Batch 28/29: Loss = 1.0463, Time = 20.14s\n",
            "Epoch 10, Batch 29/29: Loss = 0.9947, Time = 3.59s\n",
            "Epoch 10: Train Loss = 1.0841, Val Loss = 0.9193, Time = 6485.96s\n",
            "Epoch 11, Batch 1/29: Loss = 1.1118, Time = 19.89s\n",
            "Epoch 11, Batch 2/29: Loss = 1.0536, Time = 20.36s\n",
            "Epoch 11, Batch 3/29: Loss = 0.9744, Time = 20.51s\n",
            "Epoch 11, Batch 4/29: Loss = 0.9950, Time = 20.48s\n",
            "Epoch 11, Batch 5/29: Loss = 1.0658, Time = 20.48s\n",
            "Epoch 11, Batch 6/29: Loss = 1.0740, Time = 20.48s\n",
            "Epoch 11, Batch 7/29: Loss = 1.0345, Time = 20.18s\n",
            "Epoch 11, Batch 8/29: Loss = 1.0640, Time = 20.32s\n",
            "Epoch 11, Batch 9/29: Loss = 1.0620, Time = 20.50s\n",
            "Epoch 11, Batch 10/29: Loss = 1.1559, Time = 20.66s\n",
            "Epoch 11, Batch 11/29: Loss = 1.0383, Time = 20.70s\n",
            "Epoch 11, Batch 12/29: Loss = 0.9922, Time = 20.15s\n",
            "Epoch 11, Batch 13/29: Loss = 0.9041, Time = 20.57s\n",
            "Epoch 11, Batch 14/29: Loss = 1.0684, Time = 20.41s\n",
            "Epoch 11, Batch 15/29: Loss = 1.0681, Time = 20.44s\n",
            "Epoch 11, Batch 16/29: Loss = 0.8776, Time = 20.30s\n",
            "Epoch 11, Batch 17/29: Loss = 1.0308, Time = 20.01s\n",
            "Epoch 11, Batch 18/29: Loss = 1.0478, Time = 20.44s\n",
            "Epoch 11, Batch 19/29: Loss = 1.0086, Time = 20.50s\n",
            "Epoch 11, Batch 20/29: Loss = 0.9945, Time = 20.50s\n",
            "Epoch 11, Batch 21/29: Loss = 0.9464, Time = 20.21s\n",
            "Epoch 11, Batch 22/29: Loss = 1.0249, Time = 20.40s\n",
            "Epoch 11, Batch 23/29: Loss = 0.9583, Time = 20.35s\n",
            "Epoch 11, Batch 24/29: Loss = 0.9417, Time = 20.45s\n",
            "Epoch 11, Batch 25/29: Loss = 0.9762, Time = 20.45s\n",
            "Epoch 11, Batch 26/29: Loss = 0.9252, Time = 20.37s\n",
            "Epoch 11, Batch 27/29: Loss = 0.9720, Time = 20.07s\n",
            "Epoch 11, Batch 28/29: Loss = 0.9715, Time = 20.51s\n",
            "Epoch 11, Batch 29/29: Loss = 1.0059, Time = 3.30s\n",
            "Epoch 11: Train Loss = 1.0118, Val Loss = 0.8601, Time = 7075.19s\n",
            "Epoch 12, Batch 1/29: Loss = 1.0537, Time = 20.15s\n",
            "Epoch 12, Batch 2/29: Loss = 0.8685, Time = 20.35s\n",
            "Epoch 12, Batch 3/29: Loss = 0.9876, Time = 20.34s\n",
            "Epoch 12, Batch 4/29: Loss = 1.0713, Time = 20.29s\n",
            "Epoch 12, Batch 5/29: Loss = 1.0434, Time = 20.42s\n",
            "Epoch 12, Batch 6/29: Loss = 1.0608, Time = 20.33s\n",
            "Epoch 12, Batch 7/29: Loss = 0.9079, Time = 20.15s\n",
            "Epoch 12, Batch 8/29: Loss = 0.9417, Time = 20.33s\n",
            "Epoch 12, Batch 9/29: Loss = 1.1204, Time = 20.41s\n",
            "Epoch 12, Batch 10/29: Loss = 0.9545, Time = 20.29s\n",
            "Epoch 12, Batch 11/29: Loss = 0.9342, Time = 20.41s\n",
            "Epoch 12, Batch 12/29: Loss = 1.0018, Time = 20.20s\n",
            "Epoch 12, Batch 13/29: Loss = 0.8811, Time = 20.55s\n",
            "Epoch 12, Batch 14/29: Loss = 0.9723, Time = 20.53s\n",
            "Epoch 12, Batch 15/29: Loss = 0.9778, Time = 20.47s\n",
            "Epoch 12, Batch 16/29: Loss = 0.8975, Time = 20.15s\n",
            "Epoch 12, Batch 17/29: Loss = 0.9043, Time = 20.42s\n",
            "Epoch 12, Batch 18/29: Loss = 0.9255, Time = 20.32s\n",
            "Epoch 12, Batch 19/29: Loss = 1.0075, Time = 20.28s\n",
            "Epoch 12, Batch 20/29: Loss = 0.8337, Time = 20.51s\n",
            "Epoch 12, Batch 21/29: Loss = 0.8822, Time = 20.44s\n",
            "Epoch 12, Batch 22/29: Loss = 0.9596, Time = 20.07s\n",
            "Epoch 12, Batch 23/29: Loss = 0.8968, Time = 20.44s\n",
            "Epoch 12, Batch 24/29: Loss = 0.9440, Time = 20.43s\n",
            "Epoch 12, Batch 25/29: Loss = 0.9466, Time = 20.40s\n",
            "Epoch 12, Batch 26/29: Loss = 1.0668, Time = 20.40s\n",
            "Epoch 12, Batch 27/29: Loss = 0.8276, Time = 20.43s\n",
            "Epoch 12, Batch 28/29: Loss = 0.8870, Time = 20.11s\n",
            "Epoch 12, Batch 29/29: Loss = 0.8471, Time = 3.67s\n",
            "Epoch 12: Train Loss = 0.9518, Val Loss = 0.8141, Time = 7663.83s\n",
            "Epoch 13, Batch 1/29: Loss = 0.9944, Time = 19.94s\n",
            "Epoch 13, Batch 2/29: Loss = 0.9326, Time = 20.41s\n",
            "Epoch 13, Batch 3/29: Loss = 0.9062, Time = 20.25s\n",
            "Epoch 13, Batch 4/29: Loss = 0.8893, Time = 20.30s\n",
            "Epoch 13, Batch 5/29: Loss = 0.8866, Time = 20.33s\n",
            "Epoch 13, Batch 6/29: Loss = 0.8290, Time = 20.37s\n",
            "Epoch 13, Batch 7/29: Loss = 0.9836, Time = 20.06s\n",
            "Epoch 13, Batch 8/29: Loss = 0.8740, Time = 20.53s\n",
            "Epoch 13, Batch 9/29: Loss = 0.8691, Time = 20.55s\n",
            "Epoch 13, Batch 10/29: Loss = 0.9039, Time = 20.50s\n",
            "Epoch 13, Batch 11/29: Loss = 0.8898, Time = 20.50s\n",
            "Epoch 13, Batch 12/29: Loss = 1.0216, Time = 20.43s\n",
            "Epoch 13, Batch 13/29: Loss = 1.0141, Time = 20.03s\n",
            "Epoch 13, Batch 14/29: Loss = 0.9975, Time = 20.41s\n",
            "Epoch 13, Batch 15/29: Loss = 0.8924, Time = 20.52s\n",
            "Epoch 13, Batch 16/29: Loss = 0.7967, Time = 20.46s\n",
            "Epoch 13, Batch 17/29: Loss = 0.8997, Time = 20.21s\n",
            "Epoch 13, Batch 18/29: Loss = 0.9074, Time = 20.33s\n",
            "Epoch 13, Batch 19/29: Loss = 0.9157, Time = 20.53s\n",
            "Epoch 13, Batch 20/29: Loss = 1.0256, Time = 20.57s\n",
            "Epoch 13, Batch 21/29: Loss = 0.8977, Time = 20.59s\n",
            "Epoch 13, Batch 22/29: Loss = 0.8650, Time = 20.41s\n",
            "Epoch 13, Batch 23/29: Loss = 0.9718, Time = 20.16s\n",
            "Epoch 13, Batch 24/29: Loss = 0.8625, Time = 20.53s\n",
            "Epoch 13, Batch 25/29: Loss = 0.9147, Time = 20.53s\n",
            "Epoch 13, Batch 26/29: Loss = 0.8209, Time = 20.55s\n",
            "Epoch 13, Batch 27/29: Loss = 0.9403, Time = 20.66s\n",
            "Epoch 13, Batch 28/29: Loss = 0.8405, Time = 20.23s\n",
            "Epoch 13, Batch 29/29: Loss = 0.6445, Time = 3.58s\n",
            "Epoch 13: Train Loss = 0.9030, Val Loss = 0.7792, Time = 8253.64s\n",
            "Epoch 14, Batch 1/29: Loss = 0.8430, Time = 20.32s\n",
            "Epoch 14, Batch 2/29: Loss = 0.9046, Time = 20.17s\n",
            "Epoch 14, Batch 3/29: Loss = 0.9368, Time = 20.30s\n",
            "Epoch 14, Batch 4/29: Loss = 0.8704, Time = 20.35s\n",
            "Epoch 14, Batch 5/29: Loss = 0.8385, Time = 20.38s\n",
            "Epoch 14, Batch 6/29: Loss = 0.8662, Time = 20.44s\n",
            "Epoch 14, Batch 7/29: Loss = 0.8693, Time = 20.49s\n",
            "Epoch 14, Batch 8/29: Loss = 0.7948, Time = 20.07s\n",
            "Epoch 14, Batch 9/29: Loss = 0.9164, Time = 20.38s\n",
            "Epoch 14, Batch 10/29: Loss = 0.8947, Time = 20.49s\n",
            "Epoch 14, Batch 11/29: Loss = 0.8536, Time = 20.47s\n",
            "Epoch 14, Batch 12/29: Loss = 0.9024, Time = 20.13s\n",
            "Epoch 14, Batch 13/29: Loss = 0.8907, Time = 20.42s\n",
            "Epoch 14, Batch 14/29: Loss = 0.8056, Time = 20.39s\n",
            "Epoch 14, Batch 15/29: Loss = 0.8720, Time = 20.53s\n",
            "Epoch 14, Batch 16/29: Loss = 0.8985, Time = 20.56s\n",
            "Epoch 14, Batch 17/29: Loss = 0.8675, Time = 20.44s\n",
            "Epoch 14, Batch 18/29: Loss = 0.8922, Time = 20.06s\n",
            "Epoch 14, Batch 19/29: Loss = 0.9113, Time = 20.47s\n",
            "Epoch 14, Batch 20/29: Loss = 0.8659, Time = 20.46s\n",
            "Epoch 14, Batch 21/29: Loss = 0.8678, Time = 20.61s\n",
            "Epoch 14, Batch 22/29: Loss = 0.9155, Time = 20.43s\n",
            "Epoch 14, Batch 23/29: Loss = 0.9396, Time = 20.21s\n",
            "Epoch 14, Batch 24/29: Loss = 0.8441, Time = 20.53s\n",
            "Epoch 14, Batch 25/29: Loss = 0.8371, Time = 20.60s\n",
            "Epoch 14, Batch 26/29: Loss = 0.9080, Time = 20.54s\n",
            "Epoch 14, Batch 27/29: Loss = 0.8651, Time = 20.64s\n",
            "Epoch 14, Batch 28/29: Loss = 0.8698, Time = 20.14s\n",
            "Epoch 14, Batch 29/29: Loss = 0.7042, Time = 3.28s\n",
            "Epoch 14: Train Loss = 0.8705, Val Loss = 0.7512, Time = 8843.38s\n",
            "Epoch 15, Batch 1/29: Loss = 0.8951, Time = 20.21s\n",
            "Epoch 15, Batch 2/29: Loss = 0.9975, Time = 20.17s\n",
            "Epoch 15, Batch 3/29: Loss = 0.8222, Time = 20.38s\n",
            "Epoch 15, Batch 4/29: Loss = 1.0048, Time = 20.31s\n",
            "Epoch 15, Batch 5/29: Loss = 0.7913, Time = 20.37s\n",
            "Epoch 15, Batch 6/29: Loss = 0.8042, Time = 20.44s\n",
            "Epoch 15, Batch 7/29: Loss = 0.7935, Time = 20.14s\n",
            "Epoch 15, Batch 8/29: Loss = 0.8042, Time = 20.46s\n",
            "Epoch 15, Batch 9/29: Loss = 0.7717, Time = 20.55s\n",
            "Epoch 15, Batch 10/29: Loss = 0.7057, Time = 20.39s\n",
            "Epoch 15, Batch 11/29: Loss = 0.8749, Time = 20.45s\n",
            "Epoch 15, Batch 12/29: Loss = 0.8230, Time = 20.14s\n",
            "Epoch 15, Batch 13/29: Loss = 0.9352, Time = 20.44s\n",
            "Epoch 15, Batch 14/29: Loss = 0.7930, Time = 20.45s\n",
            "Epoch 15, Batch 15/29: Loss = 0.8767, Time = 20.53s\n",
            "Epoch 15, Batch 16/29: Loss = 0.7871, Time = 20.59s\n",
            "Epoch 15, Batch 17/29: Loss = 0.8973, Time = 20.51s\n",
            "Epoch 15, Batch 18/29: Loss = 1.0008, Time = 20.09s\n",
            "Epoch 15, Batch 19/29: Loss = 0.8352, Time = 20.45s\n",
            "Epoch 15, Batch 20/29: Loss = 0.7368, Time = 20.43s\n",
            "Epoch 15, Batch 21/29: Loss = 0.8041, Time = 20.56s\n",
            "Epoch 15, Batch 22/29: Loss = 0.7811, Time = 20.48s\n",
            "Epoch 15, Batch 23/29: Loss = 0.9696, Time = 20.59s\n",
            "Epoch 15, Batch 24/29: Loss = 0.8047, Time = 20.21s\n",
            "Epoch 15, Batch 25/29: Loss = 0.9209, Time = 20.65s\n",
            "Epoch 15, Batch 26/29: Loss = 0.8393, Time = 20.51s\n",
            "Epoch 15, Batch 27/29: Loss = 0.8092, Time = 20.66s\n",
            "Epoch 15, Batch 28/29: Loss = 0.8634, Time = 20.45s\n",
            "Epoch 15, Batch 29/29: Loss = 0.8250, Time = 3.30s\n",
            "Epoch 15: Train Loss = 0.8471, Val Loss = 0.7282, Time = 9433.53s\n",
            "Epoch 16, Batch 1/29: Loss = 0.7958, Time = 19.91s\n",
            "Epoch 16, Batch 2/29: Loss = 0.9733, Time = 20.31s\n",
            "Epoch 16, Batch 3/29: Loss = 0.8100, Time = 20.32s\n",
            "Epoch 16, Batch 4/29: Loss = 0.7747, Time = 20.35s\n",
            "Epoch 16, Batch 5/29: Loss = 0.7170, Time = 20.39s\n",
            "Epoch 16, Batch 6/29: Loss = 0.9035, Time = 20.33s\n",
            "Epoch 16, Batch 7/29: Loss = 0.8391, Time = 20.12s\n",
            "Epoch 16, Batch 8/29: Loss = 0.7513, Time = 20.38s\n",
            "Epoch 16, Batch 9/29: Loss = 0.8098, Time = 20.47s\n",
            "Epoch 16, Batch 10/29: Loss = 0.8614, Time = 20.47s\n",
            "Epoch 16, Batch 11/29: Loss = 0.7866, Time = 20.20s\n",
            "Epoch 16, Batch 12/29: Loss = 0.8680, Time = 20.56s\n",
            "Epoch 16, Batch 13/29: Loss = 0.7883, Time = 20.54s\n",
            "Epoch 16, Batch 14/29: Loss = 0.9292, Time = 20.60s\n",
            "Epoch 16, Batch 15/29: Loss = 0.7451, Time = 20.46s\n",
            "Epoch 16, Batch 16/29: Loss = 0.8353, Time = 20.54s\n",
            "Epoch 16, Batch 17/29: Loss = 0.8124, Time = 20.16s\n",
            "Epoch 16, Batch 18/29: Loss = 0.8620, Time = 20.54s\n",
            "Epoch 16, Batch 19/29: Loss = 0.8547, Time = 20.68s\n",
            "Epoch 16, Batch 20/29: Loss = 0.7994, Time = 20.41s\n",
            "Epoch 16, Batch 21/29: Loss = 0.7273, Time = 20.53s\n",
            "Epoch 16, Batch 22/29: Loss = 0.9188, Time = 20.77s\n",
            "Epoch 16, Batch 23/29: Loss = 0.9066, Time = 20.03s\n",
            "Epoch 16, Batch 24/29: Loss = 0.7661, Time = 20.48s\n",
            "Epoch 16, Batch 25/29: Loss = 0.7984, Time = 20.46s\n",
            "Epoch 16, Batch 26/29: Loss = 0.8135, Time = 20.49s\n",
            "Epoch 16, Batch 27/29: Loss = 0.8051, Time = 20.43s\n",
            "Epoch 16, Batch 28/29: Loss = 0.8344, Time = 20.73s\n",
            "Epoch 16, Batch 29/29: Loss = 0.8096, Time = 3.28s\n",
            "Epoch 16: Train Loss = 0.8240, Val Loss = 0.7099, Time = 10023.53s\n",
            "Epoch 17, Batch 1/29: Loss = 0.8126, Time = 20.37s\n",
            "Epoch 17, Batch 2/29: Loss = 0.8978, Time = 20.45s\n",
            "Epoch 17, Batch 3/29: Loss = 0.8974, Time = 20.34s\n",
            "Epoch 17, Batch 4/29: Loss = 0.9081, Time = 20.38s\n",
            "Epoch 17, Batch 5/29: Loss = 0.7673, Time = 20.54s\n",
            "Epoch 17, Batch 6/29: Loss = 0.8210, Time = 20.48s\n",
            "Epoch 17, Batch 7/29: Loss = 0.7871, Time = 20.30s\n",
            "Epoch 17, Batch 8/29: Loss = 0.8863, Time = 20.44s\n",
            "Epoch 17, Batch 9/29: Loss = 0.7684, Time = 20.41s\n",
            "Epoch 17, Batch 10/29: Loss = 0.6741, Time = 20.43s\n",
            "Epoch 17, Batch 11/29: Loss = 0.8699, Time = 20.38s\n",
            "Epoch 17, Batch 12/29: Loss = 0.8223, Time = 20.35s\n",
            "Epoch 17, Batch 13/29: Loss = 0.8755, Time = 20.12s\n",
            "Epoch 17, Batch 14/29: Loss = 0.7836, Time = 20.37s\n",
            "Epoch 17, Batch 15/29: Loss = 0.7443, Time = 20.50s\n",
            "Epoch 17, Batch 16/29: Loss = 0.7701, Time = 20.49s\n",
            "Epoch 17, Batch 17/29: Loss = 0.7623, Time = 20.53s\n",
            "Epoch 17, Batch 18/29: Loss = 0.7545, Time = 20.53s\n",
            "Epoch 17, Batch 19/29: Loss = 0.8472, Time = 20.25s\n",
            "Epoch 17, Batch 20/29: Loss = 0.7447, Time = 20.41s\n",
            "Epoch 17, Batch 21/29: Loss = 0.8169, Time = 20.55s\n",
            "Epoch 17, Batch 22/29: Loss = 0.8242, Time = 20.68s\n",
            "Epoch 17, Batch 23/29: Loss = 0.7747, Time = 20.71s\n",
            "Epoch 17, Batch 24/29: Loss = 0.7442, Time = 20.28s\n",
            "Epoch 17, Batch 25/29: Loss = 0.7848, Time = 20.46s\n",
            "Epoch 17, Batch 26/29: Loss = 0.7851, Time = 20.64s\n",
            "Epoch 17, Batch 27/29: Loss = 0.8017, Time = 20.70s\n",
            "Epoch 17, Batch 28/29: Loss = 0.8417, Time = 20.66s\n",
            "Epoch 17, Batch 29/29: Loss = 0.7738, Time = 3.33s\n",
            "Epoch 17: Train Loss = 0.8049, Val Loss = 0.6949, Time = 10615.03s\n",
            "Epoch 18, Batch 1/29: Loss = 0.7426, Time = 20.32s\n",
            "Epoch 18, Batch 2/29: Loss = 0.9003, Time = 20.05s\n",
            "Epoch 18, Batch 3/29: Loss = 0.8364, Time = 20.39s\n",
            "Epoch 18, Batch 4/29: Loss = 0.7665, Time = 20.36s\n",
            "Epoch 18, Batch 5/29: Loss = 0.6569, Time = 20.56s\n",
            "Epoch 18, Batch 6/29: Loss = 0.7945, Time = 20.47s\n",
            "Epoch 18, Batch 7/29: Loss = 0.7729, Time = 20.42s\n",
            "Epoch 18, Batch 8/29: Loss = 0.7849, Time = 20.14s\n",
            "Epoch 18, Batch 9/29: Loss = 0.7739, Time = 20.48s\n",
            "Epoch 18, Batch 10/29: Loss = 0.8745, Time = 20.39s\n",
            "Epoch 18, Batch 11/29: Loss = 0.8387, Time = 20.41s\n",
            "Epoch 18, Batch 12/29: Loss = 0.7715, Time = 20.72s\n",
            "Epoch 18, Batch 13/29: Loss = 0.8224, Time = 20.07s\n",
            "Epoch 18, Batch 14/29: Loss = 0.7293, Time = 20.50s\n",
            "Epoch 18, Batch 15/29: Loss = 0.8105, Time = 20.59s\n",
            "Epoch 18, Batch 16/29: Loss = 0.7866, Time = 20.39s\n",
            "Epoch 18, Batch 17/29: Loss = 0.8304, Time = 20.63s\n",
            "Epoch 18, Batch 18/29: Loss = 0.7132, Time = 20.18s\n",
            "Epoch 18, Batch 19/29: Loss = 0.8195, Time = 20.32s\n",
            "Epoch 18, Batch 20/29: Loss = 0.7571, Time = 20.36s\n",
            "Epoch 18, Batch 21/29: Loss = 0.7678, Time = 20.50s\n",
            "Epoch 18, Batch 22/29: Loss = 0.8251, Time = 20.47s\n",
            "Epoch 18, Batch 23/29: Loss = 0.8541, Time = 20.55s\n",
            "Epoch 18, Batch 24/29: Loss = 0.8700, Time = 20.06s\n",
            "Epoch 18, Batch 25/29: Loss = 0.7760, Time = 20.47s\n",
            "Epoch 18, Batch 26/29: Loss = 0.7449, Time = 20.61s\n",
            "Epoch 18, Batch 27/29: Loss = 0.7936, Time = 20.55s\n",
            "Epoch 18, Batch 28/29: Loss = 0.6811, Time = 20.03s\n",
            "Epoch 18, Batch 29/29: Loss = 0.9209, Time = 3.58s\n",
            "Epoch 18: Train Loss = 0.7937, Val Loss = 0.6832, Time = 11204.84s\n",
            "Epoch 19, Batch 1/29: Loss = 0.7835, Time = 19.90s\n",
            "Epoch 19, Batch 2/29: Loss = 0.7783, Time = 20.30s\n",
            "Epoch 19, Batch 3/29: Loss = 0.8784, Time = 20.40s\n",
            "Epoch 19, Batch 4/29: Loss = 0.7620, Time = 20.31s\n",
            "Epoch 19, Batch 5/29: Loss = 0.7730, Time = 20.29s\n",
            "Epoch 19, Batch 6/29: Loss = 0.7331, Time = 20.48s\n",
            "Epoch 19, Batch 7/29: Loss = 0.7607, Time = 20.11s\n",
            "Epoch 19, Batch 8/29: Loss = 0.7354, Time = 20.27s\n",
            "Epoch 19, Batch 9/29: Loss = 0.7338, Time = 20.26s\n",
            "Epoch 19, Batch 10/29: Loss = 0.7582, Time = 20.37s\n",
            "Epoch 19, Batch 11/29: Loss = 0.7884, Time = 20.40s\n",
            "Epoch 19, Batch 12/29: Loss = 0.7783, Time = 20.46s\n",
            "Epoch 19, Batch 13/29: Loss = 0.8626, Time = 20.20s\n",
            "Epoch 19, Batch 14/29: Loss = 0.7574, Time = 20.27s\n",
            "Epoch 19, Batch 15/29: Loss = 0.6441, Time = 20.62s\n",
            "Epoch 19, Batch 16/29: Loss = 0.8327, Time = 20.49s\n",
            "Epoch 19, Batch 17/29: Loss = 0.7300, Time = 20.52s\n",
            "Epoch 19, Batch 18/29: Loss = 0.7739, Time = 20.63s\n",
            "Epoch 19, Batch 19/29: Loss = 0.8225, Time = 20.22s\n",
            "Epoch 19, Batch 20/29: Loss = 0.8006, Time = 20.39s\n",
            "Epoch 19, Batch 21/29: Loss = 0.7724, Time = 20.35s\n",
            "Epoch 19, Batch 22/29: Loss = 0.8723, Time = 20.55s\n",
            "Epoch 19, Batch 23/29: Loss = 0.7587, Time = 20.56s\n",
            "Epoch 19, Batch 24/29: Loss = 0.6784, Time = 20.53s\n",
            "Epoch 19, Batch 25/29: Loss = 0.8414, Time = 20.26s\n",
            "Epoch 19, Batch 26/29: Loss = 0.8834, Time = 20.58s\n",
            "Epoch 19, Batch 27/29: Loss = 0.7352, Time = 20.71s\n",
            "Epoch 19, Batch 28/29: Loss = 0.7310, Time = 20.58s\n",
            "Epoch 19, Batch 29/29: Loss = 0.6666, Time = 3.35s\n",
            "Epoch 19: Train Loss = 0.7733, Val Loss = 0.6717, Time = 11794.57s\n",
            "Epoch 20, Batch 1/29: Loss = 0.7684, Time = 20.24s\n",
            "Epoch 20, Batch 2/29: Loss = 0.7291, Time = 20.35s\n",
            "Epoch 20, Batch 3/29: Loss = 0.8499, Time = 20.10s\n",
            "Epoch 20, Batch 4/29: Loss = 0.8205, Time = 20.41s\n",
            "Epoch 20, Batch 5/29: Loss = 0.7400, Time = 20.45s\n",
            "Epoch 20, Batch 6/29: Loss = 0.8558, Time = 20.42s\n",
            "Epoch 20, Batch 7/29: Loss = 0.7213, Time = 20.46s\n",
            "Epoch 20, Batch 8/29: Loss = 0.7972, Time = 20.14s\n",
            "Epoch 20, Batch 9/29: Loss = 0.7448, Time = 20.35s\n",
            "Epoch 20, Batch 10/29: Loss = 0.7482, Time = 20.37s\n",
            "Epoch 20, Batch 11/29: Loss = 0.6658, Time = 20.57s\n",
            "Epoch 20, Batch 12/29: Loss = 0.7406, Time = 20.38s\n",
            "Epoch 20, Batch 13/29: Loss = 0.7553, Time = 20.16s\n",
            "Epoch 20, Batch 14/29: Loss = 0.8256, Time = 20.43s\n",
            "Epoch 20, Batch 15/29: Loss = 0.7375, Time = 20.49s\n",
            "Epoch 20, Batch 16/29: Loss = 0.6963, Time = 20.76s\n",
            "Epoch 20, Batch 17/29: Loss = 0.7618, Time = 20.63s\n",
            "Epoch 20, Batch 18/29: Loss = 0.7768, Time = 20.34s\n",
            "Epoch 20, Batch 19/29: Loss = 0.6908, Time = 20.65s\n",
            "Epoch 20, Batch 20/29: Loss = 0.7828, Time = 20.74s\n",
            "Epoch 20, Batch 21/29: Loss = 0.7918, Time = 20.43s\n",
            "Epoch 20, Batch 22/29: Loss = 0.8034, Time = 20.51s\n",
            "Epoch 20, Batch 23/29: Loss = 0.7221, Time = 20.36s\n",
            "Epoch 20, Batch 24/29: Loss = 0.7969, Time = 20.68s\n",
            "Epoch 20, Batch 25/29: Loss = 0.7939, Time = 20.43s\n",
            "Epoch 20, Batch 26/29: Loss = 0.7939, Time = 20.59s\n",
            "Epoch 20, Batch 27/29: Loss = 0.7329, Time = 20.59s\n",
            "Epoch 20, Batch 28/29: Loss = 0.7507, Time = 20.23s\n",
            "Epoch 20, Batch 29/29: Loss = 0.8749, Time = 3.26s\n",
            "Epoch 20: Train Loss = 0.7679, Val Loss = 0.6625, Time = 12385.23s\n",
            "Epoch 21, Batch 1/29: Loss = 0.8181, Time = 20.08s\n",
            "Epoch 21, Batch 2/29: Loss = 0.7685, Time = 20.28s\n",
            "Epoch 21, Batch 3/29: Loss = 0.7902, Time = 20.55s\n",
            "Epoch 21, Batch 4/29: Loss = 0.8142, Time = 20.43s\n",
            "Epoch 21, Batch 5/29: Loss = 0.7011, Time = 20.47s\n",
            "Epoch 21, Batch 6/29: Loss = 0.6920, Time = 20.46s\n",
            "Epoch 21, Batch 7/29: Loss = 0.8325, Time = 20.02s\n",
            "Epoch 21, Batch 8/29: Loss = 0.6941, Time = 20.36s\n",
            "Epoch 21, Batch 9/29: Loss = 0.6908, Time = 20.43s\n",
            "Epoch 21, Batch 10/29: Loss = 0.8026, Time = 20.38s\n",
            "Epoch 21, Batch 11/29: Loss = 0.7635, Time = 20.44s\n",
            "Epoch 21, Batch 12/29: Loss = 0.8104, Time = 20.09s\n",
            "Epoch 21, Batch 13/29: Loss = 0.7841, Time = 20.49s\n",
            "Epoch 21, Batch 14/29: Loss = 0.6737, Time = 20.41s\n",
            "Epoch 21, Batch 15/29: Loss = 0.7905, Time = 20.47s\n",
            "Epoch 21, Batch 16/29: Loss = 0.6881, Time = 20.43s\n",
            "Epoch 21, Batch 17/29: Loss = 0.6787, Time = 20.49s\n",
            "Epoch 21, Batch 18/29: Loss = 0.8145, Time = 20.17s\n",
            "Epoch 21, Batch 19/29: Loss = 0.6369, Time = 20.46s\n",
            "Epoch 21, Batch 20/29: Loss = 0.7493, Time = 20.50s\n",
            "Epoch 21, Batch 21/29: Loss = 0.7716, Time = 20.31s\n",
            "Epoch 21, Batch 22/29: Loss = 0.7887, Time = 20.50s\n",
            "Epoch 21, Batch 23/29: Loss = 0.8321, Time = 20.63s\n",
            "Epoch 21, Batch 24/29: Loss = 0.6990, Time = 20.18s\n",
            "Epoch 21, Batch 25/29: Loss = 0.7309, Time = 20.40s\n",
            "Epoch 21, Batch 26/29: Loss = 0.7754, Time = 20.49s\n",
            "Epoch 21, Batch 27/29: Loss = 0.8091, Time = 20.61s\n",
            "Epoch 21, Batch 28/29: Loss = 0.7236, Time = 20.57s\n",
            "Epoch 21, Batch 29/29: Loss = 0.7876, Time = 3.34s\n",
            "Epoch 21: Train Loss = 0.7556, Val Loss = 0.6545, Time = 12974.81s\n",
            "Epoch 22, Batch 1/29: Loss = 0.7464, Time = 20.08s\n",
            "Epoch 22, Batch 2/29: Loss = 0.7065, Time = 20.23s\n",
            "Epoch 22, Batch 3/29: Loss = 0.7132, Time = 20.40s\n",
            "Epoch 22, Batch 4/29: Loss = 0.8124, Time = 20.31s\n",
            "Epoch 22, Batch 5/29: Loss = 0.8067, Time = 20.17s\n",
            "Epoch 22, Batch 6/29: Loss = 0.7503, Time = 20.35s\n",
            "Epoch 22, Batch 7/29: Loss = 0.7975, Time = 20.08s\n",
            "Epoch 22, Batch 8/29: Loss = 0.7651, Time = 20.30s\n",
            "Epoch 22, Batch 9/29: Loss = 0.7000, Time = 20.27s\n",
            "Epoch 22, Batch 10/29: Loss = 0.7721, Time = 20.40s\n",
            "Epoch 22, Batch 11/29: Loss = 0.7246, Time = 20.29s\n",
            "Epoch 22, Batch 12/29: Loss = 0.7459, Time = 20.39s\n",
            "Epoch 22, Batch 13/29: Loss = 0.6779, Time = 20.17s\n",
            "Epoch 22, Batch 14/29: Loss = 0.7568, Time = 20.47s\n",
            "Epoch 22, Batch 15/29: Loss = 0.7177, Time = 20.29s\n",
            "Epoch 22, Batch 16/29: Loss = 0.7019, Time = 20.38s\n",
            "Epoch 22, Batch 17/29: Loss = 0.7953, Time = 20.39s\n",
            "Epoch 22, Batch 18/29: Loss = 0.7475, Time = 20.35s\n",
            "Epoch 22, Batch 19/29: Loss = 0.7761, Time = 20.41s\n",
            "Epoch 22, Batch 20/29: Loss = 0.8024, Time = 20.07s\n",
            "Epoch 22, Batch 21/29: Loss = 0.7423, Time = 20.37s\n",
            "Epoch 22, Batch 22/29: Loss = 0.7376, Time = 20.57s\n",
            "Epoch 22, Batch 23/29: Loss = 0.6955, Time = 20.53s\n",
            "Epoch 22, Batch 24/29: Loss = 0.7469, Time = 20.14s\n",
            "Epoch 22, Batch 25/29: Loss = 0.7192, Time = 20.49s\n",
            "Epoch 22, Batch 26/29: Loss = 0.7368, Time = 20.36s\n",
            "Epoch 22, Batch 27/29: Loss = 0.6807, Time = 20.40s\n",
            "Epoch 22, Batch 28/29: Loss = 0.7988, Time = 20.39s\n",
            "Epoch 22, Batch 29/29: Loss = 0.7868, Time = 3.28s\n",
            "Epoch 22: Train Loss = 0.7469, Val Loss = 0.6475, Time = 13562.40s\n",
            "Epoch 23, Batch 1/29: Loss = 0.7798, Time = 20.13s\n",
            "Epoch 23, Batch 2/29: Loss = 0.6434, Time = 20.21s\n",
            "Epoch 23, Batch 3/29: Loss = 0.7500, Time = 20.38s\n",
            "Epoch 23, Batch 4/29: Loss = 0.7711, Time = 20.01s\n",
            "Epoch 23, Batch 5/29: Loss = 0.7546, Time = 20.35s\n",
            "Epoch 23, Batch 6/29: Loss = 0.7372, Time = 20.31s\n",
            "Epoch 23, Batch 7/29: Loss = 0.7869, Time = 20.38s\n",
            "Epoch 23, Batch 8/29: Loss = 0.7558, Time = 20.04s\n",
            "Epoch 23, Batch 9/29: Loss = 0.7145, Time = 20.35s\n",
            "Epoch 23, Batch 10/29: Loss = 0.7876, Time = 20.47s\n",
            "Epoch 23, Batch 11/29: Loss = 0.7042, Time = 20.42s\n",
            "Epoch 23, Batch 12/29: Loss = 0.7387, Time = 20.49s\n",
            "Epoch 23, Batch 13/29: Loss = 0.7153, Time = 20.61s\n",
            "Epoch 23, Batch 14/29: Loss = 0.7983, Time = 20.13s\n",
            "Epoch 23, Batch 15/29: Loss = 0.7347, Time = 20.42s\n",
            "Epoch 23, Batch 16/29: Loss = 0.6415, Time = 20.37s\n",
            "Epoch 23, Batch 17/29: Loss = 0.8110, Time = 20.61s\n",
            "Epoch 23, Batch 18/29: Loss = 0.7081, Time = 20.37s\n",
            "Epoch 23, Batch 19/29: Loss = 0.7377, Time = 20.39s\n",
            "Epoch 23, Batch 20/29: Loss = 0.7697, Time = 20.24s\n",
            "Epoch 23, Batch 21/29: Loss = 0.7455, Time = 20.46s\n",
            "Epoch 23, Batch 22/29: Loss = 0.8240, Time = 20.33s\n",
            "Epoch 23, Batch 23/29: Loss = 0.6505, Time = 20.53s\n",
            "Epoch 23, Batch 24/29: Loss = 0.7186, Time = 20.65s\n",
            "Epoch 23, Batch 25/29: Loss = 0.7645, Time = 20.54s\n",
            "Epoch 23, Batch 26/29: Loss = 0.7221, Time = 20.23s\n",
            "Epoch 23, Batch 27/29: Loss = 0.6551, Time = 20.50s\n",
            "Epoch 23, Batch 28/29: Loss = 0.7176, Time = 20.44s\n",
            "Epoch 23, Batch 29/29: Loss = 0.8586, Time = 3.29s\n",
            "Epoch 23: Train Loss = 0.7413, Val Loss = 0.6413, Time = 14151.19s\n",
            "Epoch 24, Batch 1/29: Loss = 0.7947, Time = 20.08s\n",
            "Epoch 24, Batch 2/29: Loss = 0.7782, Time = 20.28s\n",
            "Epoch 24, Batch 3/29: Loss = 0.6792, Time = 20.39s\n",
            "Epoch 24, Batch 4/29: Loss = 0.7166, Time = 20.25s\n",
            "Epoch 24, Batch 5/29: Loss = 0.6979, Time = 19.99s\n",
            "Epoch 24, Batch 6/29: Loss = 0.7764, Time = 20.38s\n",
            "Epoch 24, Batch 7/29: Loss = 0.7550, Time = 20.31s\n",
            "Epoch 24, Batch 8/29: Loss = 0.6493, Time = 20.39s\n",
            "Epoch 24, Batch 9/29: Loss = 0.7272, Time = 20.41s\n",
            "Epoch 24, Batch 10/29: Loss = 0.6878, Time = 20.06s\n",
            "Epoch 24, Batch 11/29: Loss = 0.6638, Time = 20.40s\n",
            "Epoch 24, Batch 12/29: Loss = 0.8092, Time = 20.38s\n",
            "Epoch 24, Batch 13/29: Loss = 0.7774, Time = 20.40s\n",
            "Epoch 24, Batch 14/29: Loss = 0.6454, Time = 20.48s\n",
            "Epoch 24, Batch 15/29: Loss = 0.7477, Time = 20.15s\n",
            "Epoch 24, Batch 16/29: Loss = 0.6999, Time = 20.59s\n",
            "Epoch 24, Batch 17/29: Loss = 0.6719, Time = 20.57s\n",
            "Epoch 24, Batch 18/29: Loss = 0.7040, Time = 20.58s\n",
            "Epoch 24, Batch 19/29: Loss = 0.7379, Time = 20.60s\n",
            "Epoch 24, Batch 20/29: Loss = 0.7693, Time = 20.16s\n",
            "Epoch 24, Batch 21/29: Loss = 0.7541, Time = 20.53s\n",
            "Epoch 24, Batch 22/29: Loss = 0.6846, Time = 20.59s\n",
            "Epoch 24, Batch 23/29: Loss = 0.8319, Time = 20.49s\n",
            "Epoch 24, Batch 24/29: Loss = 0.7257, Time = 20.51s\n",
            "Epoch 24, Batch 25/29: Loss = 0.7447, Time = 20.22s\n",
            "Epoch 24, Batch 26/29: Loss = 0.7575, Time = 20.55s\n",
            "Epoch 24, Batch 27/29: Loss = 0.7276, Time = 20.50s\n",
            "Epoch 24, Batch 28/29: Loss = 0.7301, Time = 20.47s\n",
            "Epoch 24, Batch 29/29: Loss = 0.8342, Time = 3.31s\n",
            "Epoch 24: Train Loss = 0.7338, Val Loss = 0.6352, Time = 14740.33s\n",
            "Epoch 25, Batch 1/29: Loss = 0.7320, Time = 20.15s\n",
            "Epoch 25, Batch 2/29: Loss = 0.7204, Time = 20.18s\n",
            "Epoch 25, Batch 3/29: Loss = 0.8263, Time = 20.13s\n",
            "Epoch 25, Batch 4/29: Loss = 0.7303, Time = 20.41s\n",
            "Epoch 25, Batch 5/29: Loss = 0.7962, Time = 20.60s\n",
            "Epoch 25, Batch 6/29: Loss = 0.7051, Time = 20.38s\n",
            "Epoch 25, Batch 7/29: Loss = 0.7491, Time = 20.40s\n",
            "Epoch 25, Batch 8/29: Loss = 0.7134, Time = 19.95s\n",
            "Epoch 25, Batch 9/29: Loss = 0.7809, Time = 20.43s\n",
            "Epoch 25, Batch 10/29: Loss = 0.8457, Time = 20.42s\n",
            "Epoch 25, Batch 11/29: Loss = 0.7470, Time = 20.58s\n",
            "Epoch 25, Batch 12/29: Loss = 0.7891, Time = 20.58s\n",
            "Epoch 25, Batch 13/29: Loss = 0.6865, Time = 20.47s\n",
            "Epoch 25, Batch 14/29: Loss = 0.6263, Time = 20.17s\n",
            "Epoch 25, Batch 15/29: Loss = 0.6832, Time = 20.48s\n",
            "Epoch 25, Batch 16/29: Loss = 0.7066, Time = 20.33s\n",
            "Epoch 25, Batch 17/29: Loss = 0.7232, Time = 20.43s\n",
            "Epoch 25, Batch 18/29: Loss = 0.7057, Time = 20.11s\n",
            "Epoch 25, Batch 19/29: Loss = 0.6627, Time = 20.40s\n",
            "Epoch 25, Batch 20/29: Loss = 0.7415, Time = 20.36s\n",
            "Epoch 25, Batch 21/29: Loss = 0.7313, Time = 20.37s\n",
            "Epoch 25, Batch 22/29: Loss = 0.6163, Time = 20.39s\n",
            "Epoch 25, Batch 23/29: Loss = 0.6603, Time = 20.39s\n",
            "Epoch 25, Batch 24/29: Loss = 0.7219, Time = 20.08s\n",
            "Epoch 25, Batch 25/29: Loss = 0.6949, Time = 20.53s\n",
            "Epoch 25, Batch 26/29: Loss = 0.6801, Time = 20.43s\n",
            "Epoch 25, Batch 27/29: Loss = 0.7857, Time = 20.46s\n",
            "Epoch 25, Batch 28/29: Loss = 0.7417, Time = 20.51s\n",
            "Epoch 25, Batch 29/29: Loss = 0.6607, Time = 3.28s\n",
            "Epoch 25: Train Loss = 0.7229, Val Loss = 0.6300, Time = 15328.97s\n",
            "Epoch 26, Batch 1/29: Loss = 0.6607, Time = 20.23s\n",
            "Epoch 26, Batch 2/29: Loss = 0.8083, Time = 19.95s\n",
            "Epoch 26, Batch 3/29: Loss = 0.7167, Time = 20.36s\n",
            "Epoch 26, Batch 4/29: Loss = 0.6803, Time = 20.25s\n",
            "Epoch 26, Batch 5/29: Loss = 0.7924, Time = 20.35s\n",
            "Epoch 26, Batch 6/29: Loss = 0.7290, Time = 20.38s\n",
            "Epoch 26, Batch 7/29: Loss = 0.7256, Time = 20.46s\n",
            "Epoch 26, Batch 8/29: Loss = 0.6783, Time = 20.02s\n",
            "Epoch 26, Batch 9/29: Loss = 0.5828, Time = 20.32s\n",
            "Epoch 26, Batch 10/29: Loss = 0.6993, Time = 20.36s\n",
            "Epoch 26, Batch 11/29: Loss = 0.6708, Time = 20.46s\n",
            "Epoch 26, Batch 12/29: Loss = 0.7948, Time = 20.55s\n",
            "Epoch 26, Batch 13/29: Loss = 0.7824, Time = 20.18s\n",
            "Epoch 26, Batch 14/29: Loss = 0.8144, Time = 20.46s\n",
            "Epoch 26, Batch 15/29: Loss = 0.7008, Time = 20.54s\n",
            "Epoch 26, Batch 16/29: Loss = 0.7545, Time = 20.45s\n",
            "Epoch 26, Batch 17/29: Loss = 0.6450, Time = 20.49s\n",
            "Epoch 26, Batch 18/29: Loss = 0.6745, Time = 20.32s\n",
            "Epoch 26, Batch 19/29: Loss = 0.7434, Time = 20.56s\n",
            "Epoch 26, Batch 20/29: Loss = 0.8023, Time = 20.50s\n",
            "Epoch 26, Batch 21/29: Loss = 0.7571, Time = 20.50s\n",
            "Epoch 26, Batch 22/29: Loss = 0.7262, Time = 20.40s\n",
            "Epoch 26, Batch 23/29: Loss = 0.6771, Time = 20.26s\n",
            "Epoch 26, Batch 24/29: Loss = 0.7301, Time = 20.63s\n",
            "Epoch 26, Batch 25/29: Loss = 0.6963, Time = 20.48s\n",
            "Epoch 26, Batch 26/29: Loss = 0.6828, Time = 20.43s\n",
            "Epoch 26, Batch 27/29: Loss = 0.6702, Time = 20.74s\n",
            "Epoch 26, Batch 28/29: Loss = 0.7187, Time = 20.22s\n",
            "Epoch 26, Batch 29/29: Loss = 0.8739, Time = 3.60s\n",
            "Epoch 26: Train Loss = 0.7237, Val Loss = 0.6263, Time = 15918.62s\n",
            "Epoch 27, Batch 1/29: Loss = 0.7278, Time = 20.00s\n",
            "Epoch 27, Batch 2/29: Loss = 0.7247, Time = 20.35s\n",
            "Epoch 27, Batch 3/29: Loss = 0.6691, Time = 20.22s\n",
            "Epoch 27, Batch 4/29: Loss = 0.6781, Time = 20.34s\n",
            "Epoch 27, Batch 5/29: Loss = 0.6297, Time = 20.36s\n",
            "Epoch 27, Batch 6/29: Loss = 0.6905, Time = 20.52s\n",
            "Epoch 27, Batch 7/29: Loss = 0.6511, Time = 20.02s\n",
            "Epoch 27, Batch 8/29: Loss = 0.7160, Time = 20.37s\n",
            "Epoch 27, Batch 9/29: Loss = 0.7131, Time = 20.37s\n",
            "Epoch 27, Batch 10/29: Loss = 0.7006, Time = 20.42s\n",
            "Epoch 27, Batch 11/29: Loss = 0.7261, Time = 20.48s\n",
            "Epoch 27, Batch 12/29: Loss = 0.7669, Time = 20.06s\n",
            "Epoch 27, Batch 13/29: Loss = 0.7446, Time = 20.41s\n",
            "Epoch 27, Batch 14/29: Loss = 0.7763, Time = 20.42s\n",
            "Epoch 27, Batch 15/29: Loss = 0.7305, Time = 20.59s\n",
            "Epoch 27, Batch 16/29: Loss = 0.7932, Time = 20.46s\n",
            "Epoch 27, Batch 17/29: Loss = 0.7258, Time = 20.11s\n",
            "Epoch 27, Batch 18/29: Loss = 0.7114, Time = 20.40s\n",
            "Epoch 27, Batch 19/29: Loss = 0.6486, Time = 20.52s\n",
            "Epoch 27, Batch 20/29: Loss = 0.7331, Time = 20.48s\n",
            "Epoch 27, Batch 21/29: Loss = 0.7332, Time = 20.38s\n",
            "Epoch 27, Batch 22/29: Loss = 0.6515, Time = 20.48s\n",
            "Epoch 27, Batch 23/29: Loss = 0.7761, Time = 20.21s\n",
            "Epoch 27, Batch 24/29: Loss = 0.7108, Time = 20.36s\n",
            "Epoch 27, Batch 25/29: Loss = 0.7741, Time = 20.56s\n",
            "Epoch 27, Batch 26/29: Loss = 0.7401, Time = 20.63s\n",
            "Epoch 27, Batch 27/29: Loss = 0.6488, Time = 20.44s\n",
            "Epoch 27, Batch 28/29: Loss = 0.6943, Time = 20.12s\n",
            "Epoch 27, Batch 29/29: Loss = 0.7808, Time = 3.30s\n",
            "Epoch 27: Train Loss = 0.7161, Val Loss = 0.6229, Time = 16507.15s\n",
            "Epoch 28, Batch 1/29: Loss = 0.7481, Time = 20.16s\n",
            "Epoch 28, Batch 2/29: Loss = 0.6728, Time = 20.30s\n",
            "Epoch 28, Batch 3/29: Loss = 0.5759, Time = 20.49s\n",
            "Epoch 28, Batch 4/29: Loss = 0.8163, Time = 20.31s\n",
            "Epoch 28, Batch 5/29: Loss = 0.6781, Time = 20.29s\n",
            "Epoch 28, Batch 6/29: Loss = 0.6576, Time = 20.35s\n",
            "Epoch 28, Batch 7/29: Loss = 0.7415, Time = 20.06s\n",
            "Epoch 28, Batch 8/29: Loss = 0.7169, Time = 20.40s\n",
            "Epoch 28, Batch 9/29: Loss = 0.6589, Time = 20.51s\n",
            "Epoch 28, Batch 10/29: Loss = 0.7649, Time = 20.31s\n",
            "Epoch 28, Batch 11/29: Loss = 0.7463, Time = 20.48s\n",
            "Epoch 28, Batch 12/29: Loss = 0.6832, Time = 20.37s\n",
            "Epoch 28, Batch 13/29: Loss = 0.7133, Time = 20.07s\n",
            "Epoch 28, Batch 14/29: Loss = 0.7693, Time = 20.25s\n",
            "Epoch 28, Batch 15/29: Loss = 0.6840, Time = 20.48s\n",
            "Epoch 28, Batch 16/29: Loss = 0.6823, Time = 20.56s\n",
            "Epoch 28, Batch 17/29: Loss = 0.7754, Time = 20.56s\n",
            "Epoch 28, Batch 18/29: Loss = 0.6780, Time = 20.13s\n",
            "Epoch 28, Batch 19/29: Loss = 0.6311, Time = 20.39s\n",
            "Epoch 28, Batch 20/29: Loss = 0.7395, Time = 20.52s\n",
            "Epoch 28, Batch 21/29: Loss = 0.6563, Time = 20.48s\n",
            "Epoch 28, Batch 22/29: Loss = 0.7312, Time = 20.41s\n",
            "Epoch 28, Batch 23/29: Loss = 0.7501, Time = 20.56s\n",
            "Epoch 28, Batch 24/29: Loss = 0.7168, Time = 20.48s\n",
            "Epoch 28, Batch 25/29: Loss = 0.6345, Time = 20.51s\n",
            "Epoch 28, Batch 26/29: Loss = 0.7299, Time = 20.58s\n",
            "Epoch 28, Batch 27/29: Loss = 0.7165, Time = 20.51s\n",
            "Epoch 28, Batch 28/29: Loss = 0.8012, Time = 20.55s\n",
            "Epoch 28, Batch 29/29: Loss = 0.7095, Time = 3.27s\n",
            "Epoch 28: Train Loss = 0.7096, Val Loss = 0.6186, Time = 17096.71s\n",
            "Epoch 29, Batch 1/29: Loss = 0.6869, Time = 19.83s\n",
            "Epoch 29, Batch 2/29: Loss = 0.6766, Time = 20.19s\n",
            "Epoch 29, Batch 3/29: Loss = 0.7605, Time = 20.27s\n",
            "Epoch 29, Batch 4/29: Loss = 0.7679, Time = 20.30s\n",
            "Epoch 29, Batch 5/29: Loss = 0.5951, Time = 20.30s\n",
            "Epoch 29, Batch 6/29: Loss = 0.7982, Time = 20.33s\n",
            "Epoch 29, Batch 7/29: Loss = 0.7068, Time = 20.19s\n",
            "Epoch 29, Batch 8/29: Loss = 0.6392, Time = 20.33s\n",
            "Epoch 29, Batch 9/29: Loss = 0.7073, Time = 20.37s\n",
            "Epoch 29, Batch 10/29: Loss = 0.7120, Time = 20.41s\n",
            "Epoch 29, Batch 11/29: Loss = 0.7246, Time = 20.14s\n",
            "Epoch 29, Batch 12/29: Loss = 0.6328, Time = 20.36s\n",
            "Epoch 29, Batch 13/29: Loss = 0.7207, Time = 20.38s\n",
            "Epoch 29, Batch 14/29: Loss = 0.6341, Time = 20.55s\n",
            "Epoch 29, Batch 15/29: Loss = 0.7426, Time = 20.43s\n",
            "Epoch 29, Batch 16/29: Loss = 0.7068, Time = 20.43s\n",
            "Epoch 29, Batch 17/29: Loss = 0.7728, Time = 20.15s\n",
            "Epoch 29, Batch 18/29: Loss = 0.7584, Time = 20.30s\n",
            "Epoch 29, Batch 19/29: Loss = 0.6959, Time = 20.59s\n",
            "Epoch 29, Batch 20/29: Loss = 0.7234, Time = 20.40s\n",
            "Epoch 29, Batch 21/29: Loss = 0.6366, Time = 20.44s\n",
            "Epoch 29, Batch 22/29: Loss = 0.7171, Time = 20.11s\n",
            "Epoch 29, Batch 23/29: Loss = 0.6830, Time = 20.38s\n",
            "Epoch 29, Batch 24/29: Loss = 0.7217, Time = 20.35s\n",
            "Epoch 29, Batch 25/29: Loss = 0.7360, Time = 20.29s\n",
            "Epoch 29, Batch 26/29: Loss = 0.7461, Time = 20.41s\n",
            "Epoch 29, Batch 27/29: Loss = 0.7336, Time = 20.44s\n",
            "Epoch 29, Batch 28/29: Loss = 0.6383, Time = 20.57s\n",
            "Epoch 29, Batch 29/29: Loss = 0.5926, Time = 3.32s\n",
            "Epoch 29: Train Loss = 0.7023, Val Loss = 0.6160, Time = 17684.59s\n",
            "Epoch 30, Batch 1/29: Loss = 0.7110, Time = 19.77s\n",
            "Epoch 30, Batch 2/29: Loss = 0.7737, Time = 20.25s\n",
            "Epoch 30, Batch 3/29: Loss = 0.6810, Time = 20.39s\n",
            "Epoch 30, Batch 4/29: Loss = 0.7289, Time = 20.18s\n",
            "Epoch 30, Batch 5/29: Loss = 0.6770, Time = 20.26s\n",
            "Epoch 30, Batch 6/29: Loss = 0.6695, Time = 20.37s\n",
            "Epoch 30, Batch 7/29: Loss = 0.7131, Time = 19.98s\n",
            "Epoch 30, Batch 8/29: Loss = 0.6878, Time = 20.40s\n",
            "Epoch 30, Batch 9/29: Loss = 0.6973, Time = 20.40s\n",
            "Epoch 30, Batch 10/29: Loss = 0.6315, Time = 20.41s\n",
            "Epoch 30, Batch 11/29: Loss = 0.7866, Time = 20.40s\n",
            "Epoch 30, Batch 12/29: Loss = 0.6782, Time = 20.28s\n",
            "Epoch 30, Batch 13/29: Loss = 0.6954, Time = 20.67s\n",
            "Epoch 30, Batch 14/29: Loss = 0.6096, Time = 20.65s\n",
            "Epoch 30, Batch 15/29: Loss = 0.6943, Time = 20.68s\n",
            "Epoch 30, Batch 16/29: Loss = 0.7122, Time = 20.12s\n",
            "Epoch 30, Batch 17/29: Loss = 0.7670, Time = 20.56s\n",
            "Epoch 30, Batch 18/29: Loss = 0.7742, Time = 20.62s\n",
            "Epoch 30, Batch 19/29: Loss = 0.6972, Time = 20.43s\n",
            "Epoch 30, Batch 20/29: Loss = 0.6693, Time = 20.42s\n",
            "Epoch 30, Batch 21/29: Loss = 0.6268, Time = 20.52s\n",
            "Epoch 30, Batch 22/29: Loss = 0.6397, Time = 20.11s\n",
            "Epoch 30, Batch 23/29: Loss = 0.7832, Time = 20.42s\n",
            "Epoch 30, Batch 24/29: Loss = 0.6668, Time = 20.36s\n",
            "Epoch 30, Batch 25/29: Loss = 0.7610, Time = 20.53s\n",
            "Epoch 30, Batch 26/29: Loss = 0.7124, Time = 20.53s\n",
            "Epoch 30, Batch 27/29: Loss = 0.6824, Time = 20.15s\n",
            "Epoch 30, Batch 28/29: Loss = 0.7541, Time = 20.50s\n",
            "Epoch 30, Batch 29/29: Loss = 0.5188, Time = 3.32s\n",
            "Epoch 30: Train Loss = 0.6965, Val Loss = 0.6127, Time = 18273.58s\n",
            "Epoch 31, Batch 1/29: Loss = 0.7022, Time = 20.14s\n",
            "Epoch 31, Batch 2/29: Loss = 0.7269, Time = 20.19s\n",
            "Epoch 31, Batch 3/29: Loss = 0.7249, Time = 20.27s\n",
            "Epoch 31, Batch 4/29: Loss = 0.7316, Time = 20.39s\n",
            "Epoch 31, Batch 5/29: Loss = 0.6897, Time = 20.33s\n",
            "Epoch 31, Batch 6/29: Loss = 0.6712, Time = 20.00s\n",
            "Epoch 31, Batch 7/29: Loss = 0.7895, Time = 20.23s\n",
            "Epoch 31, Batch 8/29: Loss = 0.6544, Time = 20.36s\n",
            "Epoch 31, Batch 9/29: Loss = 0.6929, Time = 20.49s\n",
            "Epoch 31, Batch 10/29: Loss = 0.6777, Time = 20.35s\n",
            "Epoch 31, Batch 11/29: Loss = 0.7045, Time = 20.41s\n",
            "Epoch 31, Batch 12/29: Loss = 0.6184, Time = 20.03s\n",
            "Epoch 31, Batch 13/29: Loss = 0.7376, Time = 20.44s\n",
            "Epoch 31, Batch 14/29: Loss = 0.6494, Time = 20.42s\n",
            "Epoch 31, Batch 15/29: Loss = 0.6882, Time = 20.59s\n",
            "Epoch 31, Batch 16/29: Loss = 0.6498, Time = 20.43s\n",
            "Epoch 31, Batch 17/29: Loss = 0.6349, Time = 20.10s\n",
            "Epoch 31, Batch 18/29: Loss = 0.6382, Time = 20.50s\n",
            "Epoch 31, Batch 19/29: Loss = 0.6432, Time = 20.48s\n",
            "Epoch 31, Batch 20/29: Loss = 0.6929, Time = 20.51s\n",
            "Epoch 31, Batch 21/29: Loss = 0.8121, Time = 20.14s\n",
            "Epoch 31, Batch 22/29: Loss = 0.7709, Time = 20.50s\n",
            "Epoch 31, Batch 23/29: Loss = 0.7490, Time = 20.50s\n",
            "Epoch 31, Batch 24/29: Loss = 0.7829, Time = 20.34s\n",
            "Epoch 31, Batch 25/29: Loss = 0.6600, Time = 20.67s\n",
            "Epoch 31, Batch 26/29: Loss = 0.7227, Time = 20.36s\n",
            "Epoch 31, Batch 27/29: Loss = 0.7245, Time = 20.17s\n",
            "Epoch 31, Batch 28/29: Loss = 0.6749, Time = 20.67s\n",
            "Epoch 31, Batch 29/29: Loss = 0.3958, Time = 3.29s\n",
            "Epoch 31: Train Loss = 0.6900, Val Loss = 0.6097, Time = 18861.95s\n",
            "Epoch 32, Batch 1/29: Loss = 0.7492, Time = 20.42s\n",
            "Epoch 32, Batch 2/29: Loss = 0.6630, Time = 20.01s\n",
            "Epoch 32, Batch 3/29: Loss = 0.5816, Time = 20.29s\n",
            "Epoch 32, Batch 4/29: Loss = 0.5992, Time = 20.46s\n",
            "Epoch 32, Batch 5/29: Loss = 0.6790, Time = 20.31s\n",
            "Epoch 32, Batch 6/29: Loss = 0.7786, Time = 20.36s\n",
            "Epoch 32, Batch 7/29: Loss = 0.7617, Time = 20.44s\n",
            "Epoch 32, Batch 8/29: Loss = 0.7323, Time = 20.05s\n",
            "Epoch 32, Batch 9/29: Loss = 0.6634, Time = 20.30s\n",
            "Epoch 32, Batch 10/29: Loss = 0.6812, Time = 20.33s\n",
            "Epoch 32, Batch 11/29: Loss = 0.7441, Time = 20.42s\n",
            "Epoch 32, Batch 12/29: Loss = 0.6576, Time = 20.08s\n",
            "Epoch 32, Batch 13/29: Loss = 0.7391, Time = 20.41s\n",
            "Epoch 32, Batch 14/29: Loss = 0.6011, Time = 20.33s\n",
            "Epoch 32, Batch 15/29: Loss = 0.7755, Time = 20.38s\n",
            "Epoch 32, Batch 16/29: Loss = 0.7360, Time = 20.41s\n",
            "Epoch 32, Batch 17/29: Loss = 0.6635, Time = 20.50s\n",
            "Epoch 32, Batch 18/29: Loss = 0.6951, Time = 20.13s\n",
            "Epoch 32, Batch 19/29: Loss = 0.6522, Time = 20.46s\n",
            "Epoch 32, Batch 20/29: Loss = 0.6127, Time = 20.38s\n",
            "Epoch 32, Batch 21/29: Loss = 0.7014, Time = 20.38s\n",
            "Epoch 32, Batch 22/29: Loss = 0.7206, Time = 20.43s\n",
            "Epoch 32, Batch 23/29: Loss = 0.7316, Time = 20.14s\n",
            "Epoch 32, Batch 24/29: Loss = 0.7375, Time = 20.41s\n",
            "Epoch 32, Batch 25/29: Loss = 0.6973, Time = 20.39s\n",
            "Epoch 32, Batch 26/29: Loss = 0.6233, Time = 20.45s\n",
            "Epoch 32, Batch 27/29: Loss = 0.7800, Time = 20.45s\n",
            "Epoch 32, Batch 28/29: Loss = 0.7038, Time = 20.65s\n",
            "Epoch 32, Batch 29/29: Loss = 0.7789, Time = 3.29s\n",
            "Epoch 32: Train Loss = 0.6979, Val Loss = 0.6082, Time = 19450.18s\n",
            "Epoch 33, Batch 1/29: Loss = 0.6753, Time = 20.23s\n",
            "Epoch 33, Batch 2/29: Loss = 0.6856, Time = 20.03s\n",
            "Epoch 33, Batch 3/29: Loss = 0.7186, Time = 20.32s\n",
            "Epoch 33, Batch 4/29: Loss = 0.6396, Time = 20.32s\n",
            "Epoch 33, Batch 5/29: Loss = 0.6615, Time = 20.26s\n",
            "Epoch 33, Batch 6/29: Loss = 0.6954, Time = 20.22s\n",
            "Epoch 33, Batch 7/29: Loss = 0.6388, Time = 20.33s\n",
            "Epoch 33, Batch 8/29: Loss = 0.7259, Time = 19.97s\n",
            "Epoch 33, Batch 9/29: Loss = 0.7287, Time = 20.40s\n",
            "Epoch 33, Batch 10/29: Loss = 0.6930, Time = 20.35s\n",
            "Epoch 33, Batch 11/29: Loss = 0.7039, Time = 20.46s\n",
            "Epoch 33, Batch 12/29: Loss = 0.7642, Time = 20.48s\n",
            "Epoch 33, Batch 13/29: Loss = 0.6356, Time = 20.15s\n",
            "Epoch 33, Batch 14/29: Loss = 0.7007, Time = 20.37s\n",
            "Epoch 33, Batch 15/29: Loss = 0.6032, Time = 20.44s\n",
            "Epoch 33, Batch 16/29: Loss = 0.7468, Time = 20.47s\n",
            "Epoch 33, Batch 17/29: Loss = 0.7240, Time = 19.92s\n",
            "Epoch 33, Batch 18/29: Loss = 0.7274, Time = 20.44s\n",
            "Epoch 33, Batch 19/29: Loss = 0.5998, Time = 20.42s\n",
            "Epoch 33, Batch 20/29: Loss = 0.8192, Time = 20.32s\n",
            "Epoch 33, Batch 21/29: Loss = 0.6753, Time = 20.39s\n",
            "Epoch 33, Batch 22/29: Loss = 0.6544, Time = 20.44s\n",
            "Epoch 33, Batch 23/29: Loss = 0.7221, Time = 20.10s\n",
            "Epoch 33, Batch 24/29: Loss = 0.6495, Time = 20.43s\n",
            "Epoch 33, Batch 25/29: Loss = 0.6859, Time = 20.45s\n",
            "Epoch 33, Batch 26/29: Loss = 0.6879, Time = 20.27s\n",
            "Epoch 33, Batch 27/29: Loss = 0.7630, Time = 20.48s\n",
            "Epoch 33, Batch 28/29: Loss = 0.6690, Time = 20.01s\n",
            "Epoch 33, Batch 29/29: Loss = 0.7196, Time = 3.59s\n",
            "Epoch 33: Train Loss = 0.6936, Val Loss = 0.6053, Time = 20037.42s\n",
            "Epoch 34, Batch 1/29: Loss = 0.6892, Time = 20.02s\n",
            "Epoch 34, Batch 2/29: Loss = 0.6578, Time = 19.97s\n",
            "Epoch 34, Batch 3/29: Loss = 0.7686, Time = 20.12s\n",
            "Epoch 34, Batch 4/29: Loss = 0.7593, Time = 20.12s\n",
            "Epoch 34, Batch 5/29: Loss = 0.5683, Time = 20.35s\n",
            "Epoch 34, Batch 6/29: Loss = 0.7277, Time = 20.23s\n",
            "Epoch 34, Batch 7/29: Loss = 0.6554, Time = 20.33s\n",
            "Epoch 34, Batch 8/29: Loss = 0.6997, Time = 20.03s\n",
            "Epoch 34, Batch 9/29: Loss = 0.6517, Time = 20.20s\n",
            "Epoch 34, Batch 10/29: Loss = 0.6205, Time = 20.31s\n",
            "Epoch 34, Batch 11/29: Loss = 0.6921, Time = 20.31s\n",
            "Epoch 34, Batch 12/29: Loss = 0.6590, Time = 20.27s\n",
            "Epoch 34, Batch 13/29: Loss = 0.6841, Time = 20.04s\n",
            "Epoch 34, Batch 14/29: Loss = 0.7097, Time = 20.37s\n",
            "Epoch 34, Batch 15/29: Loss = 0.7439, Time = 20.29s\n",
            "Epoch 34, Batch 16/29: Loss = 0.7072, Time = 20.31s\n",
            "Epoch 34, Batch 17/29: Loss = 0.6931, Time = 20.25s\n",
            "Epoch 34, Batch 18/29: Loss = 0.6397, Time = 19.90s\n",
            "Epoch 34, Batch 19/29: Loss = 0.7095, Time = 20.62s\n",
            "Epoch 34, Batch 20/29: Loss = 0.7524, Time = 20.52s\n",
            "Epoch 34, Batch 21/29: Loss = 0.6457, Time = 20.36s\n",
            "Epoch 34, Batch 22/29: Loss = 0.6605, Time = 20.35s\n",
            "Epoch 34, Batch 23/29: Loss = 0.7429, Time = 20.01s\n",
            "Epoch 34, Batch 24/29: Loss = 0.6561, Time = 20.21s\n",
            "Epoch 34, Batch 25/29: Loss = 0.7062, Time = 20.42s\n",
            "Epoch 34, Batch 26/29: Loss = 0.7683, Time = 20.47s\n",
            "Epoch 34, Batch 27/29: Loss = 0.6952, Time = 20.44s\n",
            "Epoch 34, Batch 28/29: Loss = 0.6695, Time = 19.99s\n",
            "Epoch 34, Batch 29/29: Loss = 0.6643, Time = 3.28s\n",
            "Epoch 34: Train Loss = 0.6896, Val Loss = 0.6041, Time = 20622.71s\n",
            "Epoch 35, Batch 1/29: Loss = 0.5925, Time = 19.91s\n",
            "Epoch 35, Batch 2/29: Loss = 0.7630, Time = 20.08s\n",
            "Epoch 35, Batch 3/29: Loss = 0.6700, Time = 20.35s\n",
            "Epoch 35, Batch 4/29: Loss = 0.7233, Time = 20.14s\n",
            "Epoch 35, Batch 5/29: Loss = 0.6582, Time = 20.20s\n",
            "Epoch 35, Batch 6/29: Loss = 0.6722, Time = 20.11s\n",
            "Epoch 35, Batch 7/29: Loss = 0.7169, Time = 19.92s\n",
            "Epoch 35, Batch 8/29: Loss = 0.6731, Time = 20.21s\n",
            "Epoch 35, Batch 9/29: Loss = 0.6650, Time = 20.28s\n",
            "Epoch 35, Batch 10/29: Loss = 0.7684, Time = 20.31s\n",
            "Epoch 35, Batch 11/29: Loss = 0.6781, Time = 20.30s\n",
            "Epoch 35, Batch 12/29: Loss = 0.7106, Time = 20.28s\n",
            "Epoch 35, Batch 13/29: Loss = 0.7110, Time = 19.95s\n",
            "Epoch 35, Batch 14/29: Loss = 0.7403, Time = 20.27s\n",
            "Epoch 35, Batch 15/29: Loss = 0.6901, Time = 20.43s\n",
            "Epoch 35, Batch 16/29: Loss = 0.7384, Time = 20.42s\n",
            "Epoch 35, Batch 17/29: Loss = 0.6935, Time = 20.36s\n",
            "Epoch 35, Batch 18/29: Loss = 0.6254, Time = 19.98s\n",
            "Epoch 35, Batch 19/29: Loss = 0.7192, Time = 20.36s\n",
            "Epoch 35, Batch 20/29: Loss = 0.6521, Time = 20.37s\n",
            "Epoch 35, Batch 21/29: Loss = 0.6858, Time = 20.38s\n",
            "Epoch 35, Batch 22/29: Loss = 0.6457, Time = 20.37s\n",
            "Epoch 35, Batch 23/29: Loss = 0.6489, Time = 20.01s\n",
            "Epoch 35, Batch 24/29: Loss = 0.7129, Time = 20.35s\n",
            "Epoch 35, Batch 25/29: Loss = 0.6703, Time = 20.47s\n",
            "Epoch 35, Batch 26/29: Loss = 0.6895, Time = 20.39s\n",
            "Epoch 35, Batch 27/29: Loss = 0.7118, Time = 20.36s\n",
            "Epoch 35, Batch 28/29: Loss = 0.6223, Time = 20.08s\n",
            "Epoch 35, Batch 29/29: Loss = 0.7722, Time = 3.28s\n",
            "Epoch 35: Train Loss = 0.6904, Val Loss = 0.6020, Time = 21207.85s\n",
            "Epoch 36, Batch 1/29: Loss = 0.6586, Time = 20.05s\n",
            "Epoch 36, Batch 2/29: Loss = 0.7310, Time = 20.18s\n",
            "Epoch 36, Batch 3/29: Loss = 0.6519, Time = 20.22s\n",
            "Epoch 36, Batch 4/29: Loss = 0.6524, Time = 20.38s\n",
            "Epoch 36, Batch 5/29: Loss = 0.6235, Time = 20.30s\n",
            "Epoch 36, Batch 6/29: Loss = 0.6977, Time = 20.23s\n",
            "Epoch 36, Batch 7/29: Loss = 0.6625, Time = 20.04s\n",
            "Epoch 36, Batch 8/29: Loss = 0.6969, Time = 20.18s\n",
            "Epoch 36, Batch 9/29: Loss = 0.6375, Time = 20.26s\n",
            "Epoch 36, Batch 10/29: Loss = 0.7905, Time = 20.38s\n",
            "Epoch 36, Batch 11/29: Loss = 0.7493, Time = 20.23s\n",
            "Epoch 36, Batch 12/29: Loss = 0.6177, Time = 20.26s\n",
            "Epoch 36, Batch 13/29: Loss = 0.6360, Time = 19.97s\n",
            "Epoch 36, Batch 14/29: Loss = 0.6332, Time = 20.33s\n",
            "Epoch 36, Batch 15/29: Loss = 0.6553, Time = 20.34s\n",
            "Epoch 36, Batch 16/29: Loss = 0.7032, Time = 20.31s\n",
            "Epoch 36, Batch 17/29: Loss = 0.6544, Time = 20.55s\n",
            "Epoch 36, Batch 18/29: Loss = 0.6955, Time = 20.07s\n",
            "Epoch 36, Batch 19/29: Loss = 0.7222, Time = 20.39s\n",
            "Epoch 36, Batch 20/29: Loss = 0.6933, Time = 20.31s\n",
            "Epoch 36, Batch 21/29: Loss = 0.7804, Time = 20.36s\n",
            "Epoch 36, Batch 22/29: Loss = 0.6262, Time = 20.44s\n",
            "Epoch 36, Batch 23/29: Loss = 0.6417, Time = 20.05s\n",
            "Epoch 36, Batch 24/29: Loss = 0.7002, Time = 20.21s\n",
            "Epoch 36, Batch 25/29: Loss = 0.7006, Time = 20.23s\n",
            "Epoch 36, Batch 26/29: Loss = 0.7090, Time = 20.39s\n",
            "Epoch 36, Batch 27/29: Loss = 0.7258, Time = 20.31s\n",
            "Epoch 36, Batch 28/29: Loss = 0.7369, Time = 20.29s\n",
            "Epoch 36, Batch 29/29: Loss = 0.7838, Time = 3.25s\n",
            "Epoch 36: Train Loss = 0.6885, Val Loss = 0.5997, Time = 21793.43s\n",
            "Epoch 37, Batch 1/29: Loss = 0.6648, Time = 19.72s\n",
            "Epoch 37, Batch 2/29: Loss = 0.6634, Time = 20.26s\n",
            "Epoch 37, Batch 3/29: Loss = 0.7105, Time = 20.27s\n",
            "Epoch 37, Batch 4/29: Loss = 0.6017, Time = 20.26s\n",
            "Epoch 37, Batch 5/29: Loss = 0.6235, Time = 20.34s\n",
            "Epoch 37, Batch 6/29: Loss = 0.6955, Time = 20.31s\n",
            "Epoch 37, Batch 7/29: Loss = 0.6548, Time = 19.94s\n",
            "Epoch 37, Batch 8/29: Loss = 0.6830, Time = 20.15s\n",
            "Epoch 37, Batch 9/29: Loss = 0.7604, Time = 20.31s\n",
            "Epoch 37, Batch 10/29: Loss = 0.6704, Time = 20.17s\n",
            "Epoch 37, Batch 11/29: Loss = 0.7060, Time = 20.29s\n",
            "Epoch 37, Batch 12/29: Loss = 0.6582, Time = 20.03s\n",
            "Epoch 37, Batch 13/29: Loss = 0.6243, Time = 20.27s\n",
            "Epoch 37, Batch 14/29: Loss = 0.6723, Time = 20.29s\n",
            "Epoch 37, Batch 15/29: Loss = 0.7561, Time = 20.32s\n",
            "Epoch 37, Batch 16/29: Loss = 0.7319, Time = 20.49s\n",
            "Epoch 37, Batch 17/29: Loss = 0.6664, Time = 20.37s\n",
            "Epoch 37, Batch 18/29: Loss = 0.7500, Time = 20.06s\n",
            "Epoch 37, Batch 19/29: Loss = 0.7172, Time = 20.34s\n",
            "Epoch 37, Batch 20/29: Loss = 0.6651, Time = 20.29s\n",
            "Epoch 37, Batch 21/29: Loss = 0.6201, Time = 20.38s\n",
            "Epoch 37, Batch 22/29: Loss = 0.6833, Time = 20.48s\n",
            "Epoch 37, Batch 23/29: Loss = 0.6806, Time = 20.11s\n",
            "Epoch 37, Batch 24/29: Loss = 0.7243, Time = 20.38s\n",
            "Epoch 37, Batch 25/29: Loss = 0.7330, Time = 20.35s\n",
            "Epoch 37, Batch 26/29: Loss = 0.6766, Time = 20.39s\n",
            "Epoch 37, Batch 27/29: Loss = 0.6953, Time = 20.47s\n",
            "Epoch 37, Batch 28/29: Loss = 0.6684, Time = 20.05s\n",
            "Epoch 37, Batch 29/29: Loss = 0.5698, Time = 3.28s\n",
            "Epoch 37: Train Loss = 0.6802, Val Loss = 0.5992, Time = 22378.85s\n",
            "Epoch 38, Batch 1/29: Loss = 0.6662, Time = 20.24s\n",
            "Epoch 38, Batch 2/29: Loss = 0.5919, Time = 20.30s\n",
            "Epoch 38, Batch 3/29: Loss = 0.7295, Time = 20.22s\n",
            "Epoch 38, Batch 4/29: Loss = 0.6963, Time = 20.17s\n",
            "Epoch 38, Batch 5/29: Loss = 0.7683, Time = 20.39s\n",
            "Epoch 38, Batch 6/29: Loss = 0.6826, Time = 20.31s\n",
            "Epoch 38, Batch 7/29: Loss = 0.6628, Time = 19.99s\n",
            "Epoch 38, Batch 8/29: Loss = 0.6884, Time = 20.31s\n",
            "Epoch 38, Batch 9/29: Loss = 0.6940, Time = 20.28s\n",
            "Epoch 38, Batch 10/29: Loss = 0.6349, Time = 20.22s\n",
            "Epoch 38, Batch 11/29: Loss = 0.7904, Time = 20.45s\n",
            "Epoch 38, Batch 12/29: Loss = 0.6036, Time = 20.29s\n",
            "Epoch 38, Batch 13/29: Loss = 0.6967, Time = 19.96s\n",
            "Epoch 38, Batch 14/29: Loss = 0.6996, Time = 20.35s\n",
            "Epoch 38, Batch 15/29: Loss = 0.7159, Time = 20.22s\n",
            "Epoch 38, Batch 16/29: Loss = 0.7145, Time = 20.35s\n",
            "Epoch 38, Batch 17/29: Loss = 0.7505, Time = 20.46s\n",
            "Epoch 38, Batch 18/29: Loss = 0.6284, Time = 19.93s\n",
            "Epoch 38, Batch 19/29: Loss = 0.7255, Time = 20.39s\n",
            "Epoch 38, Batch 20/29: Loss = 0.6389, Time = 20.26s\n",
            "Epoch 38, Batch 21/29: Loss = 0.6466, Time = 20.35s\n",
            "Epoch 38, Batch 22/29: Loss = 0.6328, Time = 20.55s\n",
            "Epoch 38, Batch 23/29: Loss = 0.6838, Time = 20.03s\n",
            "Epoch 38, Batch 24/29: Loss = 0.6657, Time = 20.23s\n",
            "Epoch 38, Batch 25/29: Loss = 0.6868, Time = 20.28s\n",
            "Epoch 38, Batch 26/29: Loss = 0.5787, Time = 20.38s\n",
            "Epoch 38, Batch 27/29: Loss = 0.7306, Time = 20.41s\n",
            "Epoch 38, Batch 28/29: Loss = 0.6898, Time = 20.29s\n",
            "Epoch 38, Batch 29/29: Loss = 0.6484, Time = 3.29s\n",
            "Epoch 38: Train Loss = 0.6808, Val Loss = 0.5967, Time = 22964.81s\n",
            "Epoch 39, Batch 1/29: Loss = 0.6472, Time = 19.76s\n",
            "Epoch 39, Batch 2/29: Loss = 0.6529, Time = 20.14s\n",
            "Epoch 39, Batch 3/29: Loss = 0.6804, Time = 20.29s\n",
            "Epoch 39, Batch 4/29: Loss = 0.7466, Time = 20.15s\n",
            "Epoch 39, Batch 5/29: Loss = 0.6360, Time = 20.27s\n",
            "Epoch 39, Batch 6/29: Loss = 0.6279, Time = 20.43s\n",
            "Epoch 39, Batch 7/29: Loss = 0.5688, Time = 19.83s\n",
            "Epoch 39, Batch 8/29: Loss = 0.5795, Time = 20.27s\n",
            "Epoch 39, Batch 9/29: Loss = 0.6901, Time = 20.33s\n",
            "Epoch 39, Batch 10/29: Loss = 0.6577, Time = 20.14s\n",
            "Epoch 39, Batch 11/29: Loss = 0.6587, Time = 20.31s\n",
            "Epoch 39, Batch 12/29: Loss = 0.6660, Time = 19.96s\n",
            "Epoch 39, Batch 13/29: Loss = 0.7175, Time = 20.37s\n",
            "Epoch 39, Batch 14/29: Loss = 0.7703, Time = 20.47s\n",
            "Epoch 39, Batch 15/29: Loss = 0.7152, Time = 20.40s\n",
            "Epoch 39, Batch 16/29: Loss = 0.7142, Time = 20.45s\n",
            "Epoch 39, Batch 17/29: Loss = 0.6715, Time = 20.13s\n",
            "Epoch 39, Batch 18/29: Loss = 0.6760, Time = 20.28s\n",
            "Epoch 39, Batch 19/29: Loss = 0.6312, Time = 20.29s\n",
            "Epoch 39, Batch 20/29: Loss = 0.8072, Time = 20.48s\n",
            "Epoch 39, Batch 21/29: Loss = 0.7123, Time = 20.44s\n",
            "Epoch 39, Batch 22/29: Loss = 0.7354, Time = 20.00s\n",
            "Epoch 39, Batch 23/29: Loss = 0.7498, Time = 20.23s\n",
            "Epoch 39, Batch 24/29: Loss = 0.6927, Time = 20.27s\n",
            "Epoch 39, Batch 25/29: Loss = 0.6528, Time = 20.31s\n",
            "Epoch 39, Batch 26/29: Loss = 0.5966, Time = 20.43s\n",
            "Epoch 39, Batch 27/29: Loss = 0.6868, Time = 20.13s\n",
            "Epoch 39, Batch 28/29: Loss = 0.6996, Time = 20.41s\n",
            "Epoch 39, Batch 29/29: Loss = 0.6989, Time = 3.28s\n",
            "Epoch 39: Train Loss = 0.6807, Val Loss = 0.5972, Time = 23550.12s\n",
            "Epoch 40, Batch 1/29: Loss = 0.6828, Time = 20.01s\n",
            "Epoch 40, Batch 2/29: Loss = 0.7235, Time = 20.15s\n",
            "Epoch 40, Batch 3/29: Loss = 0.6461, Time = 20.33s\n",
            "Epoch 40, Batch 4/29: Loss = 0.6591, Time = 20.32s\n",
            "Epoch 40, Batch 5/29: Loss = 0.7071, Time = 20.35s\n",
            "Epoch 40, Batch 6/29: Loss = 0.7468, Time = 20.02s\n",
            "Epoch 40, Batch 7/29: Loss = 0.6816, Time = 20.31s\n",
            "Epoch 40, Batch 8/29: Loss = 0.6640, Time = 20.25s\n",
            "Epoch 40, Batch 9/29: Loss = 0.7067, Time = 20.29s\n",
            "Epoch 40, Batch 10/29: Loss = 0.7100, Time = 19.94s\n",
            "Epoch 40, Batch 11/29: Loss = 0.6710, Time = 20.29s\n",
            "Epoch 40, Batch 12/29: Loss = 0.6646, Time = 20.23s\n",
            "Epoch 40, Batch 13/29: Loss = 0.6541, Time = 20.44s\n",
            "Epoch 40, Batch 14/29: Loss = 0.6369, Time = 20.65s\n",
            "Epoch 40, Batch 15/29: Loss = 0.6211, Time = 20.44s\n",
            "Epoch 40, Batch 16/29: Loss = 0.6833, Time = 20.03s\n",
            "Epoch 40, Batch 17/29: Loss = 0.6265, Time = 20.32s\n",
            "Epoch 40, Batch 18/29: Loss = 0.7062, Time = 20.52s\n",
            "Epoch 40, Batch 19/29: Loss = 0.6272, Time = 20.47s\n",
            "Epoch 40, Batch 20/29: Loss = 0.7534, Time = 20.37s\n",
            "Epoch 40, Batch 21/29: Loss = 0.6761, Time = 20.42s\n",
            "Epoch 40, Batch 22/29: Loss = 0.6768, Time = 20.04s\n",
            "Epoch 40, Batch 23/29: Loss = 0.6492, Time = 20.54s\n",
            "Epoch 40, Batch 24/29: Loss = 0.6401, Time = 20.48s\n",
            "Epoch 40, Batch 25/29: Loss = 0.6553, Time = 20.50s\n",
            "Epoch 40, Batch 26/29: Loss = 0.7530, Time = 20.47s\n",
            "Epoch 40, Batch 27/29: Loss = 0.6583, Time = 20.45s\n",
            "Epoch 40, Batch 28/29: Loss = 0.6802, Time = 20.14s\n",
            "Epoch 40, Batch 29/29: Loss = 0.8832, Time = 3.27s\n",
            "Epoch 40: Train Loss = 0.6843, Val Loss = 0.5940, Time = 24137.53s\n",
            "Epoch 41, Batch 1/29: Loss = 0.7261, Time = 20.39s\n",
            "Epoch 41, Batch 2/29: Loss = 0.6716, Time = 19.93s\n",
            "Epoch 41, Batch 3/29: Loss = 0.7657, Time = 20.43s\n",
            "Epoch 41, Batch 4/29: Loss = 0.6011, Time = 20.22s\n",
            "Epoch 41, Batch 5/29: Loss = 0.7310, Time = 20.26s\n",
            "Epoch 41, Batch 6/29: Loss = 0.6841, Time = 20.22s\n",
            "Epoch 41, Batch 7/29: Loss = 0.6595, Time = 20.03s\n",
            "Epoch 41, Batch 8/29: Loss = 0.7425, Time = 20.31s\n",
            "Epoch 41, Batch 9/29: Loss = 0.6526, Time = 20.38s\n",
            "Epoch 41, Batch 10/29: Loss = 0.6532, Time = 20.34s\n",
            "Epoch 41, Batch 11/29: Loss = 0.6264, Time = 20.57s\n",
            "Epoch 41, Batch 12/29: Loss = 0.7376, Time = 20.41s\n",
            "Epoch 41, Batch 13/29: Loss = 0.7034, Time = 19.98s\n",
            "Epoch 41, Batch 14/29: Loss = 0.6265, Time = 20.46s\n",
            "Epoch 41, Batch 15/29: Loss = 0.5898, Time = 20.49s\n",
            "Epoch 41, Batch 16/29: Loss = 0.6205, Time = 20.43s\n",
            "Epoch 41, Batch 17/29: Loss = 0.7136, Time = 20.50s\n",
            "Epoch 41, Batch 18/29: Loss = 0.7105, Time = 20.44s\n",
            "Epoch 41, Batch 19/29: Loss = 0.6395, Time = 20.16s\n",
            "Epoch 41, Batch 20/29: Loss = 0.6409, Time = 20.46s\n",
            "Epoch 41, Batch 21/29: Loss = 0.6889, Time = 20.62s\n",
            "Epoch 41, Batch 22/29: Loss = 0.6514, Time = 20.44s\n",
            "Epoch 41, Batch 23/29: Loss = 0.6600, Time = 20.42s\n",
            "Epoch 41, Batch 24/29: Loss = 0.7073, Time = 20.62s\n",
            "Epoch 41, Batch 25/29: Loss = 0.7633, Time = 20.06s\n",
            "Epoch 41, Batch 26/29: Loss = 0.6442, Time = 20.32s\n",
            "Epoch 41, Batch 27/29: Loss = 0.6076, Time = 20.43s\n",
            "Epoch 41, Batch 28/29: Loss = 0.7278, Time = 20.26s\n",
            "Epoch 41, Batch 29/29: Loss = 0.7212, Time = 3.28s\n",
            "Epoch 41: Train Loss = 0.6782, Val Loss = 0.5927, Time = 24725.64s\n",
            "Epoch 42, Batch 1/29: Loss = 0.6543, Time = 20.09s\n",
            "Epoch 42, Batch 2/29: Loss = 0.7087, Time = 20.13s\n",
            "Epoch 42, Batch 3/29: Loss = 0.6426, Time = 20.22s\n",
            "Epoch 42, Batch 4/29: Loss = 0.6392, Time = 20.33s\n",
            "Epoch 42, Batch 5/29: Loss = 0.7273, Time = 20.10s\n",
            "Epoch 42, Batch 6/29: Loss = 0.6896, Time = 20.43s\n",
            "Epoch 42, Batch 7/29: Loss = 0.7067, Time = 20.49s\n",
            "Epoch 42, Batch 8/29: Loss = 0.7357, Time = 20.53s\n",
            "Epoch 42, Batch 9/29: Loss = 0.6442, Time = 20.54s\n",
            "Epoch 42, Batch 10/29: Loss = 0.6268, Time = 20.19s\n",
            "Epoch 42, Batch 11/29: Loss = 0.6734, Time = 20.47s\n",
            "Epoch 42, Batch 12/29: Loss = 0.6579, Time = 20.56s\n",
            "Epoch 42, Batch 13/29: Loss = 0.7229, Time = 20.41s\n",
            "Epoch 42, Batch 14/29: Loss = 0.6456, Time = 20.33s\n",
            "Epoch 42, Batch 15/29: Loss = 0.6436, Time = 20.19s\n",
            "Epoch 42, Batch 16/29: Loss = 0.6557, Time = 20.50s\n",
            "Epoch 42, Batch 17/29: Loss = 0.6648, Time = 20.41s\n",
            "Epoch 42, Batch 18/29: Loss = 0.6583, Time = 20.36s\n",
            "Epoch 42, Batch 19/29: Loss = 0.6875, Time = 20.57s\n",
            "Epoch 42, Batch 20/29: Loss = 0.7283, Time = 20.13s\n",
            "Epoch 42, Batch 21/29: Loss = 0.7287, Time = 20.38s\n",
            "Epoch 42, Batch 22/29: Loss = 0.6430, Time = 20.42s\n",
            "Epoch 42, Batch 23/29: Loss = 0.6677, Time = 20.34s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}